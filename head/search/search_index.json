{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Gluu Flex Documentation # Introduction # The Gluu Server is a container distribution of free open source software (FOSS) for identity and access management (IAM). SaaS, custom, open source and commercial web and mobile applications can leverage a Gluu Server for user authentication, identity information, and policy decisions. Common use cases include: Single sign-on (SSO) Mobile authentication API access management Two-factor authentication (2FA) Customer identity and access management (CIAM) Identity federation Open Web Standards # The Gluu Server can be deployed to support the following open standards for authentication, authorization, federated identity, and identity management: OAuth 2.0 OpenID Connect User Managed Access 2.0 (UMA) SAML 2.0 System for Cross-domain Identity Management (SCIM) FIDO Universal 2nd Factor (U2F) FIDO 2.0 / WebAuthn Lightweight Directory Access Protocol (LDAP) Remote Authentication Dial-In User Service (RADIUS) Support # The Gluu Flex contract includes guaranteed response times and consultative support via our support portal . Contribute # We want to keep improving our docs. Please help us improve by submitting any improvements to our [Github](https://github.com/GluuFederation/flex/docs. If you're a Github pro, submit a pull request. If not, just open an issue on any typos, bugs, or improvements you'd like to see addressed. We need your help... even if you're not a coder, you can contribute! License #","title":"Overview"},{"location":"#gluu-flex-documentation","text":"","title":"Gluu Flex Documentation"},{"location":"#introduction","text":"The Gluu Server is a container distribution of free open source software (FOSS) for identity and access management (IAM). SaaS, custom, open source and commercial web and mobile applications can leverage a Gluu Server for user authentication, identity information, and policy decisions. Common use cases include: Single sign-on (SSO) Mobile authentication API access management Two-factor authentication (2FA) Customer identity and access management (CIAM) Identity federation","title":"Introduction"},{"location":"#open-web-standards","text":"The Gluu Server can be deployed to support the following open standards for authentication, authorization, federated identity, and identity management: OAuth 2.0 OpenID Connect User Managed Access 2.0 (UMA) SAML 2.0 System for Cross-domain Identity Management (SCIM) FIDO Universal 2nd Factor (U2F) FIDO 2.0 / WebAuthn Lightweight Directory Access Protocol (LDAP) Remote Authentication Dial-In User Service (RADIUS)","title":"Open Web Standards"},{"location":"#support","text":"The Gluu Flex contract includes guaranteed response times and consultative support via our support portal .","title":"Support"},{"location":"#contribute","text":"We want to keep improving our docs. Please help us improve by submitting any improvements to our [Github](https://github.com/GluuFederation/flex/docs. If you're a Github pro, submit a pull request. If not, just open an issue on any typos, bugs, or improvements you'd like to see addressed. We need your help... even if you're not a coder, you can contribute!","title":"Contribute"},{"location":"#license","text":"","title":"License"},{"location":"CHANGELOG/","text":"Changelog # 5.0.0-4 (2022-12-08) # Bug Fixes # getting ready for a release ( a0de091 ) 5.0.0-3 (2022-11-08) # Features # admin-ui: reviewed previously updated dependencies #416 ( ab81760 ) Bug Fixes # getting ready to release 5.0.0-3 ( e8f3ecc ) Miscellaneous Chores # release 5.0.0-2 ( 06c6e64 )","title":"Changelog"},{"location":"CHANGELOG/#changelog","text":"","title":"Changelog"},{"location":"CHANGELOG/#500-4-2022-12-08","text":"","title":"5.0.0-4 (2022-12-08)"},{"location":"CHANGELOG/#bug-fixes","text":"getting ready for a release ( a0de091 )","title":"Bug Fixes"},{"location":"CHANGELOG/#500-3-2022-11-08","text":"","title":"5.0.0-3 (2022-11-08)"},{"location":"CHANGELOG/#features","text":"admin-ui: reviewed previously updated dependencies #416 ( ab81760 )","title":"Features"},{"location":"CHANGELOG/#bug-fixes_1","text":"getting ready to release 5.0.0-3 ( e8f3ecc )","title":"Bug Fixes"},{"location":"CHANGELOG/#miscellaneous-chores","text":"release 5.0.0-2 ( 06c6e64 )","title":"Miscellaneous Chores"},{"location":"admin-ui/","text":"Where is this content? # The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates.","title":"Index"},{"location":"admin-ui/#where-is-this-content","text":"The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates.","title":"Where is this content?"},{"location":"admin-ui/properties/","text":"Where is this content? # The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates.","title":"Admin UI Properties"},{"location":"admin-ui/properties/#where-is-this-content","text":"The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates.","title":"Where is this content?"},{"location":"casa/","text":"Gluu Casa # Overview # Gluu Casa (\"Casa\") is a self-service web portal for end-users to manage authentication and authorization preferences for their account in Gluu Flex. For example, as people interact with an organization's digital services, they may need to: Enroll, delete and manage two-factor authentication (2FA) credentials for their account (e.g. FIDO security keys, mobile apps, phone numbers, etc.) Turn 2FA on and off View and manage which external apps have been authorized to access what personal data View trusted devices Casa provides a platform for people to perform these account security functions and more. Documentation # Gluu Flex incorporates the latest version of Gluu Casa .","title":"Casa"},{"location":"casa/#gluu-casa","text":"","title":"Gluu Casa"},{"location":"casa/#overview","text":"Gluu Casa (\"Casa\") is a self-service web portal for end-users to manage authentication and authorization preferences for their account in Gluu Flex. For example, as people interact with an organization's digital services, they may need to: Enroll, delete and manage two-factor authentication (2FA) credentials for their account (e.g. FIDO security keys, mobile apps, phone numbers, etc.) Turn 2FA on and off View and manage which external apps have been authorized to access what personal data View trusted devices Casa provides a platform for people to perform these account security functions and more.","title":"Overview"},{"location":"casa/#documentation","text":"Gluu Flex incorporates the latest version of Gluu Casa .","title":"Documentation"},{"location":"casa/template/","text":"Where is this content? # The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates.","title":"Template"},{"location":"casa/template/#where-is-this-content","text":"The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates.","title":"Where is this content?"},{"location":"install/","text":"Where is this content? # The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates.","title":"Index"},{"location":"install/#where-is-this-content","text":"The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates.","title":"Where is this content?"},{"location":"install/helm-install/","tags":["administration","installation","helm"],"text":"Overview # Gluu Flex enables organizations to build a scalable centralized authentication and authorization service using free open source software. The components of the project include client and server implementations of the OAuth, OpenID Connect, SCIM and FIDO standards. All these components are deployed using Gluu helm chart . You can check the reference guide to view the list of the chart components and values.","title":"Overview"},{"location":"install/helm-install/#overview","text":"Gluu Flex enables organizations to build a scalable centralized authentication and authorization service using free open source software. The components of the project include client and server implementations of the OAuth, OpenID Connect, SCIM and FIDO standards. All these components are deployed using Gluu helm chart . You can check the reference guide to view the list of the chart components and values.","title":"Overview"},{"location":"install/helm-install/amazon-eks/","tags":["administration","installation","helm","EKS","Amazon Web Services","AWS"],"text":"Install Gluu on EKS # System Requirements # The resources may be set to the minimum as below: 8 GiB RAM 8 CPU cores 50GB hard-disk Use the listing below for detailed estimation of minimum required resources. Table contains the default resources recommendations per service. Depending on the use of each service the resources needs may be increase or decrease. Service CPU Unit RAM Disk Space Processor Type Required Auth server 2.5 2.5GB N/A 64 Bit Yes LDAP (OpenDJ) 1.5 2GB 10GB 64 Bit Only if couchbase is not installed fido2 0.5 0.5GB N/A 64 Bit No scim 1.0 1.0GB N/A 64 Bit No config - job 0.5 0.5GB N/A 64 Bit Yes on fresh installs persistence - job 0.5 0.5GB N/A 64 Bit Yes on fresh installs nginx 1 1GB N/A 64 Bit Yes if not ALB auth-key-rotation 0.3 0.3GB N/A 64 Bit No [Strongly recommended] config-api 1 1GB N/A 64 Bit No casa 1 1GB N/A 64 Bit No admin-ui 2 2GB N/A 64 Bit No Releases of images are in style 1.0.0-beta.0, 1.0.0-0 Initial Setup # Install aws cli Configure your AWS user account using aws configure command. This makes you able to authenticate before creating the cluster. Note that this user account must have permissions to work with Amazon EKS IAM roles and service linked roles, AWS CloudFormation, and a VPC and related resources Install kubectl Install eksctl Create cluster using eksctl such as the following example: eksctl create cluster --name gluu-cluster --nodegroup-name gluu-nodes --node-type NODE_TYPE --nodes 2 --managed --region REGION_CODE You can adjust node-type and nodes number as per your desired cluster size Install Helm3 Create gluu namespace where our resources will reside kubectl create namespace gluu Gluu Installation using Helm # Install Nginx-Ingress , if you are not using Istio ingress helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo add stable https://charts.helm.sh/stable helm repo update helm install nginx ingress-nginx/ingress-nginx Create a file named override.yaml and add changes as per your desired configuration: FQDN/domain is not registered: Get the Loadbalancer address: kubectl get svc nginx-ingress-nginx-controller --output jsonpath='{.status.loadBalancer.ingress[0].hostname}' Add the following yaml snippet to your override.yaml file: global : isFqdnRegistered : false config : configmap : lbAddr : http:// #Add LB address from previous command FQDN/domain is registered: Add the following yaml snippet to your override.yaml file`: global : isFqdnRegistered : true fqdn : demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu config : configmap : lbAddr : http:// #Add LB address from previous command nginx : ingress : enabled : true path : / hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu tls : - secretName : tls-certificate hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu LDAP/Opendj for persistence storage Add the following yaml snippet to your override.yaml file: global : cnPersistenceType : ldap storageClass : provisioner : kubernetes.io/aws-ebs opendj : enabled : true So if your desired configuration has no-FQDN and LDAP, the final override.yaml file will look something like that: global : cnPersistenceType : ldap isFqdnRegistered : false storageClass : provisioner : kubernetes.io/aws-ebs opendj : enabled : true config : configmap : lbAddr : http:// #Add LB address from previous command nginx-ingress : ingress : path : / hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu tls : - secretName : tls-certificate hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu MySQL for persistence storage In a production environment, a production grade MySQL server should be used such as Amazon RDS For testing purposes, you can deploy it on the EKS cluster using the following commands: helm repo add bitnami https://charts.bitnami.com/bitnami helm install my-release --set auth.rootPassword=Test1234#,auth.database=gluu bitnami/mysql -n gluu Add the following yaml snippet to your override.yaml file: global : cnPersistenceType : sql config : configmap : cnSqlDbName : gluu cnSqlDbPort : 3306 cnSqlDbDialect : mysql cnSqlDbHost : my-release-mysql.gluu.svc cnSqlDbUser : root cnSqlDbTimezone : UTC cnSqldbUserPassword : Test1234# So if your desired configuration has FQDN and MySQL, the final override.yaml file will look something like that: global : cnPersistenceType : sql isFqdnRegistered : true fqdn : demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu nginx-ingress : ingress : path : / hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu tls : - secretName : tls-certificate hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu config : configmap : lbAddr : http:// #Add LB address from previous command cnSqlDbName : gluu cnSqlDbPort : 3306 cnSqlDbDialect : mysql cnSqlDbHost : my-release-mysql.gluu.svc cnSqlDbUser : root cnSqlDbTimezone : UTC cnSqldbUserPassword : Test1234# Install Gluu After finishing all the tweaks to the override.yaml file, we can use it to install gluu. helm repo add gluu-flex https://docs.gluu.org/charts helm repo update helm install gluu gluu-flex/gluu -n gluu -f override.yaml","title":"Amazon EKS"},{"location":"install/helm-install/amazon-eks/#install-gluu-on-eks","text":"","title":"Install Gluu on EKS"},{"location":"install/helm-install/amazon-eks/#system-requirements","text":"The resources may be set to the minimum as below: 8 GiB RAM 8 CPU cores 50GB hard-disk Use the listing below for detailed estimation of minimum required resources. Table contains the default resources recommendations per service. Depending on the use of each service the resources needs may be increase or decrease. Service CPU Unit RAM Disk Space Processor Type Required Auth server 2.5 2.5GB N/A 64 Bit Yes LDAP (OpenDJ) 1.5 2GB 10GB 64 Bit Only if couchbase is not installed fido2 0.5 0.5GB N/A 64 Bit No scim 1.0 1.0GB N/A 64 Bit No config - job 0.5 0.5GB N/A 64 Bit Yes on fresh installs persistence - job 0.5 0.5GB N/A 64 Bit Yes on fresh installs nginx 1 1GB N/A 64 Bit Yes if not ALB auth-key-rotation 0.3 0.3GB N/A 64 Bit No [Strongly recommended] config-api 1 1GB N/A 64 Bit No casa 1 1GB N/A 64 Bit No admin-ui 2 2GB N/A 64 Bit No Releases of images are in style 1.0.0-beta.0, 1.0.0-0","title":"System Requirements"},{"location":"install/helm-install/amazon-eks/#initial-setup","text":"Install aws cli Configure your AWS user account using aws configure command. This makes you able to authenticate before creating the cluster. Note that this user account must have permissions to work with Amazon EKS IAM roles and service linked roles, AWS CloudFormation, and a VPC and related resources Install kubectl Install eksctl Create cluster using eksctl such as the following example: eksctl create cluster --name gluu-cluster --nodegroup-name gluu-nodes --node-type NODE_TYPE --nodes 2 --managed --region REGION_CODE You can adjust node-type and nodes number as per your desired cluster size Install Helm3 Create gluu namespace where our resources will reside kubectl create namespace gluu","title":"Initial Setup"},{"location":"install/helm-install/amazon-eks/#gluu-installation-using-helm","text":"Install Nginx-Ingress , if you are not using Istio ingress helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo add stable https://charts.helm.sh/stable helm repo update helm install nginx ingress-nginx/ingress-nginx Create a file named override.yaml and add changes as per your desired configuration: FQDN/domain is not registered: Get the Loadbalancer address: kubectl get svc nginx-ingress-nginx-controller --output jsonpath='{.status.loadBalancer.ingress[0].hostname}' Add the following yaml snippet to your override.yaml file: global : isFqdnRegistered : false config : configmap : lbAddr : http:// #Add LB address from previous command FQDN/domain is registered: Add the following yaml snippet to your override.yaml file`: global : isFqdnRegistered : true fqdn : demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu config : configmap : lbAddr : http:// #Add LB address from previous command nginx : ingress : enabled : true path : / hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu tls : - secretName : tls-certificate hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu LDAP/Opendj for persistence storage Add the following yaml snippet to your override.yaml file: global : cnPersistenceType : ldap storageClass : provisioner : kubernetes.io/aws-ebs opendj : enabled : true So if your desired configuration has no-FQDN and LDAP, the final override.yaml file will look something like that: global : cnPersistenceType : ldap isFqdnRegistered : false storageClass : provisioner : kubernetes.io/aws-ebs opendj : enabled : true config : configmap : lbAddr : http:// #Add LB address from previous command nginx-ingress : ingress : path : / hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu tls : - secretName : tls-certificate hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu MySQL for persistence storage In a production environment, a production grade MySQL server should be used such as Amazon RDS For testing purposes, you can deploy it on the EKS cluster using the following commands: helm repo add bitnami https://charts.bitnami.com/bitnami helm install my-release --set auth.rootPassword=Test1234#,auth.database=gluu bitnami/mysql -n gluu Add the following yaml snippet to your override.yaml file: global : cnPersistenceType : sql config : configmap : cnSqlDbName : gluu cnSqlDbPort : 3306 cnSqlDbDialect : mysql cnSqlDbHost : my-release-mysql.gluu.svc cnSqlDbUser : root cnSqlDbTimezone : UTC cnSqldbUserPassword : Test1234# So if your desired configuration has FQDN and MySQL, the final override.yaml file will look something like that: global : cnPersistenceType : sql isFqdnRegistered : true fqdn : demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu nginx-ingress : ingress : path : / hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu tls : - secretName : tls-certificate hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu config : configmap : lbAddr : http:// #Add LB address from previous command cnSqlDbName : gluu cnSqlDbPort : 3306 cnSqlDbDialect : mysql cnSqlDbHost : my-release-mysql.gluu.svc cnSqlDbUser : root cnSqlDbTimezone : UTC cnSqldbUserPassword : Test1234# Install Gluu After finishing all the tweaks to the override.yaml file, we can use it to install gluu. helm repo add gluu-flex https://docs.gluu.org/charts helm repo update helm install gluu gluu-flex/gluu -n gluu -f override.yaml","title":"Gluu Installation using Helm"},{"location":"install/helm-install/google-gke/","tags":["administration","installation","helm","GKE","Google Cloud","GCP"],"text":"Install Gluu on GKE # System Requirements # The resources may be set to the minimum as below: 8 GiB RAM 8 CPU cores 50GB hard-disk Use the listing below for detailed estimation of minimum required resources. Table contains the default resources recommendations per service. Depending on the use of each service the resources needs may be increase or decrease. Service CPU Unit RAM Disk Space Processor Type Required Auth server 2.5 2.5GB N/A 64 Bit Yes LDAP (OpenDJ) 1.5 2GB 10GB 64 Bit Only if couchbase is not installed fido2 0.5 0.5GB N/A 64 Bit No scim 1.0 1.0GB N/A 64 Bit No config - job 0.5 0.5GB N/A 64 Bit Yes on fresh installs persistence - job 0.5 0.5GB N/A 64 Bit Yes on fresh installs nginx 1 1GB N/A 64 Bit Yes if not ALB auth-key-rotation 0.3 0.3GB N/A 64 Bit No [Strongly recommended] config-api 1 1GB N/A 64 Bit No casa 1 1GB N/A 64 Bit No admin-ui 2 2GB N/A 64 Bit No Releases of images are in style 1.0.0-beta.0, 1.0.0-0 Initial Setup # If you are using Cloud Shell, you can skip to step 4. Install gcloud Install kubectl using gcloud components install kubectl command Create cluster using a command such as the following example: gcloud container clusters create gluu-cluster --num-nodes 2 --machine-type e2-highcpu-8 --zone us-west1-a You can adjust num-nodes and machine-type as per your desired cluster size Install Helm3 Create gluu namespace where our resources will reside kubectl create namespace gluu Gluu Installation using Helm # Install Nginx-Ingress , if you are not using Istio ingress helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo add stable https://charts.helm.sh/stable helm repo update helm install nginx ingress-nginx/ingress-nginx Create a file named override.yaml and add changes as per your desired configuration: FQDN/domain is not registered: Get the Loadbalancer IP: kubectl get svc nginx-ingress-nginx-controller --output jsonpath='{.status.loadBalancer.ingress[0].ip}' Add the following yaml snippet to your override.yaml file: global : lbIp : #Add the Loadbalance IP from the previous command isFqdnRegistered : false FQDN/domain is registered: Add the following yaml snippet to your override.yaml file`: global : lbIp : #Add the LoadBalancer IP from the previous command isFqdnRegistered : true fqdn : demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu nginx-ingress : ingress : path : / hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu tls : - secretName : tls-certificate hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu LDAP/Opendj for persistence storage Add the following yaml snippet to your override.yaml file: global : cnPersistenceType : ldap storageClass : provisioner : kubernetes.io/gce-pd opendj : enabled : true So if your desired configuration has no-FQDN and LDAP, the final override.yaml file will look something like that: global : cnPersistenceType : ldap lbIp : #Add the Loadbalancer IP from the previous command isFqdnRegistered : false storageClass : provisioner : kubernetes.io/gce-pd opendj : enabled : true nginx-ingress : ingress : path : / hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu tls : - secretName : tls-certificate hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu MySQL for persistence storage In a production environment, a production grade MySQL server should be used such as Cloud SQL For testing purposes, you can deploy it on the GKE cluster using the following commands: helm repo add bitnami https://charts.bitnami.com/bitnami helm install my-release --set auth.rootPassword=Test1234#,auth.database=gluu bitnami/mysql -n gluu Add the following yaml snippet to your override.yaml file: global : cnPersistenceType : sql config : configmap : cnSqlDbName : gluu cnSqlDbPort : 3306 cnSqlDbDialect : mysql cnSqlDbHost : my-release-mysql.gluu.svc cnSqlDbUser : root cnSqlDbTimezone : UTC cnSqldbUserPassword : Test1234# So if your desired configuration has FQDN and MySQL, the final override.yaml file will look something like that: global : cnPersistenceType : sql lbIp : \"\" #Add the LoadBalancer IP from previous command isFqdnRegistered : true fqdn : demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu nginx-ingress : ingress : path : / hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu tls : - secretName : tls-certificate hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu config : configmap : cnSqlDbName : gluu cnSqlDbPort : 3306 cnSqlDbDialect : mysql cnSqlDbHost : my-release-mysql.gluu.svc cnSqlDbUser : root cnSqlDbTimezone : UTC cnSqldbUserPassword : Test1234# Install Gluu After finishing all the tweaks to the override.yaml file, we can use it to install gluu. helm repo add gluu-flex https://docs.gluu.org/charts helm repo update helm install gluu gluu-flex/gluu -n gluu -f override.yaml","title":"Google GKE"},{"location":"install/helm-install/google-gke/#install-gluu-on-gke","text":"","title":"Install Gluu on GKE"},{"location":"install/helm-install/google-gke/#system-requirements","text":"The resources may be set to the minimum as below: 8 GiB RAM 8 CPU cores 50GB hard-disk Use the listing below for detailed estimation of minimum required resources. Table contains the default resources recommendations per service. Depending on the use of each service the resources needs may be increase or decrease. Service CPU Unit RAM Disk Space Processor Type Required Auth server 2.5 2.5GB N/A 64 Bit Yes LDAP (OpenDJ) 1.5 2GB 10GB 64 Bit Only if couchbase is not installed fido2 0.5 0.5GB N/A 64 Bit No scim 1.0 1.0GB N/A 64 Bit No config - job 0.5 0.5GB N/A 64 Bit Yes on fresh installs persistence - job 0.5 0.5GB N/A 64 Bit Yes on fresh installs nginx 1 1GB N/A 64 Bit Yes if not ALB auth-key-rotation 0.3 0.3GB N/A 64 Bit No [Strongly recommended] config-api 1 1GB N/A 64 Bit No casa 1 1GB N/A 64 Bit No admin-ui 2 2GB N/A 64 Bit No Releases of images are in style 1.0.0-beta.0, 1.0.0-0","title":"System Requirements"},{"location":"install/helm-install/google-gke/#initial-setup","text":"If you are using Cloud Shell, you can skip to step 4. Install gcloud Install kubectl using gcloud components install kubectl command Create cluster using a command such as the following example: gcloud container clusters create gluu-cluster --num-nodes 2 --machine-type e2-highcpu-8 --zone us-west1-a You can adjust num-nodes and machine-type as per your desired cluster size Install Helm3 Create gluu namespace where our resources will reside kubectl create namespace gluu","title":"Initial Setup"},{"location":"install/helm-install/google-gke/#gluu-installation-using-helm","text":"Install Nginx-Ingress , if you are not using Istio ingress helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo add stable https://charts.helm.sh/stable helm repo update helm install nginx ingress-nginx/ingress-nginx Create a file named override.yaml and add changes as per your desired configuration: FQDN/domain is not registered: Get the Loadbalancer IP: kubectl get svc nginx-ingress-nginx-controller --output jsonpath='{.status.loadBalancer.ingress[0].ip}' Add the following yaml snippet to your override.yaml file: global : lbIp : #Add the Loadbalance IP from the previous command isFqdnRegistered : false FQDN/domain is registered: Add the following yaml snippet to your override.yaml file`: global : lbIp : #Add the LoadBalancer IP from the previous command isFqdnRegistered : true fqdn : demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu nginx-ingress : ingress : path : / hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu tls : - secretName : tls-certificate hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu LDAP/Opendj for persistence storage Add the following yaml snippet to your override.yaml file: global : cnPersistenceType : ldap storageClass : provisioner : kubernetes.io/gce-pd opendj : enabled : true So if your desired configuration has no-FQDN and LDAP, the final override.yaml file will look something like that: global : cnPersistenceType : ldap lbIp : #Add the Loadbalancer IP from the previous command isFqdnRegistered : false storageClass : provisioner : kubernetes.io/gce-pd opendj : enabled : true nginx-ingress : ingress : path : / hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu tls : - secretName : tls-certificate hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu MySQL for persistence storage In a production environment, a production grade MySQL server should be used such as Cloud SQL For testing purposes, you can deploy it on the GKE cluster using the following commands: helm repo add bitnami https://charts.bitnami.com/bitnami helm install my-release --set auth.rootPassword=Test1234#,auth.database=gluu bitnami/mysql -n gluu Add the following yaml snippet to your override.yaml file: global : cnPersistenceType : sql config : configmap : cnSqlDbName : gluu cnSqlDbPort : 3306 cnSqlDbDialect : mysql cnSqlDbHost : my-release-mysql.gluu.svc cnSqlDbUser : root cnSqlDbTimezone : UTC cnSqldbUserPassword : Test1234# So if your desired configuration has FQDN and MySQL, the final override.yaml file will look something like that: global : cnPersistenceType : sql lbIp : \"\" #Add the LoadBalancer IP from previous command isFqdnRegistered : true fqdn : demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu nginx-ingress : ingress : path : / hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu tls : - secretName : tls-certificate hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu config : configmap : cnSqlDbName : gluu cnSqlDbPort : 3306 cnSqlDbDialect : mysql cnSqlDbHost : my-release-mysql.gluu.svc cnSqlDbUser : root cnSqlDbTimezone : UTC cnSqldbUserPassword : Test1234# Install Gluu After finishing all the tweaks to the override.yaml file, we can use it to install gluu. helm repo add gluu-flex https://docs.gluu.org/charts helm repo update helm install gluu gluu-flex/gluu -n gluu -f override.yaml","title":"Gluu Installation using Helm"},{"location":"install/helm-install/local/","tags":["administration","installation","helm"],"text":"Install Gluu Server Locally with minikube and MicroK8s # System Requirements # For local deployments like minikube and MicroK8s or cloud installations in demo mode, resources may be set to the minimum as below: 8GB RAM 4 CPU cores 50GB hard-disk Use the listing below for detailed estimation of minimum required resources. Table contains the default resources recommendations per service. Depending on the use of each service the resources needs may be increase or decrease. Service CPU Unit RAM Disk Space Processor Type Required Auth server 2.5 2.5GB N/A 64 Bit Yes LDAP (OpenDJ) 1.5 2GB 10GB 64 Bit Only if couchbase is not installed fido2 0.5 0.5GB N/A 64 Bit No scim 1.0 1.0GB N/A 64 Bit No config - job 0.5 0.5GB N/A 64 Bit Yes on fresh installs persistence - job 0.5 0.5GB N/A 64 Bit Yes on fresh installs nginx 1 1GB N/A 64 Bit Yes if not ALB auth-key-rotation 0.3 0.3GB N/A 64 Bit No [Strongly recommended] config-api 1 1GB N/A 64 Bit No casa 1 1GB N/A 64 Bit No admin-ui 2 2GB N/A 64 Bit No Releases of images are in style 1.0.0-beta.0, 1.0.0-0 Installation Steps # Start a fresh ubuntu 18.04 or 20.04 4 CPU, 16 GB RAM, and 50GB SSD VM with ports 443 and 80 open. Then execute the following sudo su - wget https://raw.githubusercontent.com/GluuFederation/flex/vreplace-flex-version/automation/startflexmonolithdemo.sh && chmod u+x startflexmonolithdemo.sh && ./startflexmonolithdemo.sh This will install docker, microk8s, helm and Gluu with the default settings that can be found inside values.yaml . The installer will automatically add a record to your hosts record in the VM but if you want access the endpoints outside the VM you must map the ip of the instance running ubuntu to the FQDN you provided and then access the endpoints at your browser such in the example in the table below. Service Example endpoint Auth server https://FQDN/.well-known/openid-configuration fido2 https://FQDN/.well-known/fido2-configuration scim https://FQDN/.well-known/scim-configuration Casa https://FQDN/casa Admin-UI https://FQDN/admin","title":"Local Kubernetes Cluster"},{"location":"install/helm-install/local/#install-gluu-server-locally-with-minikube-and-microk8s","text":"","title":"Install Gluu Server Locally with minikube and MicroK8s"},{"location":"install/helm-install/local/#system-requirements","text":"For local deployments like minikube and MicroK8s or cloud installations in demo mode, resources may be set to the minimum as below: 8GB RAM 4 CPU cores 50GB hard-disk Use the listing below for detailed estimation of minimum required resources. Table contains the default resources recommendations per service. Depending on the use of each service the resources needs may be increase or decrease. Service CPU Unit RAM Disk Space Processor Type Required Auth server 2.5 2.5GB N/A 64 Bit Yes LDAP (OpenDJ) 1.5 2GB 10GB 64 Bit Only if couchbase is not installed fido2 0.5 0.5GB N/A 64 Bit No scim 1.0 1.0GB N/A 64 Bit No config - job 0.5 0.5GB N/A 64 Bit Yes on fresh installs persistence - job 0.5 0.5GB N/A 64 Bit Yes on fresh installs nginx 1 1GB N/A 64 Bit Yes if not ALB auth-key-rotation 0.3 0.3GB N/A 64 Bit No [Strongly recommended] config-api 1 1GB N/A 64 Bit No casa 1 1GB N/A 64 Bit No admin-ui 2 2GB N/A 64 Bit No Releases of images are in style 1.0.0-beta.0, 1.0.0-0","title":"System Requirements"},{"location":"install/helm-install/local/#installation-steps","text":"Start a fresh ubuntu 18.04 or 20.04 4 CPU, 16 GB RAM, and 50GB SSD VM with ports 443 and 80 open. Then execute the following sudo su - wget https://raw.githubusercontent.com/GluuFederation/flex/vreplace-flex-version/automation/startflexmonolithdemo.sh && chmod u+x startflexmonolithdemo.sh && ./startflexmonolithdemo.sh This will install docker, microk8s, helm and Gluu with the default settings that can be found inside values.yaml . The installer will automatically add a record to your hosts record in the VM but if you want access the endpoints outside the VM you must map the ip of the instance running ubuntu to the FQDN you provided and then access the endpoints at your browser such in the example in the table below. Service Example endpoint Auth server https://FQDN/.well-known/openid-configuration fido2 https://FQDN/.well-known/fido2-configuration scim https://FQDN/.well-known/scim-configuration Casa https://FQDN/casa Admin-UI https://FQDN/admin","title":"Installation Steps"},{"location":"install/helm-install/microsoft-azure/","tags":["administration","installation","helm","AKS","Microsoft","Azure"],"text":"Install Gluu on AKS # System Requirements # The resources may be set to the minimum as below: 8 GiB RAM 8 CPU cores 50GB hard-disk Use the listing below for detailed estimation of minimum required resources. Table contains the default resources recommendations per service. Depending on the use of each service the resources needs may be increase or decrease. Service CPU Unit RAM Disk Space Processor Type Required Auth server 2.5 2.5GB N/A 64 Bit Yes LDAP (OpenDJ) 1.5 2GB 10GB 64 Bit Only if couchbase is not installed fido2 0.5 0.5GB N/A 64 Bit No scim 1.0 1.0GB N/A 64 Bit No config - job 0.5 0.5GB N/A 64 Bit Yes on fresh installs persistence - job 0.5 0.5GB N/A 64 Bit Yes on fresh installs nginx 1 1GB N/A 64 Bit Yes if not ALB auth-key-rotation 0.3 0.3GB N/A 64 Bit No [Strongly recommended] config-api 1 1GB N/A 64 Bit No casa 1 1GB N/A 64 Bit No admin-ui 2 2GB N/A 64 Bit No Releases of images are in style 1.0.0-beta.0, 1.0.0-0 Initial Setup # Install Azure CLI Create a Resource Group az group create --name gluu-resource-group --location eastus Create an AKS cluster such as the following example: az aks create -g gluu-resource-group -n gluu-cluster --enable-managed-identity --node-vm-size NODE_TYPE --node-count 2 --enable-addons monitoring --enable-msi-auth-for-monitoring --generate-ssh-keys You can adjust node-count and node-vm-size as per your desired cluster size Connect to the cluster az aks install-cli az aks get-credentials --resource-group gluu-resource-group --name gluu-cluster Install Helm3 Create gluu namespace where our resources will reside kubectl create namespace gluu Gluu Installation using Helm # Install Nginx-Ingress , if you are not using Istio ingress helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo add stable https://charts.helm.sh/stable helm repo update helm install nginx ingress-nginx/ingress-nginx Create a file named override.yaml and add changes as per your desired configuration: FQDN/domain is not registered: Get the Loadbalancer IP: kubectl get svc nginx-ingress-nginx-controller --output jsonpath='{.status.loadBalancer.ingress[0].ip}' Add the following yaml snippet to your override.yaml file: global : lbIp : #Add the Loadbalance IP from the previous command isFqdnRegistered : false FQDN/domain is registered: Add the following yaml snippet to your override.yaml file`: global : lbIp : #Add the LoadBalancer IP from the previous command isFqdnRegistered : true fqdn : demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu nginx-ingress : ingress : path : / hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu tls : - secretName : tls-certificate hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu LDAP/Opendj for persistence storage Add the following yaml snippet to your override.yaml file: global : cnPersistenceType : ldap storageClass : provisioner : disk.csi.azure.com opendj : enabled : true So if your desired configuration has no-FQDN and LDAP, the final override.yaml file will look something like that: global : cnPersistenceType : ldap lbIp : #Add the Loadbalancer IP from the previous command isFqdnRegistered : false storageClass : provisioner : disk.csi.azure.com opendj : enabled : true nginx-ingress : ingress : path : / hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu tls : - secretName : tls-certificate hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu MySQL for persistence storage In a production environment, a production grade MySQL server should be used such as Azure Database for MySQL For testing purposes, you can deploy it on the AKS cluster using the following commands: helm repo add bitnami https://charts.bitnami.com/bitnami helm install my-release --set auth.rootPassword=Test1234#,auth.database=gluu bitnami/mysql -n gluu Add the following yaml snippet to your override.yaml file: global : cnPersistenceType : sql config : configmap : cnSqlDbName : gluu cnSqlDbPort : 3306 cnSqlDbDialect : mysql cnSqlDbHost : my-release-mysql.gluu.svc cnSqlDbUser : root cnSqlDbTimezone : UTC cnSqldbUserPassword : Test1234# So if your desired configuration has FQDN and MySQL, the final override.yaml file will look something like that: global : cnPersistenceType : sql lbIp : \"\" #Add the LoadBalancer IP from previous command isFqdnRegistered : true fqdn : demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu nginx-ingress : ingress : path : / hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu tls : - secretName : tls-certificate hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu config : configmap : cnSqlDbName : gluu cnSqlDbPort : 3306 cnSqlDbDialect : mysql cnSqlDbHost : my-release-mysql.gluu.svc cnSqlDbUser : root cnSqlDbTimezone : UTC cnSqldbUserPassword : Test1234# Install Gluu After finishing all the tweaks to the override.yaml file, we can use it to install gluu. helm repo add gluu-flex https://docs.gluu.org/charts helm repo update helm install gluu gluu-flex/gluu -n gluu -f override.yaml","title":"Microsoft Azure AKS"},{"location":"install/helm-install/microsoft-azure/#install-gluu-on-aks","text":"","title":"Install Gluu on AKS"},{"location":"install/helm-install/microsoft-azure/#system-requirements","text":"The resources may be set to the minimum as below: 8 GiB RAM 8 CPU cores 50GB hard-disk Use the listing below for detailed estimation of minimum required resources. Table contains the default resources recommendations per service. Depending on the use of each service the resources needs may be increase or decrease. Service CPU Unit RAM Disk Space Processor Type Required Auth server 2.5 2.5GB N/A 64 Bit Yes LDAP (OpenDJ) 1.5 2GB 10GB 64 Bit Only if couchbase is not installed fido2 0.5 0.5GB N/A 64 Bit No scim 1.0 1.0GB N/A 64 Bit No config - job 0.5 0.5GB N/A 64 Bit Yes on fresh installs persistence - job 0.5 0.5GB N/A 64 Bit Yes on fresh installs nginx 1 1GB N/A 64 Bit Yes if not ALB auth-key-rotation 0.3 0.3GB N/A 64 Bit No [Strongly recommended] config-api 1 1GB N/A 64 Bit No casa 1 1GB N/A 64 Bit No admin-ui 2 2GB N/A 64 Bit No Releases of images are in style 1.0.0-beta.0, 1.0.0-0","title":"System Requirements"},{"location":"install/helm-install/microsoft-azure/#initial-setup","text":"Install Azure CLI Create a Resource Group az group create --name gluu-resource-group --location eastus Create an AKS cluster such as the following example: az aks create -g gluu-resource-group -n gluu-cluster --enable-managed-identity --node-vm-size NODE_TYPE --node-count 2 --enable-addons monitoring --enable-msi-auth-for-monitoring --generate-ssh-keys You can adjust node-count and node-vm-size as per your desired cluster size Connect to the cluster az aks install-cli az aks get-credentials --resource-group gluu-resource-group --name gluu-cluster Install Helm3 Create gluu namespace where our resources will reside kubectl create namespace gluu","title":"Initial Setup"},{"location":"install/helm-install/microsoft-azure/#gluu-installation-using-helm","text":"Install Nginx-Ingress , if you are not using Istio ingress helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo add stable https://charts.helm.sh/stable helm repo update helm install nginx ingress-nginx/ingress-nginx Create a file named override.yaml and add changes as per your desired configuration: FQDN/domain is not registered: Get the Loadbalancer IP: kubectl get svc nginx-ingress-nginx-controller --output jsonpath='{.status.loadBalancer.ingress[0].ip}' Add the following yaml snippet to your override.yaml file: global : lbIp : #Add the Loadbalance IP from the previous command isFqdnRegistered : false FQDN/domain is registered: Add the following yaml snippet to your override.yaml file`: global : lbIp : #Add the LoadBalancer IP from the previous command isFqdnRegistered : true fqdn : demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu nginx-ingress : ingress : path : / hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu tls : - secretName : tls-certificate hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu LDAP/Opendj for persistence storage Add the following yaml snippet to your override.yaml file: global : cnPersistenceType : ldap storageClass : provisioner : disk.csi.azure.com opendj : enabled : true So if your desired configuration has no-FQDN and LDAP, the final override.yaml file will look something like that: global : cnPersistenceType : ldap lbIp : #Add the Loadbalancer IP from the previous command isFqdnRegistered : false storageClass : provisioner : disk.csi.azure.com opendj : enabled : true nginx-ingress : ingress : path : / hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu tls : - secretName : tls-certificate hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu MySQL for persistence storage In a production environment, a production grade MySQL server should be used such as Azure Database for MySQL For testing purposes, you can deploy it on the AKS cluster using the following commands: helm repo add bitnami https://charts.bitnami.com/bitnami helm install my-release --set auth.rootPassword=Test1234#,auth.database=gluu bitnami/mysql -n gluu Add the following yaml snippet to your override.yaml file: global : cnPersistenceType : sql config : configmap : cnSqlDbName : gluu cnSqlDbPort : 3306 cnSqlDbDialect : mysql cnSqlDbHost : my-release-mysql.gluu.svc cnSqlDbUser : root cnSqlDbTimezone : UTC cnSqldbUserPassword : Test1234# So if your desired configuration has FQDN and MySQL, the final override.yaml file will look something like that: global : cnPersistenceType : sql lbIp : \"\" #Add the LoadBalancer IP from previous command isFqdnRegistered : true fqdn : demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu nginx-ingress : ingress : path : / hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu tls : - secretName : tls-certificate hosts : - demoexample.gluu.org #CHANGE-THIS to the FQDN used for Gluu config : configmap : cnSqlDbName : gluu cnSqlDbPort : 3306 cnSqlDbDialect : mysql cnSqlDbHost : my-release-mysql.gluu.svc cnSqlDbUser : root cnSqlDbTimezone : UTC cnSqldbUserPassword : Test1234# Install Gluu After finishing all the tweaks to the override.yaml file, we can use it to install gluu. helm repo add gluu-flex https://docs.gluu.org/charts helm repo update helm install gluu gluu-flex/gluu -n gluu -f override.yaml","title":"Gluu Installation using Helm"},{"location":"install/helm-install/rancher/","tags":["administration","installation","helm"],"text":"Install Gluu Server Using Rancher Marketplace # For this quick start we will use a single node Kubernetes install in docker with a self-signed certificate . Note For a more generic setup, use Rancher UI to deploy the setup. For more options please follow this link . Installation Steps # Provision a Linux 4 CPU, 16 GB RAM, and 50GB SSD VM with ports 443 and 80 open. Save the VM IP address. For development environments, the VM can be set up using VMWare Workstation Player or VirtualBox with Ubuntu 20.0.4 operating system running on VM. Install Docker . Execute docker run -d --restart = unless-stopped -p 80 :80 -p 443 :443 --privileged rancher/rancher:latest The final line of the returned text is the container-id , which you'll need for the next step. Execute the following command to get the boostrap password for login. docker logs <container-id> 2 > & 1 | grep \"Bootstrap Password:\" Head to https://<VM-IP-ADDRESS-FROM-FIRST-STEP> and log in with the username admin and the password from the previous step. If you are logging into Rancher for the first time, you'll need to enter just the password, and on the next step, Rancher will ask you to reset your current password. Next you'll see the Rancher home page with a list of existing clusters. By default, the name of the newly created cluster would be local . Click on the cluster name to go to the dashboard. From the top-left menu expand Apps & Marketplace and click charts . Search for Gluu and begin your installation. During Step 1 of installation, be sure to select the Customize Helm options before install options. In Step 2, customize the settings for the Gluu installation. Specifically Optional Services from where you can enable Gluu modules. In Step 3, unselect the Wait option and start the installation.","title":"Using Rancher Marketplace"},{"location":"install/helm-install/rancher/#install-gluu-server-using-rancher-marketplace","text":"For this quick start we will use a single node Kubernetes install in docker with a self-signed certificate . Note For a more generic setup, use Rancher UI to deploy the setup. For more options please follow this link .","title":"Install Gluu Server Using Rancher Marketplace"},{"location":"install/helm-install/rancher/#installation-steps","text":"Provision a Linux 4 CPU, 16 GB RAM, and 50GB SSD VM with ports 443 and 80 open. Save the VM IP address. For development environments, the VM can be set up using VMWare Workstation Player or VirtualBox with Ubuntu 20.0.4 operating system running on VM. Install Docker . Execute docker run -d --restart = unless-stopped -p 80 :80 -p 443 :443 --privileged rancher/rancher:latest The final line of the returned text is the container-id , which you'll need for the next step. Execute the following command to get the boostrap password for login. docker logs <container-id> 2 > & 1 | grep \"Bootstrap Password:\" Head to https://<VM-IP-ADDRESS-FROM-FIRST-STEP> and log in with the username admin and the password from the previous step. If you are logging into Rancher for the first time, you'll need to enter just the password, and on the next step, Rancher will ask you to reset your current password. Next you'll see the Rancher home page with a list of existing clusters. By default, the name of the newly created cluster would be local . Click on the cluster name to go to the dashboard. From the top-left menu expand Apps & Marketplace and click charts . Search for Gluu and begin your installation. During Step 1 of installation, be sure to select the Customize Helm options before install options. In Step 2, customize the settings for the Gluu installation. Specifically Optional Services from where you can enable Gluu modules. In Step 3, unselect the Wait option and start the installation.","title":"Installation Steps"},{"location":"install/vm-install/","text":"Where is this content? # The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates.","title":"Index"},{"location":"install/vm-install/#where-is-this-content","text":"The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates.","title":"Where is this content?"},{"location":"install/vm-install/dynamic-download/","text":"Where is this content? # The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates. This upcoming page is for Gluu Flex, an upcoming Gluu product. If you're looking for the Janssen Project installation page, find it here .","title":"Dynamic download"},{"location":"install/vm-install/dynamic-download/#where-is-this-content","text":"The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates. This upcoming page is for Gluu Flex, an upcoming Gluu product. If you're looking for the Janssen Project installation page, find it here .","title":"Where is this content?"},{"location":"install/vm-install/rhel/","text":"Where is this content? # The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates. This content is for Gluu Flex, an upcoming Gluu product. If you're looking for the Gluu 4 installation page, find it here , and if you're looking for the Janssen Project installation page, find it here .","title":"RHEL"},{"location":"install/vm-install/rhel/#where-is-this-content","text":"The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates. This content is for Gluu Flex, an upcoming Gluu product. If you're looking for the Gluu 4 installation page, find it here , and if you're looking for the Janssen Project installation page, find it here .","title":"Where is this content?"},{"location":"install/vm-install/setup/","text":"Where is this content? # The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates.","title":"Setup Script"},{"location":"install/vm-install/setup/#where-is-this-content","text":"The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates.","title":"Where is this content?"},{"location":"install/vm-install/suse/","text":"Where is this content? # The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates. This content is for Gluu Flex, an upcoming Gluu product. If you're looking for the Gluu 4 installation page, find it here , and if you're looking for the Janssen Project installation page, find it here .","title":"Suse"},{"location":"install/vm-install/suse/#where-is-this-content","text":"The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates. This content is for Gluu Flex, an upcoming Gluu product. If you're looking for the Gluu 4 installation page, find it here , and if you're looking for the Janssen Project installation page, find it here .","title":"Where is this content?"},{"location":"install/vm-install/ubuntu/","text":"Ubuntu Janssen Installation # Before you install, check the VM system requirements . Supported Versions # Ubuntu 20.04 Install the Package # Download the release package from the Github Gluu Flex Releases wget https://github.com/JanssenProject/jans/releases/download/v5.0.0-4/flex_5.0.0-4.ubuntu20.04_amd64.deb -P ~/ Verify integrity of the downloaded package by verifying published sha256sum . Download sha256sum file for the package wget https://github.com/JanssenProject/jans/releases/download/v5.0.0-4/flex_5.0.0-4.ubuntu20.04_amd64.deb.sha256sum -P ~/ Check the hash if it is matching. sha256sum -c flex_5.0.0-4.ubuntu20.04_amd64.deb.sha256sum Output similar to below should confirm the integrity of the downloaded package. <package-name>: OK Install the package apt install -y ~/flex_5.0.0-4.ubuntu20.04_amd64.deb Run the setup script # Run the setup script in interactive mode: python3 /opt/jans/flex/flex-linux-setup/flex_setup.py See more detailed instructions on the setup script if you're confused how to answer any of the questions, for details about command line arguments, or you would prefer to use a properties file instead of interactive mode. Ubuntu Janssen Un-Installation # Removing Janssen is a two step process: Delete files installed by Gluu Flex Remove and purge the jans package Use the command below to uninstall the Gluu Flex server python3 /opt/jans/flex/flex-linux-setup/flex_setup.py --remove-flex The command below removes and purges the jans package apt-get --purge remove jans","title":"Ubuntu"},{"location":"install/vm-install/ubuntu/#ubuntu-janssen-installation","text":"Before you install, check the VM system requirements .","title":"Ubuntu Janssen Installation"},{"location":"install/vm-install/ubuntu/#supported-versions","text":"Ubuntu 20.04","title":"Supported Versions"},{"location":"install/vm-install/ubuntu/#install-the-package","text":"Download the release package from the Github Gluu Flex Releases wget https://github.com/JanssenProject/jans/releases/download/v5.0.0-4/flex_5.0.0-4.ubuntu20.04_amd64.deb -P ~/ Verify integrity of the downloaded package by verifying published sha256sum . Download sha256sum file for the package wget https://github.com/JanssenProject/jans/releases/download/v5.0.0-4/flex_5.0.0-4.ubuntu20.04_amd64.deb.sha256sum -P ~/ Check the hash if it is matching. sha256sum -c flex_5.0.0-4.ubuntu20.04_amd64.deb.sha256sum Output similar to below should confirm the integrity of the downloaded package. <package-name>: OK Install the package apt install -y ~/flex_5.0.0-4.ubuntu20.04_amd64.deb","title":"Install the Package"},{"location":"install/vm-install/ubuntu/#run-the-setup-script","text":"Run the setup script in interactive mode: python3 /opt/jans/flex/flex-linux-setup/flex_setup.py See more detailed instructions on the setup script if you're confused how to answer any of the questions, for details about command line arguments, or you would prefer to use a properties file instead of interactive mode.","title":"Run the setup script"},{"location":"install/vm-install/ubuntu/#ubuntu-janssen-un-installation","text":"Removing Janssen is a two step process: Delete files installed by Gluu Flex Remove and purge the jans package Use the command below to uninstall the Gluu Flex server python3 /opt/jans/flex/flex-linux-setup/flex_setup.py --remove-flex The command below removes and purges the jans package apt-get --purge remove jans","title":"Ubuntu Janssen Un-Installation"},{"location":"install/vm-install/vm-requirements/","text":"VM System Requirements # Gluu Flex currently provides packages for these Linux distros: Ubuntu (versions: 20.04) SUSE (SLES or LEAP) (version: 15) RedHat Enterprise Linux (version: 8) Hardware Requirements # A single-VM deployment is where all services are running on one server. Although, the requirements can vary based on the size of the data and the required concurrency, the following guidelines can help you plan: Development and Test Environments # 4 GB RAM 2 CPU 20 GB Disk Production Environment Recommendation: # 8 GB RAM 4 CPU 4 GB swap space 50 GB Disk Port Configuration # Gluu Flex requires the following ports to be open for incoming connections. Port Protocol Notes 443 TCP TLS/HTTP You may want to use a redirect on port 80 to 443, although it is not required. Of course you will also need some way to login to your server, but that is out of scope of these docs. Check your server firewall documentation to configure your firewall to allow https . Hostname / IP Address Configuration # It is recommended that you use a static ip address for Gluu Flex. Your server should also return the hostname for the hostname command, it's recommended that you add the hostname to the /etc/hosts file. File Descriptor Configuration (FD) # Like most database and Internet servers, you must have at least 65k file descriptors. If you don't, your server will hang. First, check the current file descriptor limit using command below. If the existing FD limit exceeds 65535, then you're good. # cat /proc/sys/fs/file-max If FD limit is less than 65535 (e.g. 1024), then follow the steps below to increase the value. 1) Set soft and hard limits by adding the following lines in the /etc/security/limits.conf file * soft nofile 65535 * hard nofile 262144 2) Add the following lines to /etc/pam.d/login if not already present session required pam_limits.so 3) Increase the FD limit in /proc/sys/fs/file-max echo 65535 > /proc/sys/fs/file-max** 4) Use the ulimit command to set the FD limit to the hard limit specified in /etc/security/limits.conf . If setting to hard limit doesn't work, then try to set it to the soft limit. ulimit -n 262144 5) Restart the system","title":"VM Requirements"},{"location":"install/vm-install/vm-requirements/#vm-system-requirements","text":"Gluu Flex currently provides packages for these Linux distros: Ubuntu (versions: 20.04) SUSE (SLES or LEAP) (version: 15) RedHat Enterprise Linux (version: 8)","title":"VM System Requirements"},{"location":"install/vm-install/vm-requirements/#hardware-requirements","text":"A single-VM deployment is where all services are running on one server. Although, the requirements can vary based on the size of the data and the required concurrency, the following guidelines can help you plan:","title":"Hardware Requirements"},{"location":"install/vm-install/vm-requirements/#development-and-test-environments","text":"4 GB RAM 2 CPU 20 GB Disk","title":"Development and Test Environments"},{"location":"install/vm-install/vm-requirements/#production-environment-recommendation","text":"8 GB RAM 4 CPU 4 GB swap space 50 GB Disk","title":"Production Environment Recommendation:"},{"location":"install/vm-install/vm-requirements/#port-configuration","text":"Gluu Flex requires the following ports to be open for incoming connections. Port Protocol Notes 443 TCP TLS/HTTP You may want to use a redirect on port 80 to 443, although it is not required. Of course you will also need some way to login to your server, but that is out of scope of these docs. Check your server firewall documentation to configure your firewall to allow https .","title":"Port Configuration"},{"location":"install/vm-install/vm-requirements/#hostname-ip-address-configuration","text":"It is recommended that you use a static ip address for Gluu Flex. Your server should also return the hostname for the hostname command, it's recommended that you add the hostname to the /etc/hosts file.","title":"Hostname / IP Address Configuration"},{"location":"install/vm-install/vm-requirements/#file-descriptor-configuration-fd","text":"Like most database and Internet servers, you must have at least 65k file descriptors. If you don't, your server will hang. First, check the current file descriptor limit using command below. If the existing FD limit exceeds 65535, then you're good. # cat /proc/sys/fs/file-max If FD limit is less than 65535 (e.g. 1024), then follow the steps below to increase the value. 1) Set soft and hard limits by adding the following lines in the /etc/security/limits.conf file * soft nofile 65535 * hard nofile 262144 2) Add the following lines to /etc/pam.d/login if not already present session required pam_limits.so 3) Increase the FD limit in /proc/sys/fs/file-max echo 65535 > /proc/sys/fs/file-max** 4) Use the ulimit command to set the FD limit to the hard limit specified in /etc/security/limits.conf . If setting to hard limit doesn't work, then try to set it to the soft limit. ulimit -n 262144 5) Restart the system","title":"File Descriptor Configuration (FD)"},{"location":"reference/","tags":["administration","reference"],"text":"Overview # Please use the left navigation menu to browse the content of this section while we are still working on developing content for Overview page.","title":"Overview"},{"location":"reference/#overview","text":"Please use the left navigation menu to browse the content of this section while we are still working on developing content for Overview page.","title":"Overview"},{"location":"reference/kubernetes/","tags":["administration","reference","kubernetes","architecture","components"],"text":"Overview # This Reference guide helps you learn about the components and architecture of Gluu Flex. Gluu Flex components # auth-server : The OAuth Authorization Server, the OpenID Connect Provider, the UMA Authorization Server--this is the main Internet facing component of Janssen. It's the service that returns tokens, JWT's and identity assertions. This service must be Internet facing. auth-key-rotation : Responsible for regenerating auth-keys per x hours. config-api : The API to configure the auth-server and other components is consolidated in this component. This service should not be Internet-facing. OpenDJ : A directory server which implements a wide range of Lightweight Directory Access Protocol and related standards, including full compliance with LDAPv3 but also support for Directory Service Markup Language (DSMLv2).Written in Java, OpenDJ offers multi-master replication, access control, and many extensions. Fido : Provides the server side endpoints to enroll and validate devices that use FIDO. It provides both FIDO U2F (register, authenticate) and FIDO 2 (attestation, assertion) endpoints. This service must be internet facing. SCIM : a JSON/REST API to manage user data. Use it to add, edit and update user information. This service should not be Internet facing. Casa : self-service web portal for end-users to manage authentication and authorization preferences for their account in a Gluu Server. Admin UI : The admin web portal to configure and control your Gluu server. Architectural diagram of Gluu #","title":"Overview"},{"location":"reference/kubernetes/#overview","text":"This Reference guide helps you learn about the components and architecture of Gluu Flex.","title":"Overview"},{"location":"reference/kubernetes/#gluu-flex-components","text":"auth-server : The OAuth Authorization Server, the OpenID Connect Provider, the UMA Authorization Server--this is the main Internet facing component of Janssen. It's the service that returns tokens, JWT's and identity assertions. This service must be Internet facing. auth-key-rotation : Responsible for regenerating auth-keys per x hours. config-api : The API to configure the auth-server and other components is consolidated in this component. This service should not be Internet-facing. OpenDJ : A directory server which implements a wide range of Lightweight Directory Access Protocol and related standards, including full compliance with LDAPv3 but also support for Directory Service Markup Language (DSMLv2).Written in Java, OpenDJ offers multi-master replication, access control, and many extensions. Fido : Provides the server side endpoints to enroll and validate devices that use FIDO. It provides both FIDO U2F (register, authenticate) and FIDO 2 (attestation, assertion) endpoints. This service must be internet facing. SCIM : a JSON/REST API to manage user data. Use it to add, edit and update user information. This service should not be Internet facing. Casa : self-service web portal for end-users to manage authentication and authorization preferences for their account in a Gluu Server. Admin UI : The admin web portal to configure and control your Gluu server.","title":"Gluu Flex components"},{"location":"reference/kubernetes/#architectural-diagram-of-gluu","text":"","title":"Architectural diagram of Gluu"},{"location":"reference/kubernetes/docker-admin-ui/","tags":["administration","reference","kubernetes","docker image"],"text":"docker-admin-ui # A containerized application for Gluu Admin UI frontend. Versions # See Releases for stable versions. For bleeding-edge/unstable version, use gluufederation/admin-ui:1.0.0_dev . Environment Variables # The following environment variables are supported by the container: CN_CONFIG_ADAPTER : The config backend adapter, can be consul (default), kubernetes , or google . CN_CONFIG_CONSUL_HOST : hostname or IP of Consul (default to localhost ). CN_CONFIG_CONSUL_PORT : port of Consul (default to 8500 ). CN_CONFIG_CONSUL_CONSISTENCY : Consul consistency mode (choose one of default , consistent , or stale ). Default to stale mode. CN_CONFIG_CONSUL_SCHEME : supported Consul scheme ( http or https ). CN_CONFIG_CONSUL_VERIFY : whether to verify cert or not (default to false ). CN_CONFIG_CONSUL_CACERT_FILE : path to Consul CA cert file (default to /etc/certs/consul_ca.crt ). This file will be used if it exists and CN_CONFIG_CONSUL_VERIFY set to true . CN_CONFIG_CONSUL_CERT_FILE : path to Consul cert file (default to /etc/certs/consul_client.crt ). CN_CONFIG_CONSUL_KEY_FILE : path to Consul key file (default to /etc/certs/consul_client.key ). CN_CONFIG_CONSUL_TOKEN_FILE : path to file contains ACL token (default to /etc/certs/consul_token ). CN_CONFIG_KUBERNETES_NAMESPACE : Kubernetes namespace (default to default ). CN_CONFIG_KUBERNETES_CONFIGMAP : Kubernetes configmaps name (default to jans ). CN_CONFIG_KUBERNETES_USE_KUBE_CONFIG : Load credentials from $HOME/.kube/config , only useful for non-container environment (default to false ). CN_SECRET_ADAPTER : The secrets' adapter, can be vault (default), kubernetes , or google . CN_SECRET_VAULT_SCHEME : supported Vault scheme ( http or https ). CN_SECRET_VAULT_HOST : hostname or IP of Vault (default to localhost ). CN_SECRET_VAULT_PORT : port of Vault (default to 8200 ). CN_SECRET_VAULT_VERIFY : whether to verify cert or not (default to false ). CN_SECRET_VAULT_ROLE_ID_FILE : path to file contains Vault AppRole role ID (default to /etc/certs/vault_role_id ). CN_SECRET_VAULT_SECRET_ID_FILE : path to file contains Vault AppRole secret ID (default to /etc/certs/vault_secret_id ). CN_SECRET_VAULT_CERT_FILE : path to Vault cert file (default to /etc/certs/vault_client.crt ). CN_SECRET_VAULT_KEY_FILE : path to Vault key file (default to /etc/certs/vault_client.key ). CN_SECRET_VAULT_CACERT_FILE : path to Vault CA cert file (default to /etc/certs/vault_ca.crt ). This file will be used if it exists and CN_SECRET_VAULT_VERIFY set to true . CN_SECRET_KUBERNETES_NAMESPACE : Kubernetes namespace (default to default ). CN_SECRET_KUBERNETES_CONFIGMAP : Kubernetes secrets name (default to jans ). CN_SECRET_KUBERNETES_USE_KUBE_CONFIG : Load credentials from $HOME/.kube/config , only useful for non-container environment (default to false ). CN_WAIT_MAX_TIME : How long the startup \"health checks\" should run (default to 300 seconds). CN_WAIT_SLEEP_DURATION : Delay between startup \"health checks\" (default to 10 seconds). GOOGLE_PROJECT_ID : Google Project ID (default to empty string). Used when CN_CONFIG_ADAPTER or CN_SECRET_ADAPTER set to google . GOOGLE_APPLICATION_CREDENTIALS : Path to Google credentials JSON file (default to /etc/jans/conf/google-credentials.json ). Used when CN_CONFIG_ADAPTER or CN_SECRET_ADAPTER set to google . CN_GOOGLE_SECRET_VERSION_ID : Janssen secret version ID in Google Secret Manager. Defaults to latest , which is recommended. CN_GOOGLE_SECRET_NAME_PREFIX : Prefix for Janssen secret in Google Secret Manager. Defaults to jans . If left jans-secret secret will be created. CN_GOOGLE_SECRET_MANAGER_PASSPHRASE : Passphrase for Janssen secret in Google Secret Manager. This is recommended to be changed and defaults to secret . CN_TOKEN_SERVER_BASE_HOSTNAME : Hostname of token server (default to localhost ). CN_TOKEN_SERVER_AUTHZ_ENDPOINT : Authorization endpoint at token server (default to /jans-auth/authorize.htm ). CN_TOKEN_SERVER_TOKEN_ENDPOINT : Token endpoint at token server (default to /jans-auth/restv1/token ). CN_TOKEN_SERVER_INTROSPECTION_ENDPOINT : Introspection endpoint at token server (default to /jans-auth/restv1/introspection ). CN_TOKEN_SERVER_USERINFO_ENDPOINT : User info endpoint at token server (default to /jans-auth/restv1/userinfo ). CN_TOKEN_SERVER_CLIENT_ID : Client ID registered at token server. CN_TOKEN_SERVER_CERT_FILE : Path to token server certificate (default to /etc/certs/token_server.crt ). CN_PERSISTENCE_TYPE : Persistence backend being used (one of ldap , couchbase , or hybrid ; default to ldap ). CN_HYBRID_MAPPING : Specify data mapping for each persistence (default to \"{}\" ). Note this environment only takes effect when CN_PERSISTENCE_TYPE is set to hybrid . See hybrid mapping section for details. CN_LDAP_URL : Address and port of LDAP server (default to localhost:1636 ). CN_LDAP_USE_SSL : Whether to use SSL connection to LDAP server (default to true ). CN_COUCHBASE_URL : Address of Couchbase server (default to localhost ). CN_COUCHBASE_USER : Username of Couchbase server (default to admin ). CN_COUCHBASE_CERT_FILE : Couchbase root certificate location (default to /etc/certs/couchbase.crt ). CN_COUCHBASE_PASSWORD_FILE : Path to file contains Couchbase password (default to /etc/jans/conf/couchbase_password ). CN_COUCHBASE_CONN_TIMEOUT : Connect timeout used when a bucket is opened (default to 10000 milliseconds). CN_COUCHBASE_CONN_MAX_WAIT : Maximum time to wait before retrying connection (default to 20000 milliseconds). CN_COUCHBASE_SCAN_CONSISTENCY : Default scan consistency; one of not_bounded , request_plus , or statement_plus (default to not_bounded ). CN_COUCHBASE_BUCKET_PREFIX : Prefix for Couchbase buckets (default to jans ). CN_COUCHBASE_TRUSTSTORE_ENABLE : Enable truststore for encrypted Couchbase connection (default to true ). CN_COUCHBASE_KEEPALIVE_INTERVAL : Keep-alive interval for Couchbase connection (default to 30000 milliseconds). CN_COUCHBASE_KEEPALIVE_TIMEOUT : Keep-alive timeout for Couchbase connection (default to 2500 milliseconds). CN_SQL_DB_DIALECT : Dialect name of SQL backend (one of mysql , pgsql ; default to mysql ). CN_SQL_DB_HOST : Host of SQL backend (default to localhost ). CN_SQL_DB_PORT : Port of SQL backend (default to 3306 ). CN_SQL_DB_NAME : Database name (default to jans ) CN_SQL_DB_USER : Username to interact with SQL backend (default to jans ). CN_GOOGLE_SPANNER_INSTANCE_ID : Instance ID of Google Spanner (default to empty string). CN_GOOGLE_SPANNER_DATABASE_ID : Database ID of Google Spanner (default to empty string). GOOGLE_APPLICATION_CREDENTIALS : Path to Google credentials JSON file (default to /etc/jans/conf/google-credentials.json ). GOOGLE_PROJECT_ID : Google Project ID (default to empty string). GOOGLE_PROJECT_ID : Google Project ID (default to empty string). Used when CN_CONFIG_ADAPTER or CN_SECRET_ADAPTER set to google . GOOGLE_APPLICATION_CREDENTIALS : Path to Google credentials JSON file (default to /etc/jans/conf/google-credentials.json ). Used when CN_CONFIG_ADAPTER or CN_SECRET_ADAPTER set to google . CN_GOOGLE_SPANNER_INSTANCE_ID : Google Spanner instance ID. CN_GOOGLE_SPANNER_DATABASE_ID : Google Spanner database ID. GLUU_ADMIN_UI_PLUGINS : Comma-separated additional plugins to be enabled (default to empty string). See Adding plugins for details. GLUU_ADMIN_UI_AUTH_METHOD : Authentication method for admin-ui (one of basic or casa ; default to basic ). Note, changing the value require restart to jans-config-api. Hybrid mapping # Hybrid persistence supports all available persistence types. To configure hybrid persistence and its data mapping, follow steps below: Set CN_PERSISTENCE_TYPE environment variable to hybrid Set CN_HYBRID_MAPPING with the following format: { \"default\": \"<couchbase|ldap|spanner|sql>\", \"user\": \"<couchbase|ldap|spanner|sql>\", \"site\": \"<couchbase|ldap|spanner|sql>\", \"cache\": \"<couchbase|ldap|spanner|sql>\", \"token\": \"<couchbase|ldap|spanner|sql>\", \"session\": \"<couchbase|ldap|spanner|sql>\", } Example: { \"default\": \"sql\", \"user\": \"spanner\", \"site\": \"ldap\", \"cache\": \"sql\", \"token\": \"couchbase\", \"session\": \"spanner\", } Adding plugins # To add plugins to AdminUI, for example myplugin.zip Set the name of the plugin (without the extension name) in environment variable GLUU_ADMIN_UI_PLUGINS , for example: GLUU_ADMIN_UI_PLUGINS=myplugin . Mount myplugin.zip to /app/plugins/myplugin.zip inside the pod/container. Note that if /app/plugins/myplugin.zip is not exist, plugin will be ignored.","title":"docker-admin-ui"},{"location":"reference/kubernetes/docker-admin-ui/#docker-admin-ui","text":"A containerized application for Gluu Admin UI frontend.","title":"docker-admin-ui"},{"location":"reference/kubernetes/docker-admin-ui/#versions","text":"See Releases for stable versions. For bleeding-edge/unstable version, use gluufederation/admin-ui:1.0.0_dev .","title":"Versions"},{"location":"reference/kubernetes/docker-admin-ui/#environment-variables","text":"The following environment variables are supported by the container: CN_CONFIG_ADAPTER : The config backend adapter, can be consul (default), kubernetes , or google . CN_CONFIG_CONSUL_HOST : hostname or IP of Consul (default to localhost ). CN_CONFIG_CONSUL_PORT : port of Consul (default to 8500 ). CN_CONFIG_CONSUL_CONSISTENCY : Consul consistency mode (choose one of default , consistent , or stale ). Default to stale mode. CN_CONFIG_CONSUL_SCHEME : supported Consul scheme ( http or https ). CN_CONFIG_CONSUL_VERIFY : whether to verify cert or not (default to false ). CN_CONFIG_CONSUL_CACERT_FILE : path to Consul CA cert file (default to /etc/certs/consul_ca.crt ). This file will be used if it exists and CN_CONFIG_CONSUL_VERIFY set to true . CN_CONFIG_CONSUL_CERT_FILE : path to Consul cert file (default to /etc/certs/consul_client.crt ). CN_CONFIG_CONSUL_KEY_FILE : path to Consul key file (default to /etc/certs/consul_client.key ). CN_CONFIG_CONSUL_TOKEN_FILE : path to file contains ACL token (default to /etc/certs/consul_token ). CN_CONFIG_KUBERNETES_NAMESPACE : Kubernetes namespace (default to default ). CN_CONFIG_KUBERNETES_CONFIGMAP : Kubernetes configmaps name (default to jans ). CN_CONFIG_KUBERNETES_USE_KUBE_CONFIG : Load credentials from $HOME/.kube/config , only useful for non-container environment (default to false ). CN_SECRET_ADAPTER : The secrets' adapter, can be vault (default), kubernetes , or google . CN_SECRET_VAULT_SCHEME : supported Vault scheme ( http or https ). CN_SECRET_VAULT_HOST : hostname or IP of Vault (default to localhost ). CN_SECRET_VAULT_PORT : port of Vault (default to 8200 ). CN_SECRET_VAULT_VERIFY : whether to verify cert or not (default to false ). CN_SECRET_VAULT_ROLE_ID_FILE : path to file contains Vault AppRole role ID (default to /etc/certs/vault_role_id ). CN_SECRET_VAULT_SECRET_ID_FILE : path to file contains Vault AppRole secret ID (default to /etc/certs/vault_secret_id ). CN_SECRET_VAULT_CERT_FILE : path to Vault cert file (default to /etc/certs/vault_client.crt ). CN_SECRET_VAULT_KEY_FILE : path to Vault key file (default to /etc/certs/vault_client.key ). CN_SECRET_VAULT_CACERT_FILE : path to Vault CA cert file (default to /etc/certs/vault_ca.crt ). This file will be used if it exists and CN_SECRET_VAULT_VERIFY set to true . CN_SECRET_KUBERNETES_NAMESPACE : Kubernetes namespace (default to default ). CN_SECRET_KUBERNETES_CONFIGMAP : Kubernetes secrets name (default to jans ). CN_SECRET_KUBERNETES_USE_KUBE_CONFIG : Load credentials from $HOME/.kube/config , only useful for non-container environment (default to false ). CN_WAIT_MAX_TIME : How long the startup \"health checks\" should run (default to 300 seconds). CN_WAIT_SLEEP_DURATION : Delay between startup \"health checks\" (default to 10 seconds). GOOGLE_PROJECT_ID : Google Project ID (default to empty string). Used when CN_CONFIG_ADAPTER or CN_SECRET_ADAPTER set to google . GOOGLE_APPLICATION_CREDENTIALS : Path to Google credentials JSON file (default to /etc/jans/conf/google-credentials.json ). Used when CN_CONFIG_ADAPTER or CN_SECRET_ADAPTER set to google . CN_GOOGLE_SECRET_VERSION_ID : Janssen secret version ID in Google Secret Manager. Defaults to latest , which is recommended. CN_GOOGLE_SECRET_NAME_PREFIX : Prefix for Janssen secret in Google Secret Manager. Defaults to jans . If left jans-secret secret will be created. CN_GOOGLE_SECRET_MANAGER_PASSPHRASE : Passphrase for Janssen secret in Google Secret Manager. This is recommended to be changed and defaults to secret . CN_TOKEN_SERVER_BASE_HOSTNAME : Hostname of token server (default to localhost ). CN_TOKEN_SERVER_AUTHZ_ENDPOINT : Authorization endpoint at token server (default to /jans-auth/authorize.htm ). CN_TOKEN_SERVER_TOKEN_ENDPOINT : Token endpoint at token server (default to /jans-auth/restv1/token ). CN_TOKEN_SERVER_INTROSPECTION_ENDPOINT : Introspection endpoint at token server (default to /jans-auth/restv1/introspection ). CN_TOKEN_SERVER_USERINFO_ENDPOINT : User info endpoint at token server (default to /jans-auth/restv1/userinfo ). CN_TOKEN_SERVER_CLIENT_ID : Client ID registered at token server. CN_TOKEN_SERVER_CERT_FILE : Path to token server certificate (default to /etc/certs/token_server.crt ). CN_PERSISTENCE_TYPE : Persistence backend being used (one of ldap , couchbase , or hybrid ; default to ldap ). CN_HYBRID_MAPPING : Specify data mapping for each persistence (default to \"{}\" ). Note this environment only takes effect when CN_PERSISTENCE_TYPE is set to hybrid . See hybrid mapping section for details. CN_LDAP_URL : Address and port of LDAP server (default to localhost:1636 ). CN_LDAP_USE_SSL : Whether to use SSL connection to LDAP server (default to true ). CN_COUCHBASE_URL : Address of Couchbase server (default to localhost ). CN_COUCHBASE_USER : Username of Couchbase server (default to admin ). CN_COUCHBASE_CERT_FILE : Couchbase root certificate location (default to /etc/certs/couchbase.crt ). CN_COUCHBASE_PASSWORD_FILE : Path to file contains Couchbase password (default to /etc/jans/conf/couchbase_password ). CN_COUCHBASE_CONN_TIMEOUT : Connect timeout used when a bucket is opened (default to 10000 milliseconds). CN_COUCHBASE_CONN_MAX_WAIT : Maximum time to wait before retrying connection (default to 20000 milliseconds). CN_COUCHBASE_SCAN_CONSISTENCY : Default scan consistency; one of not_bounded , request_plus , or statement_plus (default to not_bounded ). CN_COUCHBASE_BUCKET_PREFIX : Prefix for Couchbase buckets (default to jans ). CN_COUCHBASE_TRUSTSTORE_ENABLE : Enable truststore for encrypted Couchbase connection (default to true ). CN_COUCHBASE_KEEPALIVE_INTERVAL : Keep-alive interval for Couchbase connection (default to 30000 milliseconds). CN_COUCHBASE_KEEPALIVE_TIMEOUT : Keep-alive timeout for Couchbase connection (default to 2500 milliseconds). CN_SQL_DB_DIALECT : Dialect name of SQL backend (one of mysql , pgsql ; default to mysql ). CN_SQL_DB_HOST : Host of SQL backend (default to localhost ). CN_SQL_DB_PORT : Port of SQL backend (default to 3306 ). CN_SQL_DB_NAME : Database name (default to jans ) CN_SQL_DB_USER : Username to interact with SQL backend (default to jans ). CN_GOOGLE_SPANNER_INSTANCE_ID : Instance ID of Google Spanner (default to empty string). CN_GOOGLE_SPANNER_DATABASE_ID : Database ID of Google Spanner (default to empty string). GOOGLE_APPLICATION_CREDENTIALS : Path to Google credentials JSON file (default to /etc/jans/conf/google-credentials.json ). GOOGLE_PROJECT_ID : Google Project ID (default to empty string). GOOGLE_PROJECT_ID : Google Project ID (default to empty string). Used when CN_CONFIG_ADAPTER or CN_SECRET_ADAPTER set to google . GOOGLE_APPLICATION_CREDENTIALS : Path to Google credentials JSON file (default to /etc/jans/conf/google-credentials.json ). Used when CN_CONFIG_ADAPTER or CN_SECRET_ADAPTER set to google . CN_GOOGLE_SPANNER_INSTANCE_ID : Google Spanner instance ID. CN_GOOGLE_SPANNER_DATABASE_ID : Google Spanner database ID. GLUU_ADMIN_UI_PLUGINS : Comma-separated additional plugins to be enabled (default to empty string). See Adding plugins for details. GLUU_ADMIN_UI_AUTH_METHOD : Authentication method for admin-ui (one of basic or casa ; default to basic ). Note, changing the value require restart to jans-config-api.","title":"Environment Variables"},{"location":"reference/kubernetes/docker-admin-ui/#hybrid-mapping","text":"Hybrid persistence supports all available persistence types. To configure hybrid persistence and its data mapping, follow steps below: Set CN_PERSISTENCE_TYPE environment variable to hybrid Set CN_HYBRID_MAPPING with the following format: { \"default\": \"<couchbase|ldap|spanner|sql>\", \"user\": \"<couchbase|ldap|spanner|sql>\", \"site\": \"<couchbase|ldap|spanner|sql>\", \"cache\": \"<couchbase|ldap|spanner|sql>\", \"token\": \"<couchbase|ldap|spanner|sql>\", \"session\": \"<couchbase|ldap|spanner|sql>\", } Example: { \"default\": \"sql\", \"user\": \"spanner\", \"site\": \"ldap\", \"cache\": \"sql\", \"token\": \"couchbase\", \"session\": \"spanner\", }","title":"Hybrid mapping"},{"location":"reference/kubernetes/docker-admin-ui/#adding-plugins","text":"To add plugins to AdminUI, for example myplugin.zip Set the name of the plugin (without the extension name) in environment variable GLUU_ADMIN_UI_PLUGINS , for example: GLUU_ADMIN_UI_PLUGINS=myplugin . Mount myplugin.zip to /app/plugins/myplugin.zip inside the pod/container. Note that if /app/plugins/myplugin.zip is not exist, plugin will be ignored.","title":"Adding plugins"},{"location":"reference/kubernetes/docker-casa/","tags":["administration","reference","kubernetes","docker image"],"text":"Overview # Docker assets for Casa Versions # See Releases for stable versions. For bleeding-edge/unstable version, use gluufederation/casa:5.0.0_dev . Environment Variables # The following environment variables are supported by the container: CN_CONFIG_ADAPTER : The config backend adapter, can be consul (default) or kubernetes . CN_CONFIG_CONSUL_HOST : hostname or IP of Consul (default to localhost ). CN_CONFIG_CONSUL_PORT : port of Consul (default to 8500 ). CN_CONFIG_CONSUL_CONSISTENCY : Consul consistency mode (choose one of default , consistent , or stale ). Default to stale mode. CN_CONFIG_CONSUL_SCHEME : supported Consul scheme ( http or https ). CN_CONFIG_CONSUL_VERIFY : whether to verify cert or not (default to false ). CN_CONFIG_CONSUL_CACERT_FILE : path to Consul CA cert file (default to /etc/certs/consul_ca.crt ). This file will be used if it exists and CN_CONFIG_CONSUL_VERIFY set to true . CN_CONFIG_CONSUL_CERT_FILE : path to Consul cert file (default to /etc/certs/consul_client.crt ). CN_CONFIG_CONSUL_KEY_FILE : path to Consul key file (default to /etc/certs/consul_client.key ). CN_CONFIG_CONSUL_TOKEN_FILE : path to file contains ACL token (default to /etc/certs/consul_token ). CN_CONFIG_KUBERNETES_NAMESPACE : Kubernetes namespace (default to default ). CN_CONFIG_KUBERNETES_CONFIGMAP : Kubernetes configmaps name (default to jans ). CN_CONFIG_KUBERNETES_USE_KUBE_CONFIG : Load credentials from $HOME/.kube/config , only useful for non-container environment (default to false ). CN_SECRET_ADAPTER : The secrets' adapter, can be vault or kubernetes . CN_SECRET_VAULT_SCHEME : supported Vault scheme ( http or https ). CN_SECRET_VAULT_HOST : hostname or IP of Vault (default to localhost ). CN_SECRET_VAULT_PORT : port of Vault (default to 8200 ). CN_SECRET_VAULT_VERIFY : whether to verify cert or not (default to false ). CN_SECRET_VAULT_ROLE_ID_FILE : path to file contains Vault AppRole role ID (default to /etc/certs/vault_role_id ). CN_SECRET_VAULT_SECRET_ID_FILE : path to file contains Vault AppRole secret ID (default to /etc/certs/vault_secret_id ). CN_SECRET_VAULT_CERT_FILE : path to Vault cert file (default to /etc/certs/vault_client.crt ). CN_SECRET_VAULT_KEY_FILE : path to Vault key file (default to /etc/certs/vault_client.key ). CN_SECRET_VAULT_CACERT_FILE : path to Vault CA cert file (default to /etc/certs/vault_ca.crt ). This file will be used if it exists and CN_SECRET_VAULT_VERIFY set to true . CN_SECRET_KUBERNETES_NAMESPACE : Kubernetes namespace (default to default ). CN_SECRET_KUBERNETES_CONFIGMAP : Kubernetes secrets name (default to jans ). CN_SECRET_KUBERNETES_USE_KUBE_CONFIG : Load credentials from $HOME/.kube/config , only useful for non-container environment (default to false ). CN_WAIT_MAX_TIME : How long the startup \"health checks\" should run (default to 300 seconds). CN_WAIT_SLEEP_DURATION : Delay between startup \"health checks\" (default to 10 seconds). CN_MAX_RAM_PERCENTAGE : Value passed to Java option -XX:MaxRAMPercentage . CN_PERSISTENCE_TYPE : Persistence backend being used (one of ldap , couchbase , or hybrid ; default to ldap ). CN_HYBRID_MAPPING : Specify data mapping for each persistence (default to \"{}\" ). Note this environment only takes effect when CN_PERSISTENCE_TYPE is set to hybrid . See hybrid mapping section for details. CN_LDAP_URL : Address and port of LDAP server (default to localhost:1636 ); required if CN_PERSISTENCE_TYPE is set to ldap or hybrid . CN_LDAP_USE_SSL : Whether to use SSL connection to LDAP server (default to true ). CN_COUCHBASE_URL : Address of Couchbase server (default to localhost ); required if CN_PERSISTENCE_TYPE is set to couchbase or hybrid . CN_COUCHBASE_USER : Username of Couchbase server (default to admin ); required if CN_PERSISTENCE_TYPE is set to couchbase or hybrid . CN_COUCHBASE_CERT_FILE : Couchbase root certificate location (default to /etc/certs/couchbase.crt ); required if CN_PERSISTENCE_TYPE is set to couchbase or hybrid . CN_COUCHBASE_CONN_TIMEOUT : Connect timeout used when a bucket is opened (default to 10000 milliseconds). CN_COUCHBASE_CONN_MAX_WAIT : Maximum time to wait before retrying connection (default to 20000 milliseconds). CN_COUCHBASE_SCAN_CONSISTENCY : Default scan consistency; one of not_bounded , request_plus , or statement_plus (default to not_bounded ). CN_COUCHBASE_BUCKET_PREFIX : Prefix for Couchbase buckets (default to jans ). CN_COUCHBASE_TRUSTSTORE_ENABLE : Enable truststore for encrypted Couchbase connection (default to true ). CN_COUCHBASE_KEEPALIVE_INTERVAL : Keep-alive interval for Couchbase connection (default to 30000 milliseconds). CN_COUCHBASE_KEEPALIVE_TIMEOUT : Keep-alive timeout for Couchbase connection (default to 2500 milliseconds). CN_JAVA_OPTIONS : Java options passed to entrypoint, i.e. -Xmx1024m (default to empty-string). CN_DOCUMENT_STORE_TYPE : Document store type (one of LOCAL or JCA ; default to LOCAL ). CN_JACKRABBIT_URL : URL to remote repository (default to http://localhost:8080 ). CN_JACKRABBIT_SYNC_INTERVAL : Interval between files sync (default to 300 seconds). CN_JACKRABBIT_ADMIN_ID : Admin username (default to admin ). CN_JACKRABBIT_ADMIN_PASSWORD_FILE : Absolute path to file contains password for admin user (default to /etc/jans/conf/jackrabbit_admin_password ). CN_SSL_CERT_FROM_SECRETS : Determine whether to get SSL cert from secrets backend (default to false ). Note that the flag will take effect only if there's no mounted /etc/certs/web_https.crt file. CN_SQL_DB_DIALECT : Dialect name of SQL backend (one of mysql , pgsql ; default to mysql ). CN_SQL_DB_HOST : Host of SQL backend (default to localhost ). CN_SQL_DB_PORT : Port of SQL backend (default to 3306 ). CN_SQL_DB_NAME : Database name (default to jans ) CN_SQL_DB_USER : Username to interact with SQL backend (default to jans ). CN_GOOGLE_SPANNER_INSTANCE_ID : Instance ID of Google Spanner (default to empty string). CN_GOOGLE_SPANNER_DATABASE_ID : Database ID of Google Spanner (default to empty string). GOOGLE_APPLICATION_CREDENTIALS : JSON file (contains Google credentials) that should be injected into container. GOOGLE_PROJECT_ID : ID of Google project. CN_GOOGLE_SECRET_VERSION_ID : Janssen secret version ID in Google Secret Manager. Defaults to latest , which is recommended. CN_GOOGLE_SECRET_NAME_PREFIX : Prefix for Janssen secret in Google Secret Manager. Defaults to jans . If left jans-secret secret will be created. CN_GOOGLE_SECRET_MANAGER_PASSPHRASE : Passphrase for Janssen secret in Google Secret Manager. This is recommended to be changed and defaults to secret . GLUU_CASA_APP_LOGGERS : Custom logging configuration in JSON-string format with hash type (see Configure app loggers section for details). GLUU_CASA_ADMIN_LOCK_FILE : Path to lock file to enable/disable administration feature (default to /opt/jans/jetty/casa/resources/.administrable ). If file is not exist, the feature is disabled. CN_PROMETHEUS_PORT : Port used by Prometheus JMX agent (default to empty string). To enable Prometheus JMX agent, set the value to a number. See Exposing metrics for details. Configure app loggers # App loggers can be configured to define where the logs will be redirected and what is the level the logs should be displayed. Supported redirect target: STDOUT FILE Supported level: OFF FATAL ERROR WARN INFO DEBUG TRACE The following key-value pairs are the defaults: { \"casa_log_target\" : \"STDOUT\" , \"casa_log_level\" : \"INFO\" , \"timer_log_target\" : \"FILE\" , \"timer_log_level\" : \"INFO\" } To enable prefix on STDOUT logging, set the enable_stdout_log_prefix key. Example: {\"casa_log_target\":\"STDOUT\",\"timer_log_target\":\"STDOUT\",\"enable_stdout_log_prefix\":true} Exposing metrics # As per v1.0.1, certain metrics can be exposed via Prometheus JMX exporter. To expose the metrics, set the CN_PROMETHEUS_PORT environment variable, i.e. CN_PROMETHEUS_PORT=9093 . Afterwards, metrics can be scraped by Prometheus or accessed manually by making request to /metrics URL, i.e. http://container:9093/metrics . Note that Prometheus JMX exporter uses pre-defined config file (see conf/prometheus-config.yaml ). To customize the config, mount custom config file to /opt/prometheus/prometheus-config.yaml inside the container. Hybrid mapping # Hybrid persistence supports all available persistence types. To configure hybrid persistence and its data mapping, follow steps below: Set CN_PERSISTENCE_TYPE environment variable to hybrid Set CN_HYBRID_MAPPING with the following format: { \"default\": \"<couchbase|ldap|spanner|sql>\", \"user\": \"<couchbase|ldap|spanner|sql>\", \"site\": \"<couchbase|ldap|spanner|sql>\", \"cache\": \"<couchbase|ldap|spanner|sql>\", \"token\": \"<couchbase|ldap|spanner|sql>\", \"session\": \"<couchbase|ldap|spanner|sql>\", } Example: { \"default\": \"sql\", \"user\": \"spanner\", \"site\": \"ldap\", \"cache\": \"sql\", \"token\": \"couchbase\", \"session\": \"spanner\", }","title":"Docker casa"},{"location":"reference/kubernetes/docker-casa/#overview","text":"Docker assets for Casa","title":"Overview"},{"location":"reference/kubernetes/docker-casa/#versions","text":"See Releases for stable versions. For bleeding-edge/unstable version, use gluufederation/casa:5.0.0_dev .","title":"Versions"},{"location":"reference/kubernetes/docker-casa/#environment-variables","text":"The following environment variables are supported by the container: CN_CONFIG_ADAPTER : The config backend adapter, can be consul (default) or kubernetes . CN_CONFIG_CONSUL_HOST : hostname or IP of Consul (default to localhost ). CN_CONFIG_CONSUL_PORT : port of Consul (default to 8500 ). CN_CONFIG_CONSUL_CONSISTENCY : Consul consistency mode (choose one of default , consistent , or stale ). Default to stale mode. CN_CONFIG_CONSUL_SCHEME : supported Consul scheme ( http or https ). CN_CONFIG_CONSUL_VERIFY : whether to verify cert or not (default to false ). CN_CONFIG_CONSUL_CACERT_FILE : path to Consul CA cert file (default to /etc/certs/consul_ca.crt ). This file will be used if it exists and CN_CONFIG_CONSUL_VERIFY set to true . CN_CONFIG_CONSUL_CERT_FILE : path to Consul cert file (default to /etc/certs/consul_client.crt ). CN_CONFIG_CONSUL_KEY_FILE : path to Consul key file (default to /etc/certs/consul_client.key ). CN_CONFIG_CONSUL_TOKEN_FILE : path to file contains ACL token (default to /etc/certs/consul_token ). CN_CONFIG_KUBERNETES_NAMESPACE : Kubernetes namespace (default to default ). CN_CONFIG_KUBERNETES_CONFIGMAP : Kubernetes configmaps name (default to jans ). CN_CONFIG_KUBERNETES_USE_KUBE_CONFIG : Load credentials from $HOME/.kube/config , only useful for non-container environment (default to false ). CN_SECRET_ADAPTER : The secrets' adapter, can be vault or kubernetes . CN_SECRET_VAULT_SCHEME : supported Vault scheme ( http or https ). CN_SECRET_VAULT_HOST : hostname or IP of Vault (default to localhost ). CN_SECRET_VAULT_PORT : port of Vault (default to 8200 ). CN_SECRET_VAULT_VERIFY : whether to verify cert or not (default to false ). CN_SECRET_VAULT_ROLE_ID_FILE : path to file contains Vault AppRole role ID (default to /etc/certs/vault_role_id ). CN_SECRET_VAULT_SECRET_ID_FILE : path to file contains Vault AppRole secret ID (default to /etc/certs/vault_secret_id ). CN_SECRET_VAULT_CERT_FILE : path to Vault cert file (default to /etc/certs/vault_client.crt ). CN_SECRET_VAULT_KEY_FILE : path to Vault key file (default to /etc/certs/vault_client.key ). CN_SECRET_VAULT_CACERT_FILE : path to Vault CA cert file (default to /etc/certs/vault_ca.crt ). This file will be used if it exists and CN_SECRET_VAULT_VERIFY set to true . CN_SECRET_KUBERNETES_NAMESPACE : Kubernetes namespace (default to default ). CN_SECRET_KUBERNETES_CONFIGMAP : Kubernetes secrets name (default to jans ). CN_SECRET_KUBERNETES_USE_KUBE_CONFIG : Load credentials from $HOME/.kube/config , only useful for non-container environment (default to false ). CN_WAIT_MAX_TIME : How long the startup \"health checks\" should run (default to 300 seconds). CN_WAIT_SLEEP_DURATION : Delay between startup \"health checks\" (default to 10 seconds). CN_MAX_RAM_PERCENTAGE : Value passed to Java option -XX:MaxRAMPercentage . CN_PERSISTENCE_TYPE : Persistence backend being used (one of ldap , couchbase , or hybrid ; default to ldap ). CN_HYBRID_MAPPING : Specify data mapping for each persistence (default to \"{}\" ). Note this environment only takes effect when CN_PERSISTENCE_TYPE is set to hybrid . See hybrid mapping section for details. CN_LDAP_URL : Address and port of LDAP server (default to localhost:1636 ); required if CN_PERSISTENCE_TYPE is set to ldap or hybrid . CN_LDAP_USE_SSL : Whether to use SSL connection to LDAP server (default to true ). CN_COUCHBASE_URL : Address of Couchbase server (default to localhost ); required if CN_PERSISTENCE_TYPE is set to couchbase or hybrid . CN_COUCHBASE_USER : Username of Couchbase server (default to admin ); required if CN_PERSISTENCE_TYPE is set to couchbase or hybrid . CN_COUCHBASE_CERT_FILE : Couchbase root certificate location (default to /etc/certs/couchbase.crt ); required if CN_PERSISTENCE_TYPE is set to couchbase or hybrid . CN_COUCHBASE_CONN_TIMEOUT : Connect timeout used when a bucket is opened (default to 10000 milliseconds). CN_COUCHBASE_CONN_MAX_WAIT : Maximum time to wait before retrying connection (default to 20000 milliseconds). CN_COUCHBASE_SCAN_CONSISTENCY : Default scan consistency; one of not_bounded , request_plus , or statement_plus (default to not_bounded ). CN_COUCHBASE_BUCKET_PREFIX : Prefix for Couchbase buckets (default to jans ). CN_COUCHBASE_TRUSTSTORE_ENABLE : Enable truststore for encrypted Couchbase connection (default to true ). CN_COUCHBASE_KEEPALIVE_INTERVAL : Keep-alive interval for Couchbase connection (default to 30000 milliseconds). CN_COUCHBASE_KEEPALIVE_TIMEOUT : Keep-alive timeout for Couchbase connection (default to 2500 milliseconds). CN_JAVA_OPTIONS : Java options passed to entrypoint, i.e. -Xmx1024m (default to empty-string). CN_DOCUMENT_STORE_TYPE : Document store type (one of LOCAL or JCA ; default to LOCAL ). CN_JACKRABBIT_URL : URL to remote repository (default to http://localhost:8080 ). CN_JACKRABBIT_SYNC_INTERVAL : Interval between files sync (default to 300 seconds). CN_JACKRABBIT_ADMIN_ID : Admin username (default to admin ). CN_JACKRABBIT_ADMIN_PASSWORD_FILE : Absolute path to file contains password for admin user (default to /etc/jans/conf/jackrabbit_admin_password ). CN_SSL_CERT_FROM_SECRETS : Determine whether to get SSL cert from secrets backend (default to false ). Note that the flag will take effect only if there's no mounted /etc/certs/web_https.crt file. CN_SQL_DB_DIALECT : Dialect name of SQL backend (one of mysql , pgsql ; default to mysql ). CN_SQL_DB_HOST : Host of SQL backend (default to localhost ). CN_SQL_DB_PORT : Port of SQL backend (default to 3306 ). CN_SQL_DB_NAME : Database name (default to jans ) CN_SQL_DB_USER : Username to interact with SQL backend (default to jans ). CN_GOOGLE_SPANNER_INSTANCE_ID : Instance ID of Google Spanner (default to empty string). CN_GOOGLE_SPANNER_DATABASE_ID : Database ID of Google Spanner (default to empty string). GOOGLE_APPLICATION_CREDENTIALS : JSON file (contains Google credentials) that should be injected into container. GOOGLE_PROJECT_ID : ID of Google project. CN_GOOGLE_SECRET_VERSION_ID : Janssen secret version ID in Google Secret Manager. Defaults to latest , which is recommended. CN_GOOGLE_SECRET_NAME_PREFIX : Prefix for Janssen secret in Google Secret Manager. Defaults to jans . If left jans-secret secret will be created. CN_GOOGLE_SECRET_MANAGER_PASSPHRASE : Passphrase for Janssen secret in Google Secret Manager. This is recommended to be changed and defaults to secret . GLUU_CASA_APP_LOGGERS : Custom logging configuration in JSON-string format with hash type (see Configure app loggers section for details). GLUU_CASA_ADMIN_LOCK_FILE : Path to lock file to enable/disable administration feature (default to /opt/jans/jetty/casa/resources/.administrable ). If file is not exist, the feature is disabled. CN_PROMETHEUS_PORT : Port used by Prometheus JMX agent (default to empty string). To enable Prometheus JMX agent, set the value to a number. See Exposing metrics for details.","title":"Environment Variables"},{"location":"reference/kubernetes/docker-casa/#configure-app-loggers","text":"App loggers can be configured to define where the logs will be redirected and what is the level the logs should be displayed. Supported redirect target: STDOUT FILE Supported level: OFF FATAL ERROR WARN INFO DEBUG TRACE The following key-value pairs are the defaults: { \"casa_log_target\" : \"STDOUT\" , \"casa_log_level\" : \"INFO\" , \"timer_log_target\" : \"FILE\" , \"timer_log_level\" : \"INFO\" } To enable prefix on STDOUT logging, set the enable_stdout_log_prefix key. Example: {\"casa_log_target\":\"STDOUT\",\"timer_log_target\":\"STDOUT\",\"enable_stdout_log_prefix\":true}","title":"Configure app loggers"},{"location":"reference/kubernetes/docker-casa/#exposing-metrics","text":"As per v1.0.1, certain metrics can be exposed via Prometheus JMX exporter. To expose the metrics, set the CN_PROMETHEUS_PORT environment variable, i.e. CN_PROMETHEUS_PORT=9093 . Afterwards, metrics can be scraped by Prometheus or accessed manually by making request to /metrics URL, i.e. http://container:9093/metrics . Note that Prometheus JMX exporter uses pre-defined config file (see conf/prometheus-config.yaml ). To customize the config, mount custom config file to /opt/prometheus/prometheus-config.yaml inside the container.","title":"Exposing metrics"},{"location":"reference/kubernetes/docker-casa/#hybrid-mapping","text":"Hybrid persistence supports all available persistence types. To configure hybrid persistence and its data mapping, follow steps below: Set CN_PERSISTENCE_TYPE environment variable to hybrid Set CN_HYBRID_MAPPING with the following format: { \"default\": \"<couchbase|ldap|spanner|sql>\", \"user\": \"<couchbase|ldap|spanner|sql>\", \"site\": \"<couchbase|ldap|spanner|sql>\", \"cache\": \"<couchbase|ldap|spanner|sql>\", \"token\": \"<couchbase|ldap|spanner|sql>\", \"session\": \"<couchbase|ldap|spanner|sql>\", } Example: { \"default\": \"sql\", \"user\": \"spanner\", \"site\": \"ldap\", \"cache\": \"sql\", \"token\": \"couchbase\", \"session\": \"spanner\", }","title":"Hybrid mapping"},{"location":"reference/kubernetes/docker-flex-monolith/","tags":["administration","reference","kubernetes","docker image"],"text":"Overview # This image is for testing and development purposes only! Use Flex helm charts for production setups Docker monolith image packaging for Gluu Flex.This image packs janssen services including, the auth-server, config-api, fido2, and scim and the Gluu admin ui and Casa. Versions # See Releases for stable versions. This image should never be used in production. For bleeding-edge/unstable version, use gluufederation/monolith:5.0.0_dev . Environment Variables # The following environment variables are supported by the container: ENV Description Default CN_HOSTNAME Hostname to install gluu with. demoexample.gluu.org CN_ADMIN_PASS Password of the admin user. 1t5Fin3#security CN_ORG_NAME Organization name. Used for ssl cert generation. Gluu CN_EMAIL Email. Used for ssl cert generation. support@gluu.org CN_CITY City. Used for ssl cert generation. Austin CN_STATE State. Used for ssl cert generation TX CN_COUNTRY Country. Used for ssl cert generation. US IS_FQDN_REGISTERED If a DNS record has been added for the docker vm. false CN_INSTALL_LDAP NOT SUPPORTED YET false CN_INSTALL_CONFIG_API Installs the Config API service. true CN_INSTALL_SCIM Installs the SCIM API service. true CN_INSTALL_FIDO2 Installs the FIDO2 API service. true CN_INSTALL_CASA Installs the Casa service. true CN_INSTALL_ADMIN_UI Installs the Admin UI service. true MYSQL_DATABASE MySQL gluu flex database. gluu MYSQL_USER MySQL database user. gluu MYSQL_PASSWORD MySQL database user password. 1t5Fin3#security MYSQL_HOST MySQL host. mysql which is the docker compose service name Pre-requisites # Clone this repository and cd into the docker-flex-monolith folder Docker . Docker compose should be installed by default with Docker. How to run # docker compose -f flex-mysql-compose.yml up -d Clean up # Remove setup and volumes docker compose -f mysql-docker-compose.yml down && rm -rf jans-* Test # docker exec -ti docker-flex-monolith-flex-1 bash Run /opt/jans/jans-cli/config-cli.py #or /opt/jans/jans-cli/scim-cli.py Access endpoints externally # Add to your /etc/hosts file the ip domain record which should be the ip of the instance docker is installed at and the domain used in the env above CN_HOSTNAME . # For-example 172 .22.0.3 demoexample.gluu.org After adding the record you can hit endpoints such as https://demoexample.gluu.org/.well-known/openid-configuration Quick start # Grab a fresh ubuntu 22.04 lts VM and run: wget https://raw.githubusercontent.com/GluuFederation/flex/main/automation/startflexmonolithdemo.sh && chmod u+x startflexmonolithdemo.sh && sudo bash startflexmonolithdemo.sh demoexample.gluu.org MYSQL","title":"Overview"},{"location":"reference/kubernetes/docker-flex-monolith/#overview","text":"This image is for testing and development purposes only! Use Flex helm charts for production setups Docker monolith image packaging for Gluu Flex.This image packs janssen services including, the auth-server, config-api, fido2, and scim and the Gluu admin ui and Casa.","title":"Overview"},{"location":"reference/kubernetes/docker-flex-monolith/#versions","text":"See Releases for stable versions. This image should never be used in production. For bleeding-edge/unstable version, use gluufederation/monolith:5.0.0_dev .","title":"Versions"},{"location":"reference/kubernetes/docker-flex-monolith/#environment-variables","text":"The following environment variables are supported by the container: ENV Description Default CN_HOSTNAME Hostname to install gluu with. demoexample.gluu.org CN_ADMIN_PASS Password of the admin user. 1t5Fin3#security CN_ORG_NAME Organization name. Used for ssl cert generation. Gluu CN_EMAIL Email. Used for ssl cert generation. support@gluu.org CN_CITY City. Used for ssl cert generation. Austin CN_STATE State. Used for ssl cert generation TX CN_COUNTRY Country. Used for ssl cert generation. US IS_FQDN_REGISTERED If a DNS record has been added for the docker vm. false CN_INSTALL_LDAP NOT SUPPORTED YET false CN_INSTALL_CONFIG_API Installs the Config API service. true CN_INSTALL_SCIM Installs the SCIM API service. true CN_INSTALL_FIDO2 Installs the FIDO2 API service. true CN_INSTALL_CASA Installs the Casa service. true CN_INSTALL_ADMIN_UI Installs the Admin UI service. true MYSQL_DATABASE MySQL gluu flex database. gluu MYSQL_USER MySQL database user. gluu MYSQL_PASSWORD MySQL database user password. 1t5Fin3#security MYSQL_HOST MySQL host. mysql which is the docker compose service name","title":"Environment Variables"},{"location":"reference/kubernetes/docker-flex-monolith/#pre-requisites","text":"Clone this repository and cd into the docker-flex-monolith folder Docker . Docker compose should be installed by default with Docker.","title":"Pre-requisites"},{"location":"reference/kubernetes/docker-flex-monolith/#how-to-run","text":"docker compose -f flex-mysql-compose.yml up -d","title":"How to run"},{"location":"reference/kubernetes/docker-flex-monolith/#clean-up","text":"Remove setup and volumes docker compose -f mysql-docker-compose.yml down && rm -rf jans-*","title":"Clean up"},{"location":"reference/kubernetes/docker-flex-monolith/#test","text":"docker exec -ti docker-flex-monolith-flex-1 bash Run /opt/jans/jans-cli/config-cli.py #or /opt/jans/jans-cli/scim-cli.py","title":"Test"},{"location":"reference/kubernetes/docker-flex-monolith/#access-endpoints-externally","text":"Add to your /etc/hosts file the ip domain record which should be the ip of the instance docker is installed at and the domain used in the env above CN_HOSTNAME . # For-example 172 .22.0.3 demoexample.gluu.org After adding the record you can hit endpoints such as https://demoexample.gluu.org/.well-known/openid-configuration","title":"Access endpoints externally"},{"location":"reference/kubernetes/docker-flex-monolith/#quick-start","text":"Grab a fresh ubuntu 22.04 lts VM and run: wget https://raw.githubusercontent.com/GluuFederation/flex/main/automation/startflexmonolithdemo.sh && chmod u+x startflexmonolithdemo.sh && sudo bash startflexmonolithdemo.sh demoexample.gluu.org MYSQL","title":"Quick start"},{"location":"reference/kubernetes/helm-chart/","tags":["administration","reference","kubernetes"],"text":"gluu # Gluu Access and Identity Management Homepage: https://www.gluu.org Maintainers # Name Email Url moabu support@gluu.org Source Code # https://gluu.org/docs/gluu-server https://github.com/GluuFederation/flex/flex-cn-setup Requirements # Kubernetes: >=v1.21.0-0 Repository Name Version admin-ui 5.0.10 auth-server 5.0.10 auth-server-key-rotation 5.0.10 casa 5.0.10 cn-istio-ingress 5.0.10 config 5.0.10 config-api 5.0.10 fido2 5.0.10 nginx-ingress 5.0.10 opendj 5.0.10 oxpassport 5.0.10 oxshibboleth 5.0.10 persistence 5.0.10 scim 5.0.10 Values # Key Type Default Description admin-ui object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"hpa\":{\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"gluufederation/admin-ui\",\"tag\":\"1.0.5-1\"},\"livenessProbe\":{\"failureThreshold\":20,\"initialDelaySeconds\":60,\"periodSeconds\":25,\"tcpSocket\":{\"port\":8080},\"timeoutSeconds\":5},\"pdb\":{\"enabled\":true,\"maxUnavailable\":\"90%\"},\"readinessProbe\":{\"failureThreshold\":20,\"initialDelaySeconds\":60,\"periodSeconds\":25,\"tcpSocket\":{\"port\":8080},\"timeoutSeconds\":5},\"replicas\":1,\"resources\":{\"limits\":{\"cpu\":\"2500m\",\"memory\":\"2500Mi\"},\"requests\":{\"cpu\":\"2500m\",\"memory\":\"2500Mi\"}},\"topologySpreadConstraints\":{},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} Admin GUI for configuration of the auth-server admin-ui.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of admin-ui.additionalLabels object {} Additional labels that will be added across the gateway in the format of admin-ui.dnsConfig object {} Add custom dns config admin-ui.dnsPolicy string \"\" Add custom dns policy admin-ui.hpa object {\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50} Configure the HorizontalPodAutoscaler admin-ui.hpa.behavior object {} Scaling Policies admin-ui.hpa.metrics list [] metrics if targetCPUUtilizationPercentage is not set admin-ui.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. admin-ui.image.pullSecrets list [] Image Pull Secrets admin-ui.image.repository string \"gluufederation/admin-ui\" Image to use for deploying. admin-ui.image.tag string \"1.0.5-1\" Image tag to use for deploying. admin-ui.livenessProbe object {\"failureThreshold\":20,\"initialDelaySeconds\":60,\"periodSeconds\":25,\"tcpSocket\":{\"port\":8080},\"timeoutSeconds\":5} Configure the liveness healthcheck for the admin ui if needed. admin-ui.pdb object {\"enabled\":true,\"maxUnavailable\":\"90%\"} Configure the PodDisruptionBudget admin-ui.readinessProbe object {\"failureThreshold\":20,\"initialDelaySeconds\":60,\"periodSeconds\":25,\"tcpSocket\":{\"port\":8080},\"timeoutSeconds\":5} Configure the readiness healthcheck for the admin ui if needed. admin-ui.replicas int 1 Service replica number. admin-ui.resources object {\"limits\":{\"cpu\":\"2500m\",\"memory\":\"2500Mi\"},\"requests\":{\"cpu\":\"2500m\",\"memory\":\"2500Mi\"}} Resource specs. admin-ui.resources.limits.cpu string \"2500m\" CPU limit. admin-ui.resources.limits.memory string \"2500Mi\" Memory limit. admin-ui.resources.requests.cpu string \"2500m\" CPU request. admin-ui.resources.requests.memory string \"2500Mi\" Memory request. admin-ui.topologySpreadConstraints object {} Configure the topology spread constraints. Notice this is a map NOT a list as in the upstream API https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ admin-ui.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service admin-ui.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 admin-ui.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 admin-ui.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers admin-ui.volumes list [] Configure any additional volumes that need to be attached to the pod auth-server object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"hpa\":{\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"janssenproject/auth-server\",\"tag\":\"1.0.5-1\"},\"livenessProbe\":{\"exec\":{\"command\":[\"python3\",\"/app/scripts/healthcheck.py\"]},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5},\"pdb\":{\"enabled\":true,\"maxUnavailable\":\"90%\"},\"readinessProbe\":{\"exec\":{\"command\":[\"python3\",\"/app/scripts/healthcheck.py\"]},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5},\"replicas\":1,\"resources\":{\"limits\":{\"cpu\":\"2500m\",\"memory\":\"2500Mi\"},\"requests\":{\"cpu\":\"2500m\",\"memory\":\"2500Mi\"}},\"topologySpreadConstraints\":{},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} OAuth Authorization Server, the OpenID Connect Provider, the UMA Authorization Server--this is the main Internet facing component of Gluu. It's the service that returns tokens, JWT's and identity assertions. This service must be Internet facing. auth-server-key-rotation object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"janssenproject/certmanager\",\"tag\":\"1.0.5-1\"},\"keysLife\":48,\"resources\":{\"limits\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"},\"requests\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"}},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} Responsible for regenerating auth-keys per x hours auth-server-key-rotation.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of auth-server-key-rotation.additionalLabels object {} Additional labels that will be added across the gateway in the format of auth-server-key-rotation.dnsConfig object {} Add custom dns config auth-server-key-rotation.dnsPolicy string \"\" Add custom dns policy auth-server-key-rotation.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. auth-server-key-rotation.image.pullSecrets list [] Image Pull Secrets auth-server-key-rotation.image.repository string \"janssenproject/certmanager\" Image to use for deploying. auth-server-key-rotation.image.tag string \"1.0.5-1\" Image tag to use for deploying. auth-server-key-rotation.keysLife int 48 Auth server key rotation keys life in hours auth-server-key-rotation.resources object {\"limits\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"},\"requests\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"}} Resource specs. auth-server-key-rotation.resources.limits.cpu string \"300m\" CPU limit. auth-server-key-rotation.resources.limits.memory string \"300Mi\" Memory limit. auth-server-key-rotation.resources.requests.cpu string \"300m\" CPU request. auth-server-key-rotation.resources.requests.memory string \"300Mi\" Memory request. auth-server-key-rotation.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service auth-server-key-rotation.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 auth-server-key-rotation.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 auth-server-key-rotation.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers auth-server-key-rotation.volumes list [] Configure any additional volumes that need to be attached to the pod auth-server.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of auth-server.additionalLabels object {} Additional labels that will be added across the gateway in the format of auth-server.dnsConfig object {} Add custom dns config auth-server.dnsPolicy string \"\" Add custom dns policy auth-server.hpa object {\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50} Configure the HorizontalPodAutoscaler auth-server.hpa.behavior object {} Scaling Policies auth-server.hpa.metrics list [] metrics if targetCPUUtilizationPercentage is not set auth-server.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. auth-server.image.pullSecrets list [] Image Pull Secrets auth-server.image.repository string \"janssenproject/auth-server\" Image to use for deploying. auth-server.image.tag string \"1.0.5-1\" Image tag to use for deploying. auth-server.livenessProbe object {\"exec\":{\"command\":[\"python3\",\"/app/scripts/healthcheck.py\"]},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5} Configure the liveness healthcheck for the auth server if needed. auth-server.livenessProbe.exec object {\"command\":[\"python3\",\"/app/scripts/healthcheck.py\"]} Executes the python3 healthcheck. https://github.com/JanssenProject/docker-jans-auth-server/blob/master/scripts/healthcheck.py auth-server.pdb object {\"enabled\":true,\"maxUnavailable\":\"90%\"} Configure the PodDisruptionBudget auth-server.readinessProbe object {\"exec\":{\"command\":[\"python3\",\"/app/scripts/healthcheck.py\"]},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5} Configure the readiness healthcheck for the auth server if needed. https://github.com/JanssenProject/docker-jans-auth-server/blob/master/scripts/healthcheck.py auth-server.replicas int 1 Service replica number. auth-server.resources object {\"limits\":{\"cpu\":\"2500m\",\"memory\":\"2500Mi\"},\"requests\":{\"cpu\":\"2500m\",\"memory\":\"2500Mi\"}} Resource specs. auth-server.resources.limits.cpu string \"2500m\" CPU limit. auth-server.resources.limits.memory string \"2500Mi\" Memory limit. auth-server.resources.requests.cpu string \"2500m\" CPU request. auth-server.resources.requests.memory string \"2500Mi\" Memory request. auth-server.topologySpreadConstraints object {} Configure the topology spread constraints. Notice this is a map NOT a list as in the upstream API https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ auth-server.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service auth-server.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 auth-server.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 auth-server.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers auth-server.volumes list [] Configure any additional volumes that need to be attached to the pod casa object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"hpa\":{\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"gluufederation/casa\",\"tag\":\"5.0.0-3\"},\"livenessProbe\":{\"httpGet\":{\"path\":\"/casa/health-check\",\"port\":\"http-casa\"},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5},\"pdb\":{\"enabled\":true,\"maxUnavailable\":\"90%\"},\"readinessProbe\":{\"httpGet\":{\"path\":\"/casa/health-check\",\"port\":\"http-casa\"},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5},\"replicas\":1,\"resources\":{\"limits\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"},\"requests\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"}},\"topologySpreadConstraints\":{},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} Gluu Casa (\"Casa\") is a self-service web portal for end-users to manage authentication and authorization preferences for their account in a Gluu Server. casa.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of casa.additionalLabels object {} Additional labels that will be added across the gateway in the format of casa.dnsConfig object {} Add custom dns config casa.dnsPolicy string \"\" Add custom dns policy casa.hpa object {\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50} Configure the HorizontalPodAutoscaler casa.hpa.behavior object {} Scaling Policies casa.hpa.metrics list [] metrics if targetCPUUtilizationPercentage is not set casa.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. casa.image.pullSecrets list [] Image Pull Secrets casa.image.repository string \"gluufederation/casa\" Image to use for deploying. casa.image.tag string \"5.0.0-3\" Image tag to use for deploying. casa.livenessProbe object {\"httpGet\":{\"path\":\"/casa/health-check\",\"port\":\"http-casa\"},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5} Configure the liveness healthcheck for casa if needed. casa.livenessProbe.httpGet.path string \"/casa/health-check\" http liveness probe endpoint casa.pdb object {\"enabled\":true,\"maxUnavailable\":\"90%\"} Configure the PodDisruptionBudget casa.readinessProbe object {\"httpGet\":{\"path\":\"/casa/health-check\",\"port\":\"http-casa\"},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5} Configure the readiness healthcheck for the casa if needed. casa.readinessProbe.httpGet.path string \"/casa/health-check\" http readiness probe endpoint casa.replicas int 1 Service replica number. casa.resources object {\"limits\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"},\"requests\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"}} Resource specs. casa.resources.limits.cpu string \"500m\" CPU limit. casa.resources.limits.memory string \"500Mi\" Memory limit. casa.resources.requests.cpu string \"500m\" CPU request. casa.resources.requests.memory string \"500Mi\" Memory request. casa.topologySpreadConstraints object {} Configure the topology spread constraints. Notice this is a map NOT a list as in the upstream API https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ casa.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service casa.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 casa.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 casa.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers casa.volumes list [] Configure any additional volumes that need to be attached to the pod config object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"adminPassword\":\"Test1234#\",\"city\":\"Austin\",\"configmap\":{\"cnAwsAccessKeyId\":\"\",\"cnAwsDefaultRegion\":\"us-west-1\",\"cnAwsProfile\":\"janssen\",\"cnAwsSecretAccessKey\":\"\",\"cnAwsSecretsEndpointUrl\":\"\",\"cnAwsSecretsNamePrefix\":\"janssen\",\"cnAwsSecretsReplicaRegions\":[],\"cnCacheType\":\"NATIVE_PERSISTENCE\",\"cnConfigGoogleSecretNamePrefix\":\"gluu\",\"cnConfigGoogleSecretVersionId\":\"latest\",\"cnConfigKubernetesConfigMap\":\"cn\",\"cnCouchbaseBucketPrefix\":\"jans\",\"cnCouchbaseCrt\":\"SWFtTm90YVNlcnZpY2VBY2NvdW50Q2hhbmdlTWV0b09uZQo=\",\"cnCouchbaseIndexNumReplica\":0,\"cnCouchbasePassword\":\"P@ssw0rd\",\"cnCouchbaseSuperUser\":\"admin\",\"cnCouchbaseSuperUserPassword\":\"Test1234#\",\"cnCouchbaseUrl\":\"cbgluu.default.svc.cluster.local\",\"cnCouchbaseUser\":\"gluu\",\"cnGoogleProjectId\":\"google-project-to-save-config-and-secrets-to\",\"cnGoogleSecretManagerPassPhrase\":\"Test1234#\",\"cnGoogleSecretManagerServiceAccount\":\"SWFtTm90YVNlcnZpY2VBY2NvdW50Q2hhbmdlTWV0b09uZQo=\",\"cnGoogleSpannerDatabaseId\":\"\",\"cnGoogleSpannerInstanceId\":\"\",\"cnJettyRequestHeaderSize\":8192,\"cnLdapUrl\":\"opendj:1636\",\"cnMaxRamPercent\":\"75.0\",\"cnPersistenceHybridMapping\":\"{}\",\"cnRedisSentinelGroup\":\"\",\"cnRedisSslTruststore\":\"\",\"cnRedisType\":\"STANDALONE\",\"cnRedisUrl\":\"redis.redis.svc.cluster.local:6379\",\"cnRedisUseSsl\":false,\"cnScimProtectionMode\":\"OAUTH\",\"cnSecretGoogleSecretNamePrefix\":\"gluu\",\"cnSecretGoogleSecretVersionId\":\"latest\",\"cnSecretKubernetesSecret\":\"cn\",\"cnSqlDbDialect\":\"mysql\",\"cnSqlDbHost\":\"my-release-mysql.default.svc.cluster.local\",\"cnSqlDbName\":\"jans\",\"cnSqlDbPort\":3306,\"cnSqlDbSchema\":\"\",\"cnSqlDbTimezone\":\"UTC\",\"cnSqlDbUser\":\"jans\",\"cnSqldbUserPassword\":\"Test1234#\",\"lbAddr\":\"\"},\"countryCode\":\"US\",\"dnsConfig\":{},\"dnsPolicy\":\"\",\"email\":\"support@gluu.org\",\"image\":{\"pullSecrets\":[],\"repository\":\"janssenproject/configurator\",\"tag\":\"1.0.5-1\"},\"ldapPassword\":\"P@ssw0rds\",\"migration\":{\"enabled\":false,\"migrationDataFormat\":\"ldif\",\"migrationDir\":\"/ce-migration\"},\"orgName\":\"Gluu\",\"redisPassword\":\"P@assw0rd\",\"resources\":{\"limits\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"},\"requests\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"}},\"state\":\"TX\",\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} Configuration parameters for setup and initial configuration secret and config layers used by Gluu services. config-api object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"hpa\":{\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"janssenproject/config-api\",\"tag\":\"1.0.5-1\"},\"livenessProbe\":{\"httpGet\":{\"path\":\"/jans-config-api/api/v1/health/live\",\"port\":8074},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5},\"pdb\":{\"enabled\":true,\"maxUnavailable\":\"90%\"},\"readinessProbe\":{\"httpGet\":{\"path\":\"jans-config-api/api/v1/health/ready\",\"port\":8074},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5},\"replicas\":1,\"resources\":{\"limits\":{\"cpu\":\"1000m\",\"memory\":\"400Mi\"},\"requests\":{\"cpu\":\"1000m\",\"memory\":\"400Mi\"}},\"topologySpreadConstraints\":{},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} Config Api endpoints can be used to configure the auth-server, which is an open-source OpenID Connect Provider (OP) and UMA Authorization Server (AS). config-api.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of config-api.additionalLabels object {} Additional labels that will be added across the gateway in the format of config-api.dnsConfig object {} Add custom dns config config-api.dnsPolicy string \"\" Add custom dns policy config-api.hpa object {\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50} Configure the HorizontalPodAutoscaler config-api.hpa.behavior object {} Scaling Policies config-api.hpa.metrics list [] metrics if targetCPUUtilizationPercentage is not set config-api.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. config-api.image.pullSecrets list [] Image Pull Secrets config-api.image.repository string \"janssenproject/config-api\" Image to use for deploying. config-api.image.tag string \"1.0.5-1\" Image tag to use for deploying. config-api.livenessProbe object {\"httpGet\":{\"path\":\"/jans-config-api/api/v1/health/live\",\"port\":8074},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5} Configure the liveness healthcheck for the auth server if needed. config-api.livenessProbe.httpGet object {\"path\":\"/jans-config-api/api/v1/health/live\",\"port\":8074} http liveness probe endpoint config-api.pdb object {\"enabled\":true,\"maxUnavailable\":\"90%\"} Configure the PodDisruptionBudget config-api.readinessProbe.httpGet object {\"path\":\"jans-config-api/api/v1/health/ready\",\"port\":8074} http readiness probe endpoint config-api.replicas int 1 Service replica number. config-api.resources object {\"limits\":{\"cpu\":\"1000m\",\"memory\":\"400Mi\"},\"requests\":{\"cpu\":\"1000m\",\"memory\":\"400Mi\"}} Resource specs. config-api.resources.limits.cpu string \"1000m\" CPU limit. config-api.resources.limits.memory string \"400Mi\" Memory limit. config-api.resources.requests.cpu string \"1000m\" CPU request. config-api.resources.requests.memory string \"400Mi\" Memory request. config-api.topologySpreadConstraints object {} Configure the topology spread constraints. Notice this is a map NOT a list as in the upstream API https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ config-api.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service config-api.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 config-api.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 config-api.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers config-api.volumes list [] Configure any additional volumes that need to be attached to the pod config.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of config.additionalLabels object {} Additional labels that will be added across the gateway in the format of config.adminPassword string \"Test1234#\" Admin password to log in to the UI. config.city string \"Austin\" City. Used for certificate creation. config.configmap.cnCacheType string \"NATIVE_PERSISTENCE\" Cache type. NATIVE_PERSISTENCE , REDIS . or IN_MEMORY . Defaults to NATIVE_PERSISTENCE . config.configmap.cnConfigGoogleSecretNamePrefix string \"gluu\" Prefix for Gluu configuration secret in Google Secret Manager. Defaults to gluu. If left intact gluu-configuration secret will be created. Used only when global.configAdapterName and global.configSecretAdapter is set to google. config.configmap.cnConfigGoogleSecretVersionId string \"latest\" Secret version to be used for configuration. Defaults to latest and should normally always stay that way. Used only when global.configAdapterName and global.configSecretAdapter is set to google. Used only when global.configAdapterName and global.configSecretAdapter is set to google. config.configmap.cnConfigKubernetesConfigMap string \"cn\" The name of the Kubernetes ConfigMap that will hold the configuration layer config.configmap.cnCouchbaseBucketPrefix string \"jans\" The prefix of couchbase buckets. This helps with separation in between different environments and allows for the same couchbase cluster to be used by different setups of Gluu. config.configmap.cnCouchbaseCrt string \"SWFtTm90YVNlcnZpY2VBY2NvdW50Q2hhbmdlTWV0b09uZQo=\" Couchbase certificate authority string. This must be encoded using base64. This can also be found in your couchbase UI Security > Root Certificate. In mTLS setups this is not required. config.configmap.cnCouchbaseIndexNumReplica int 0 The number of replicas per index created. Please note that the number of index nodes must be one greater than the number of index replicas. That means if your couchbase cluster only has 2 index nodes you cannot place the number of replicas to be higher than 1. config.configmap.cnCouchbasePassword string \"P@ssw0rd\" Couchbase password for the restricted user config.configmap.cnCouchbaseUser that is often used inside the services. The password must contain one digit, one uppercase letter, one lower case letter and one symbol . config.configmap.cnCouchbaseSuperUser string \"admin\" The Couchbase super user (admin) username. This user is used during initialization only. config.configmap.cnCouchbaseSuperUserPassword string \"Test1234#\" Couchbase password for the superuser config.configmap.cnCouchbaseSuperUser that is used during the initialization process. The password must contain one digit, one uppercase letter, one lower case letter and one symbol config.configmap.cnCouchbaseUrl string \"cbgluu.default.svc.cluster.local\" Couchbase URL. Used only when global.cnPersistenceType is hybrid or couchbase. This should be in FQDN format for either remote or local Couchbase clusters. The address can be an internal address inside the kubernetes cluster config.configmap.cnCouchbaseUser string \"gluu\" Couchbase restricted user. Used only when global.cnPersistenceType is hybrid or couchbase. config.configmap.cnGoogleProjectId string \"google-project-to-save-config-and-secrets-to\" Project id of the Google project the secret manager belongs to. Used only when global.configAdapterName and global.configSecretAdapter is set to google. config.configmap.cnGoogleSecretManagerPassPhrase string \"Test1234#\" Passphrase for Gluu secret in Google Secret Manager. This is used for encrypting and decrypting data from the Google Secret Manager. Used only when global.configAdapterName and global.configSecretAdapter is set to google. config.configmap.cnGoogleSecretManagerServiceAccount string \"SWFtTm90YVNlcnZpY2VBY2NvdW50Q2hhbmdlTWV0b09uZQo=\" Service account with roles roles/secretmanager.admin base64 encoded string. This is used often inside the services to reach the configuration layer. Used only when global.configAdapterName and global.configSecretAdapter is set to google. config.configmap.cnGoogleSpannerDatabaseId string \"\" Google Spanner Database ID. Used only when global.cnPersistenceType is spanner. config.configmap.cnGoogleSpannerInstanceId string \"\" Google Spanner ID. Used only when global.cnPersistenceType is spanner. config.configmap.cnJettyRequestHeaderSize int 8192 Jetty header size in bytes in the auth server config.configmap.cnLdapUrl string \"opendj:1636\" OpenDJ internal address. Leave as default. Used when global.cnPersistenceType is set to ldap . config.configmap.cnMaxRamPercent string \"75.0\" Value passed to Java option -XX:MaxRAMPercentage config.configmap.cnPersistenceHybridMapping string \"{}\" Specify data that should be saved in LDAP (one of default, user, cache, site, token, or session; default to default). Note this environment only takes effect when global.cnPersistenceType is set to hybrid . { \"default\": \"<couchbase config.configmap.cnRedisSentinelGroup string \"\" Redis Sentinel Group. Often set when config.configmap.cnRedisType is set to SENTINEL . Can be used when config.configmap.cnCacheType is set to REDIS . config.configmap.cnRedisSslTruststore string \"\" Redis SSL truststore. Optional. Can be used when config.configmap.cnCacheType is set to REDIS . config.configmap.cnRedisType string \"STANDALONE\" Redis service type. STANDALONE or CLUSTER . Can be used when config.configmap.cnCacheType is set to REDIS . config.configmap.cnRedisUrl string \"redis.redis.svc.cluster.local:6379\" Redis URL and port number : . Can be used when config.configmap.cnCacheType is set to REDIS . config.configmap.cnRedisUseSsl bool false Boolean to use SSL in Redis. Can be used when config.configmap.cnCacheType is set to REDIS . config.configmap.cnScimProtectionMode string \"OAUTH\" SCIM protection mode OAUTH config.configmap.cnSecretGoogleSecretNamePrefix string \"gluu\" Prefix for Gluu secret in Google Secret Manager. Defaults to gluu. If left gluu-secret secret will be created. Used only when global.configAdapterName and global.configSecretAdapter is set to google. config.configmap.cnSecretGoogleSecretVersionId string \"latest\" Secret version to be used for secret configuration. Defaults to latest and should normally always stay that way. Used only when global.configAdapterName and global.configSecretAdapter is set to google. config.configmap.cnSecretKubernetesSecret string \"cn\" Kubernetes secret name holding configuration keys. Used when global.configSecretAdapter is set to kubernetes which is the default. config.configmap.cnSqlDbDialect string \"mysql\" SQL database dialect. mysql or pgsql config.configmap.cnSqlDbHost string \"my-release-mysql.default.svc.cluster.local\" SQL database host uri. config.configmap.cnSqlDbName string \"jans\" SQL database name. config.configmap.cnSqlDbPort int 3306 SQL database port. config.configmap.cnSqlDbSchema string \"\" Schema name used by SQL database (default to empty-string; if using MySQL, the schema name will be resolved as the database name, whereas in PostgreSQL the schema name will be resolved as \"public\" ). config.configmap.cnSqlDbTimezone string \"UTC\" SQL database timezone. config.configmap.cnSqlDbUser string \"jans\" SQL database username. config.configmap.cnSqldbUserPassword string \"Test1234#\" SQL password injected the secrets . config.configmap.lbAddr string \"\" Load balancer address for AWS if the FQDN is not registered. config.countryCode string \"US\" Country code. Used for certificate creation. config.dnsConfig object {} Add custom dns config config.dnsPolicy string \"\" Add custom dns policy config.email string \"support@gluu.org\" Email address of the administrator usually. Used for certificate creation. config.image.pullSecrets list [] Image Pull Secrets config.image.repository string \"janssenproject/configurator\" Image to use for deploying. config.image.tag string \"1.0.5-1\" Image tag to use for deploying. config.ldapPassword string \"P@ssw0rds\" LDAP admin password if OpenDJ is used for persistence. config.migration object {\"enabled\":false,\"migrationDataFormat\":\"ldif\",\"migrationDir\":\"/ce-migration\"} CE to CN Migration section config.migration.enabled bool false Boolean flag to enable migration from CE config.migration.migrationDataFormat string \"ldif\" migration data-format depending on persistence backend. Supported data formats are ldif, couchbase+json, spanner+avro, postgresql+json, and mysql+json. config.migration.migrationDir string \"/ce-migration\" Directory holding all migration files config.orgName string \"Gluu\" Organization name. Used for certificate creation. config.redisPassword string \"P@assw0rd\" Redis admin password if config.configmap.cnCacheType is set to REDIS . config.resources object {\"limits\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"},\"requests\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"}} Resource specs. config.resources.limits.cpu string \"300m\" CPU limit. config.resources.limits.memory string \"300Mi\" Memory limit. config.resources.requests.cpu string \"300m\" CPU request. config.resources.requests.memory string \"300Mi\" Memory request. config.state string \"TX\" State code. Used for certificate creation. config.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service. config.usrEnvs.normal object {} Add custom normal envs to the service. variable1: value1 config.usrEnvs.secret object {} Add custom secret envs to the service. variable1: value1 config.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers config.volumes list [] Configure any additional volumes that need to be attached to the pod fido2 object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"hpa\":{\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"janssenproject/fido2\",\"tag\":\"1.0.5-1\"},\"livenessProbe\":{\"httpGet\":{\"path\":\"/jans-fido2/sys/health-check\",\"port\":\"http-fido2\"},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5},\"pdb\":{\"enabled\":true,\"maxUnavailable\":\"90%\"},\"readinessProbe\":{\"httpGet\":{\"path\":\"/jans-fido2/sys/health-check\",\"port\":\"http-fido2\"},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5},\"replicas\":1,\"resources\":{\"limits\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"},\"requests\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"}},\"service\":{\"name\":\"http-fido2\",\"port\":8080},\"topologySpreadConstraints\":{},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} FIDO 2.0 (FIDO2) is an open authentication standard that enables leveraging common devices to authenticate to online services in both mobile and desktop environments. fido2.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of fido2.additionalLabels object {} Additional labels that will be added across the gateway in the format of fido2.dnsConfig object {} Add custom dns config fido2.dnsPolicy string \"\" Add custom dns policy fido2.hpa object {\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50} Configure the HorizontalPodAutoscaler fido2.hpa.behavior object {} Scaling Policies fido2.hpa.metrics list [] metrics if targetCPUUtilizationPercentage is not set fido2.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. fido2.image.pullSecrets list [] Image Pull Secrets fido2.image.repository string \"janssenproject/fido2\" Image to use for deploying. fido2.image.tag string \"1.0.5-1\" Image tag to use for deploying. fido2.livenessProbe object {\"httpGet\":{\"path\":\"/jans-fido2/sys/health-check\",\"port\":\"http-fido2\"},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5} Configure the liveness healthcheck for the fido2 if needed. fido2.livenessProbe.httpGet object {\"path\":\"/jans-fido2/sys/health-check\",\"port\":\"http-fido2\"} http liveness probe endpoint fido2.pdb object {\"enabled\":true,\"maxUnavailable\":\"90%\"} Configure the PodDisruptionBudget fido2.readinessProbe object {\"httpGet\":{\"path\":\"/jans-fido2/sys/health-check\",\"port\":\"http-fido2\"},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5} Configure the readiness healthcheck for the fido2 if needed. fido2.replicas int 1 Service replica number. fido2.resources object {\"limits\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"},\"requests\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"}} Resource specs. fido2.resources.limits.cpu string \"500m\" CPU limit. fido2.resources.limits.memory string \"500Mi\" Memory limit. fido2.resources.requests.cpu string \"500m\" CPU request. fido2.resources.requests.memory string \"500Mi\" Memory request. fido2.service.name string \"http-fido2\" The name of the fido2 port within the fido2 service. Please keep it as default. fido2.service.port int 8080 Port of the fido2 service. Please keep it as default. fido2.topologySpreadConstraints object {} Configure the topology spread constraints. Notice this is a map NOT a list as in the upstream API https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ fido2.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service fido2.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 fido2.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 fido2.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers fido2.volumes list [] Configure any additional volumes that need to be attached to the pod global object {\"admin-ui\":{\"adminUiServiceName\":\"admin-ui\",\"enabled\":true,\"ingress\":{\"adminUiEnabled\":false}},\"alb\":{\"ingress\":false},\"auth-server\":{\"appLoggers\":{\"auditStatsLogLevel\":\"INFO\",\"auditStatsLogTarget\":\"FILE\",\"authLogLevel\":\"INFO\",\"authLogTarget\":\"STDOUT\",\"httpLogLevel\":\"INFO\",\"httpLogTarget\":\"FILE\",\"ldapStatsLogLevel\":\"INFO\",\"ldapStatsLogTarget\":\"FILE\",\"persistenceDurationLogLevel\":\"INFO\",\"persistenceDurationLogTarget\":\"FILE\",\"persistenceLogLevel\":\"INFO\",\"persistenceLogTarget\":\"FILE\",\"scriptLogLevel\":\"INFO\",\"scriptLogTarget\":\"FILE\"},\"authEncKeys\":\"RSA1_5 RSA-OAEP\",\"authServerServiceName\":\"auth-server\",\"authSigKeys\":\"RS256 RS384 RS512 ES256 ES384 ES512 PS256 PS384 PS512\",\"enabled\":true,\"ingress\":{\"authServerEnabled\":true,\"authServerProtectedRegister\":false,\"authServerProtectedToken\":false,\"deviceCodeEnabled\":true,\"firebaseMessagingEnabled\":true,\"openidConfigEnabled\":true,\"u2fConfigEnabled\":true,\"uma2ConfigEnabled\":true,\"webdiscoveryEnabled\":true,\"webfingerEnabled\":true}},\"auth-server-key-rotation\":{\"enabled\":false},\"awsStorageType\":\"io1\",\"azureStorageAccountType\":\"Standard_LRS\",\"azureStorageKind\":\"Managed\",\"casa\":{\"casaServiceName\":\"casa\",\"enabled\":true,\"ingress\":{\"casaEnabled\":false}},\"cloud\":{\"testEnviroment\":false},\"cnAwsConfigFile\":\"/etc/jans/conf/aws_config_file\",\"cnAwsSecretsReplicaRegionsFile\":\"/etc/jans/conf/aws_secrets_replica_regions\",\"cnAwsSharedCredentialsFile\":\"/etc/jans/conf/aws_shared_credential_file\",\"cnDocumentStoreType\":\"LOCAL\",\"cnGoogleApplicationCredentials\":\"/etc/jans/conf/google-credentials.json\",\"cnObExtSigningAlias\":\"\",\"cnObExtSigningJwksCrt\":\"\",\"cnObExtSigningJwksKey\":\"\",\"cnObExtSigningJwksKeyPassPhrase\":\"\",\"cnObExtSigningJwksUri\":\"\",\"cnObStaticSigningKeyKid\":\"\",\"cnObTransportAlias\":\"\",\"cnObTransportCrt\":\"\",\"cnObTransportKey\":\"\",\"cnObTransportKeyPassPhrase\":\"\",\"cnObTransportTrustStore\":\"\",\"cnPersistenceType\":\"sql\",\"cnPrometheusPort\":\"\",\"config\":{\"enabled\":true},\"config-api\":{\"adminUiAppLoggers\":{\"adminUiAuditLogLevel\":\"INFO\",\"adminUiAuditLogTarget\":\"FILE\",\"adminUiLogLevel\":\"INFO\",\"adminUiLogTarget\":\"FILE\"},\"appLoggers\":{\"configApiLogLevel\":\"INFO\",\"configApiLogTarget\":\"STDOUT\",\"ldapStatsLogLevel\":\"INFO\",\"ldapStatsLogTarget\":\"FILE\",\"persistenceDurationLogLevel\":\"INFO\",\"persistenceDurationLogTarget\":\"FILE\",\"persistenceLogLevel\":\"INFO\",\"persistenceLogTarget\":\"FILE\",\"scriptLogLevel\":\"INFO\",\"scriptLogTarget\":\"FILE\"},\"configApiServerServiceName\":\"config-api\",\"enabled\":true,\"ingress\":{\"configApiEnabled\":true}},\"configAdapterName\":\"kubernetes\",\"configSecretAdapter\":\"kubernetes\",\"distribution\":\"default\",\"fido2\":{\"appLoggers\":{\"fido2LogLevel\":\"INFO\",\"fido2LogTarget\":\"STDOUT\",\"persistenceLogLevel\":\"INFO\",\"persistenceLogTarget\":\"FILE\"},\"enabled\":true,\"fido2ServiceName\":\"fido2\",\"ingress\":{\"fido2ConfigEnabled\":false}},\"fqdn\":\"demoexample.gluu.org\",\"gcePdStorageType\":\"pd-standard\",\"isFqdnRegistered\":false,\"istio\":{\"additionalAnnotations\":{},\"additionalLabels\":{},\"enabled\":false,\"gateways\":[],\"ingress\":false,\"namespace\":\"istio-system\"},\"jobTtlSecondsAfterFinished\":300,\"lbIp\":\"22.22.22.22\",\"nginx-ingress\":{\"enabled\":true},\"opendj\":{\"enabled\":false,\"ldapServiceName\":\"opendj\"},\"oxpassport\":{\"enabled\":false,\"oxPassportServiceName\":\"oxpassport\"},\"oxshibboleth\":{\"appLoggers\":{\"auditStatsLogLevel\":\"INFO\",\"auditStatsLogTarget\":\"FILE\",\"consentAuditLogLevel\":\"INFO\",\"consentAuditLogTarget\":\"FILE\",\"containerLogLevel\":\"\",\"encryptionLogLevel\":\"\",\"httpclientLogLevel\":\"\",\"idpLogLevel\":\"INFO\",\"idpLogTarget\":\"STDOUT\",\"ldapLogLevel\":\"\",\"messagesLogLevel\":\"\",\"opensamlLogLevel\":\"\",\"propsLogLevel\":\"\",\"scriptLogLevel\":\"INFO\",\"scriptLogTarget\":\"FILE\",\"springLogLevel\":\"\",\"xmlsecLogLevel\":\"\"},\"enabled\":false,\"oxShibbolethServiceName\":\"oxshibboleth\"},\"persistence\":{\"enabled\":true},\"scim\":{\"appLoggers\":{\"ldapStatsLogLevel\":\"INFO\",\"ldapStatsLogTarget\":\"FILE\",\"persistenceDurationLogLevel\":\"INFO\",\"persistenceDurationLogTarget\":\"FILE\",\"persistenceLogLevel\":\"INFO\",\"persistenceLogTarget\":\"FILE\",\"scimLogLevel\":\"INFO\",\"scimLogTarget\":\"STDOUT\",\"scriptLogLevel\":\"INFO\",\"scriptLogTarget\":\"FILE\"},\"enabled\":true,\"ingress\":{\"scimConfigEnabled\":false,\"scimEnabled\":false},\"scimServiceName\":\"scim\"},\"storageClass\":{\"allowVolumeExpansion\":true,\"allowedTopologies\":[],\"mountOptions\":[\"debug\"],\"parameters\":{},\"provisioner\":\"microk8s.io/hostpath\",\"reclaimPolicy\":\"Retain\",\"volumeBindingMode\":\"WaitForFirstConsumer\"},\"upgrade\":{\"enabled\":false},\"usrEnvs\":{\"normal\":{},\"secret\":{}}} Parameters used globally across all services helm charts. global.admin-ui.adminUiServiceName string \"admin-ui\" Name of the admin-ui service. Please keep it as default. global.admin-ui.enabled bool true Boolean flag to enable/disable the admin-ui chart and admin ui config api plugin. global.admin-ui.ingress.adminUiEnabled bool false Enable Admin UI endpoints in either istio or nginx ingress depending on users choice global.alb.ingress bool false Activates ALB ingress global.auth-server-key-rotation.enabled bool false Boolean flag to enable/disable the auth-server-key rotation cronjob chart. global.auth-server.appLoggers object {\"auditStatsLogLevel\":\"INFO\",\"auditStatsLogTarget\":\"FILE\",\"authLogLevel\":\"INFO\",\"authLogTarget\":\"STDOUT\",\"httpLogLevel\":\"INFO\",\"httpLogTarget\":\"FILE\",\"ldapStatsLogLevel\":\"INFO\",\"ldapStatsLogTarget\":\"FILE\",\"persistenceDurationLogLevel\":\"INFO\",\"persistenceDurationLogTarget\":\"FILE\",\"persistenceLogLevel\":\"INFO\",\"persistenceLogTarget\":\"FILE\",\"scriptLogLevel\":\"INFO\",\"scriptLogTarget\":\"FILE\"} App loggers can be configured to define where the logs will be redirected to and the level of each in which it should be displayed. global.auth-server.appLoggers.auditStatsLogLevel string \"INFO\" jans-auth_audit.log level global.auth-server.appLoggers.auditStatsLogTarget string \"FILE\" jans-auth_script.log target global.auth-server.appLoggers.authLogLevel string \"INFO\" jans-auth.log level global.auth-server.appLoggers.authLogTarget string \"STDOUT\" jans-auth.log target global.auth-server.appLoggers.httpLogLevel string \"INFO\" http_request_response.log level global.auth-server.appLoggers.httpLogTarget string \"FILE\" http_request_response.log target global.auth-server.appLoggers.ldapStatsLogLevel string \"INFO\" jans-auth_persistence_ldap_statistics.log level global.auth-server.appLoggers.ldapStatsLogTarget string \"FILE\" jans-auth_persistence_ldap_statistics.log target global.auth-server.appLoggers.persistenceDurationLogLevel string \"INFO\" jans-auth_persistence_duration.log level global.auth-server.appLoggers.persistenceDurationLogTarget string \"FILE\" jans-auth_persistence_duration.log target global.auth-server.appLoggers.persistenceLogLevel string \"INFO\" jans-auth_persistence.log level global.auth-server.appLoggers.persistenceLogTarget string \"FILE\" jans-auth_persistence.log target global.auth-server.appLoggers.scriptLogLevel string \"INFO\" jans-auth_script.log level global.auth-server.appLoggers.scriptLogTarget string \"FILE\" jans-auth_script.log target global.auth-server.authEncKeys string \"RSA1_5 RSA-OAEP\" space-separated key algorithm for encryption (default to RSA1_5 RSA-OAEP ) global.auth-server.authServerServiceName string \"auth-server\" Name of the auth-server service. Please keep it as default. global.auth-server.authSigKeys string \"RS256 RS384 RS512 ES256 ES384 ES512 PS256 PS384 PS512\" space-separated key algorithm for signing (default to RS256 RS384 RS512 ES256 ES384 ES512 PS256 PS384 PS512 ) global.auth-server.enabled bool true Boolean flag to enable/disable auth-server chart. You should never set this to false. global.auth-server.ingress object {\"authServerEnabled\":true,\"authServerProtectedRegister\":false,\"authServerProtectedToken\":false,\"deviceCodeEnabled\":true,\"firebaseMessagingEnabled\":true,\"openidConfigEnabled\":true,\"u2fConfigEnabled\":true,\"uma2ConfigEnabled\":true,\"webdiscoveryEnabled\":true,\"webfingerEnabled\":true} Enable endpoints in either istio or nginx ingress depending on users choice global.auth-server.ingress.authServerEnabled bool true Enable Auth server endpoints /jans-auth global.auth-server.ingress.authServerProtectedRegister bool false Enable mTLS onn Auth server endpoint /jans-auth/restv1/register. Currently not working in Istio. global.auth-server.ingress.authServerProtectedToken bool false Enable mTLS on Auth server endpoint /jans-auth/restv1/token. Currently not working in Istio. global.auth-server.ingress.deviceCodeEnabled bool true Enable endpoint /device-code global.auth-server.ingress.firebaseMessagingEnabled bool true Enable endpoint /firebase-messaging-sw.js global.auth-server.ingress.openidConfigEnabled bool true Enable endpoint /.well-known/openid-configuration global.auth-server.ingress.u2fConfigEnabled bool true Enable endpoint /.well-known/fido-configuration global.auth-server.ingress.uma2ConfigEnabled bool true Enable endpoint /.well-known/uma2-configuration global.auth-server.ingress.webdiscoveryEnabled bool true Enable endpoint /.well-known/simple-web-discovery global.auth-server.ingress.webfingerEnabled bool true Enable endpoint /.well-known/webfinger global.awsStorageType string \"io1\" Volume storage type if using AWS volumes. global.azureStorageAccountType string \"Standard_LRS\" Volume storage type if using Azure disks. global.azureStorageKind string \"Managed\" Azure storage kind if using Azure disks global.casa.casaServiceName string \"casa\" Name of the casa service. Please keep it as default. global.casa.enabled bool true Boolean flag to enable/disable the casa chart. global.casa.ingress object {\"casaEnabled\":false} Enable endpoints in either istio or nginx ingress depending on users choice global.casa.ingress.casaEnabled bool false Enable casa endpoints /casa global.cloud.testEnviroment bool false Boolean flag if enabled will strip resources requests and limits from all services. global.cnDocumentStoreType string \"LOCAL\" Document store type to use for shibboleth files LOCAL. global.cnGoogleApplicationCredentials string \"/etc/jans/conf/google-credentials.json\" Base64 encoded service account. The sa must have roles/secretmanager.admin to use Google secrets and roles/spanner.databaseUser to use Spanner. Leave as this is a sensible default. global.cnObExtSigningAlias string \"\" Open banking external signing AS Alias. This is a kid value.Used in SSA Validation, kid used while encoding a JWT sent to token URL i.e. XkwIzWy44xWSlcWnMiEc8iq9s2G global.cnObExtSigningJwksCrt string \"\" Open banking external signing jwks AS certificate authority string. Used in SSA Validation. This must be encoded using base64.. Used when .global.cnObExtSigningJwksUri is set. global.cnObExtSigningJwksKey string \"\" Open banking external signing jwks AS key string. Used in SSA Validation. This must be encoded using base64. Used when .global.cnObExtSigningJwksUri is set. global.cnObExtSigningJwksKeyPassPhrase string \"\" Open banking external signing jwks AS key passphrase to unlock provided key. This must be encoded using base64. Used when .global.cnObExtSigningJwksUri is set. global.cnObExtSigningJwksUri string \"\" Open banking external signing jwks uri. Used in SSA Validation. global.cnObStaticSigningKeyKid string \"\" Open banking signing AS kid to force the AS to use a specific signing key. i.e. Wy44xWSlcWnMiEc8iq9s2G global.cnObTransportAlias string \"\" Open banking transport Alias used inside the JVM. global.cnObTransportCrt string \"\" Open banking AS transport crt. Used in SSA Validation. This must be encoded using base64. global.cnObTransportKey string \"\" Open banking AS transport key. Used in SSA Validation. This must be encoded using base64. global.cnObTransportKeyPassPhrase string \"\" Open banking AS transport key passphrase to unlock AS transport key. This must be encoded using base64. global.cnObTransportTrustStore string \"\" Open banking AS transport truststore crt. This is normally generated from the OB issuing CA, OB Root CA and Signing CA. Used when .global.cnObExtSigningJwksUri is set. Used in SSA Validation. This must be encoded using base64. global.cnPersistenceType string \"sql\" Persistence backend to run Gluu with ldap global.cnPrometheusPort string \"\" Port used by Prometheus JMX agent (default to empty string). To enable Prometheus JMX agent, set the value to a number. global.config-api.adminUiAppLoggers.adminUiAuditLogLevel string \"INFO\" config-api admin-ui plugin audit log level global.config-api.adminUiAppLoggers.adminUiAuditLogTarget string \"FILE\" config-api admin-ui plugin audit log target global.config-api.adminUiAppLoggers.adminUiLogLevel string \"INFO\" config-api admin-ui plugin log target global.config-api.adminUiAppLoggers.adminUiLogTarget string \"FILE\" config-api admin-ui plugin log level global.config-api.appLoggers object {\"configApiLogLevel\":\"INFO\",\"configApiLogTarget\":\"STDOUT\",\"ldapStatsLogLevel\":\"INFO\",\"ldapStatsLogTarget\":\"FILE\",\"persistenceDurationLogLevel\":\"INFO\",\"persistenceDurationLogTarget\":\"FILE\",\"persistenceLogLevel\":\"INFO\",\"persistenceLogTarget\":\"FILE\",\"scriptLogLevel\":\"INFO\",\"scriptLogTarget\":\"FILE\"} App loggers can be configured to define where the logs will be redirected to and the level of each in which it should be displayed. global.config-api.appLoggers.configApiLogLevel string \"INFO\" configapi.log level global.config-api.appLoggers.configApiLogTarget string \"STDOUT\" configapi.log target global.config-api.appLoggers.ldapStatsLogLevel string \"INFO\" config-api_persistence_ldap_statistics.log level global.config-api.appLoggers.ldapStatsLogTarget string \"FILE\" config-api_persistence_ldap_statistics.log target global.config-api.appLoggers.persistenceDurationLogLevel string \"INFO\" config-api_persistence_duration.log level global.config-api.appLoggers.persistenceDurationLogTarget string \"FILE\" config-api_persistence_duration.log target global.config-api.appLoggers.persistenceLogLevel string \"INFO\" jans-auth_persistence.log level global.config-api.appLoggers.persistenceLogTarget string \"FILE\" config-api_persistence.log target global.config-api.appLoggers.scriptLogLevel string \"INFO\" config-api_script.log level global.config-api.appLoggers.scriptLogTarget string \"FILE\" config-api_script.log target global.config-api.configApiServerServiceName string \"config-api\" Name of the config-api service. Please keep it as default. global.config-api.enabled bool true Boolean flag to enable/disable the config-api chart. global.config-api.ingress object {\"configApiEnabled\":true} Enable endpoints in either istio or nginx ingress depending on users choice global.config.enabled bool true Boolean flag to enable/disable the configuration chart. This normally should never be false global.configAdapterName string \"kubernetes\" The config backend adapter that will hold Gluu configuration layer. aws global.configSecretAdapter string \"kubernetes\" The config backend adapter that will hold Gluu secret layer. aws global.distribution string \"default\" Gluu distributions supported are: default global.fido2.appLoggers object {\"fido2LogLevel\":\"INFO\",\"fido2LogTarget\":\"STDOUT\",\"persistenceLogLevel\":\"INFO\",\"persistenceLogTarget\":\"FILE\"} App loggers can be configured to define where the logs will be redirected to and the level of each in which it should be displayed. global.fido2.appLoggers.fido2LogLevel string \"INFO\" fido2.log level global.fido2.appLoggers.fido2LogTarget string \"STDOUT\" fido2.log target global.fido2.appLoggers.persistenceLogLevel string \"INFO\" fido2_persistence.log level global.fido2.appLoggers.persistenceLogTarget string \"FILE\" fido2_persistence.log target global.fido2.enabled bool true Boolean flag to enable/disable the fido2 chart. global.fido2.fido2ServiceName string \"fido2\" Name of the fido2 service. Please keep it as default. global.fido2.ingress object {\"fido2ConfigEnabled\":false} Enable endpoints in either istio or nginx ingress depending on users choice global.fido2.ingress.fido2ConfigEnabled bool false Enable endpoint /.well-known/fido2-configuration global.fqdn string \"demoexample.gluu.org\" Fully qualified domain name to be used for Gluu installation. This address will be used to reach Gluu services. global.gcePdStorageType string \"pd-standard\" GCE storage kind if using Google disks global.isFqdnRegistered bool false Boolean flag to enable mapping global.lbIp to global.fqdn inside pods on clouds that provide static ip for load balancers. On cloud that provide only addresses to the LB this flag will enable a script to actively scan config.configmap.lbAddr and update the hosts file inside the pods automatically. global.istio.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of global.istio.additionalLabels object {} Additional labels that will be added across the gateway in the format of global.istio.enabled bool false Boolean flag that enables using istio side-cars with Gluu services. global.istio.gateways list [] Override the gateway that can be created by default. This is used when istio ingress has already been setup and the gateway exists. global.istio.ingress bool false Boolean flag that enables using istio gateway for Gluu. This assumes istio ingress is installed and hence the LB is available. global.istio.namespace string \"istio-system\" The namespace istio is deployed in. The is normally istio-system. global.jobTtlSecondsAfterFinished int 300 https://kubernetes.io/docs/concepts/workloads/controllers/ttlafterfinished/ global.lbIp string \"22.22.22.22\" The Loadbalancer IP created by nginx or istio on clouds that provide static IPs. This is not needed if global.fqdn is globally resolvable. global.nginx-ingress.enabled bool true Boolean flag to enable/disable the nginx-ingress definitions chart. global.opendj.enabled bool false Boolean flag to enable/disable the OpenDJ chart. global.opendj.ldapServiceName string \"opendj\" Name of the OpenDJ service. Please keep it as default. global.oxpassport.enabled bool false Boolean flag to enable/disable passport chart global.oxpassport.oxPassportServiceName string \"oxpassport\" Name of the oxPassport service. Please keep it as default. global.oxshibboleth.appLoggers object {\"auditStatsLogLevel\":\"INFO\",\"auditStatsLogTarget\":\"FILE\",\"consentAuditLogLevel\":\"INFO\",\"consentAuditLogTarget\":\"FILE\",\"containerLogLevel\":\"\",\"encryptionLogLevel\":\"\",\"httpclientLogLevel\":\"\",\"idpLogLevel\":\"INFO\",\"idpLogTarget\":\"STDOUT\",\"ldapLogLevel\":\"\",\"messagesLogLevel\":\"\",\"opensamlLogLevel\":\"\",\"propsLogLevel\":\"\",\"scriptLogLevel\":\"INFO\",\"scriptLogTarget\":\"FILE\",\"springLogLevel\":\"\",\"xmlsecLogLevel\":\"\"} App loggers can be configured to define where the logs will be redirected to and the level of each in which it should be displayed. log levels are \"OFF\", \"FATAL\", \"ERROR\", \"WARN\", \"INFO\", \"DEBUG\", \"TRACE\" Targets are \"STDOUT\" and \"FILE\" global.oxshibboleth.appLoggers.auditStatsLogLevel string \"INFO\" idp-audit.log level global.oxshibboleth.appLoggers.auditStatsLogTarget string \"FILE\" idp-audit.log target global.oxshibboleth.appLoggers.consentAuditLogLevel string \"INFO\" idp-consent-audit.log level global.oxshibboleth.appLoggers.consentAuditLogTarget string \"FILE\" idp-consent-audit.log target global.oxshibboleth.appLoggers.idpLogLevel string \"INFO\" idp-process.log level global.oxshibboleth.appLoggers.idpLogTarget string \"STDOUT\" idp-process.log target global.oxshibboleth.appLoggers.ldapLogLevel string \"\" https://github.com/GluuFederation/docker-oxshibboleth#additional-logger-configuration The below are very noisy logs and are better left untouched global.oxshibboleth.appLoggers.scriptLogLevel string \"INFO\" idp-script.log level global.oxshibboleth.appLoggers.scriptLogTarget string \"FILE\" idp-script.log target global.oxshibboleth.enabled bool false Boolean flag to enable/disable the oxShibbboleth chart. global.oxshibboleth.oxShibbolethServiceName string \"oxshibboleth\" Name of the oxShibboleth service. Please keep it as default. global.persistence.enabled bool true Boolean flag to enable/disable the persistence chart. global.scim.appLoggers object {\"ldapStatsLogLevel\":\"INFO\",\"ldapStatsLogTarget\":\"FILE\",\"persistenceDurationLogLevel\":\"INFO\",\"persistenceDurationLogTarget\":\"FILE\",\"persistenceLogLevel\":\"INFO\",\"persistenceLogTarget\":\"FILE\",\"scimLogLevel\":\"INFO\",\"scimLogTarget\":\"STDOUT\",\"scriptLogLevel\":\"INFO\",\"scriptLogTarget\":\"FILE\"} App loggers can be configured to define where the logs will be redirected to and the level of each in which it should be displayed. global.scim.appLoggers.ldapStatsLogLevel string \"INFO\" jans-scim_persistence_ldap_statistics.log level global.scim.appLoggers.ldapStatsLogTarget string \"FILE\" jans-scim_persistence_ldap_statistics.log target global.scim.appLoggers.persistenceDurationLogLevel string \"INFO\" jans-scim_persistence_duration.log level global.scim.appLoggers.persistenceDurationLogTarget string \"FILE\" jans-scim_persistence_duration.log target global.scim.appLoggers.persistenceLogLevel string \"INFO\" jans-scim_persistence.log level global.scim.appLoggers.persistenceLogTarget string \"FILE\" jans-scim_persistence.log target global.scim.appLoggers.scimLogLevel string \"INFO\" jans-scim.log level global.scim.appLoggers.scimLogTarget string \"STDOUT\" jans-scim.log target global.scim.appLoggers.scriptLogLevel string \"INFO\" jans-scim_script.log level global.scim.appLoggers.scriptLogTarget string \"FILE\" jans-scim_script.log target global.scim.enabled bool true Boolean flag to enable/disable the SCIM chart. global.scim.ingress object {\"scimConfigEnabled\":false,\"scimEnabled\":false} Enable endpoints in either istio or nginx ingress depending on users choice global.scim.ingress.scimConfigEnabled bool false Enable endpoint /.well-known/scim-configuration global.scim.ingress.scimEnabled bool false Enable SCIM endpoints /jans-scim global.scim.scimServiceName string \"scim\" Name of the scim service. Please keep it as default. global.storageClass object {\"allowVolumeExpansion\":true,\"allowedTopologies\":[],\"mountOptions\":[\"debug\"],\"parameters\":{},\"provisioner\":\"microk8s.io/hostpath\",\"reclaimPolicy\":\"Retain\",\"volumeBindingMode\":\"WaitForFirstConsumer\"} StorageClass section for OpenDJ charts. This is not currently used by the openbanking distribution. You may specify custom parameters as needed. global.storageClass.parameters object {} parameters: fsType: \"\" kind: \"\" pool: \"\" storageAccountType: \"\" type: \"\" global.upgrade.enabled bool false Boolean flag used when running upgrading through versions command. Used when upgrading with LDAP as the persistence to load the 101x ldif. global.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service. Envs defined in global.userEnvs will be globally available to all services global.usrEnvs.normal object {} Add custom normal envs to the service. variable1: value1 global.usrEnvs.secret object {} Add custom secret envs to the service. variable1: value1 installer-settings object {\"acceptLicense\":\"\",\"aws\":{\"arn\":{\"arnAcmCert\":\"\",\"enabled\":\"\"},\"lbType\":\"\",\"vpcCidr\":\"0.0.0.0/0\"},\"confirmSettings\":false,\"couchbase\":{\"backup\":{\"fullSchedule\":\"\",\"incrementalSchedule\":\"\",\"retentionTime\":\"\",\"storageSize\":\"\"},\"clusterName\":\"\",\"commonName\":\"\",\"customFileOverride\":\"\",\"install\":\"\",\"lowResourceInstall\":\"\",\"namespace\":\"\",\"subjectAlternativeName\":\"\",\"totalNumberOfExpectedTransactionsPerSec\":\"\",\"totalNumberOfExpectedUsers\":\"\",\"volumeType\":\"\"},\"currentVersion\":\"\",\"google\":{\"useSecretManager\":\"\"},\"images\":{\"edit\":\"\"},\"ldap\":{\"backup\":{\"fullSchedule\":\"\"}},\"namespace\":\"\",\"nginxIngress\":{\"namespace\":\"\",\"releaseName\":\"\"},\"nodes\":{\"ips\":\"\",\"names\":\"\",\"zones\":\"\"},\"openbanking\":{\"cnObTransportTrustStoreP12password\":\"\",\"hasCnObTransportTrustStore\":false},\"postgres\":{\"install\":\"\",\"namespace\":\"\"},\"redis\":{\"install\":\"\",\"namespace\":\"\"},\"releaseName\":\"\",\"sql\":{\"install\":\"\",\"namespace\":\"\"},\"upgrade\":{\"image\":{\"repository\":\"\",\"tag\":\"\"},\"targetVersion\":\"\"},\"volumeProvisionStrategy\":\"\"} Only used by the installer. These settings do not affect nor are used by the chart nginx-ingress object {\"certManager\":{\"certificate\":{\"enabled\":false,\"issuerGroup\":\"cert-manager.io\",\"issuerKind\":\"ClusterIssuer\",\"issuerName\":\"\"}},\"ingress\":{\"additionalAnnotations\":{},\"additionalLabels\":{},\"adminUiAdditionalAnnotations\":{},\"adminUiLabels\":{},\"authServerAdditionalAnnotations\":{},\"authServerLabels\":{},\"authServerProtectedRegisterAdditionalAnnotations\":{},\"authServerProtectedRegisterLabels\":{},\"authServerProtectedTokenAdditionalAnnotations\":{},\"authServerProtectedTokenLabels\":{},\"casaAdditionalAnnotations\":{},\"casaLabels\":{},\"configApiAdditionalAnnotations\":{},\"configApiLabels\":{},\"deviceCodeAdditionalAnnotations\":{},\"deviceCodeLabels\":{},\"fido2ConfigAdditionalAnnotations\":{},\"fido2ConfigLabels\":{},\"firebaseMessagingAdditionalAnnotations\":{},\"firebaseMessagingLabels\":{},\"hosts\":[\"demoexample.gluu.org\"],\"openidAdditionalAnnotations\":{},\"openidConfigLabels\":{},\"path\":\"/\",\"scimAdditionalAnnotations\":{},\"scimConfigAdditionalAnnotations\":{},\"scimConfigLabels\":{},\"scimLabels\":{},\"tls\":[{\"hosts\":[\"demoexample.gluu.org\"],\"secretName\":\"tls-certificate\"}],\"u2fAdditionalAnnotations\":{},\"u2fConfigLabels\":{},\"uma2AdditionalAnnotations\":{},\"uma2ConfigLabels\":{},\"webdiscoveryAdditionalAnnotations\":{},\"webdiscoveryLabels\":{},\"webfingerAdditionalAnnotations\":{},\"webfingerLabels\":{}}} Nginx ingress definitions chart nginx-ingress.ingress.additionalAnnotations object {} Additional annotations that will be added across all ingress definitions in the format of {cert-manager.io/issuer: \"letsencrypt-prod\"} Enable client certificate authentication nginx.ingress.kubernetes.io/auth-tls-verify-client: \"optional\" Create the secret containing the trusted ca certificates nginx.ingress.kubernetes.io/auth-tls-secret: \"gluu/tls-certificate\" Specify the verification depth in the client certificates chain nginx.ingress.kubernetes.io/auth-tls-verify-depth: \"1\" Specify if certificates are passed to upstream server nginx.ingress.kubernetes.io/auth-tls-pass-certificate-to-upstream: \"true\" nginx-ingress.ingress.additionalLabels object {} Additional labels that will be added across all ingress definitions in the format of nginx-ingress.ingress.adminUiAdditionalAnnotations object {} openid-configuration ingress resource additional annotations. nginx-ingress.ingress.adminUiLabels object {} Admin UI ingress resource labels. key app is taken. nginx-ingress.ingress.authServerAdditionalAnnotations object {} Auth server ingress resource additional annotations. nginx-ingress.ingress.authServerLabels object {} Auth server ingress resource labels. key app is taken nginx-ingress.ingress.authServerProtectedRegisterAdditionalAnnotations object {} Auth server protected register ingress resource additional annotations. nginx-ingress.ingress.authServerProtectedRegisterLabels object {} Auth server protected token ingress resource labels. key app is taken nginx-ingress.ingress.authServerProtectedTokenAdditionalAnnotations object {} Auth server protected token ingress resource additional annotations. nginx-ingress.ingress.authServerProtectedTokenLabels object {} Auth server protected token ingress resource labels. key app is taken nginx-ingress.ingress.casaAdditionalAnnotations object {} Casa ingress resource additional annotations. nginx-ingress.ingress.casaLabels object {} Casa ingress resource labels. key app is taken nginx-ingress.ingress.configApiAdditionalAnnotations object {} ConfigAPI ingress resource additional annotations. nginx-ingress.ingress.configApiLabels object {} configAPI ingress resource labels. key app is taken nginx-ingress.ingress.deviceCodeAdditionalAnnotations object {} device-code ingress resource additional annotations. nginx-ingress.ingress.deviceCodeLabels object {} device-code ingress resource labels. key app is taken nginx-ingress.ingress.fido2ConfigAdditionalAnnotations object {} fido2 config ingress resource additional annotations. nginx-ingress.ingress.fido2ConfigLabels object {} fido2 config ingress resource labels. key app is taken nginx-ingress.ingress.firebaseMessagingAdditionalAnnotations object {} Firebase Messaging ingress resource additional annotations. nginx-ingress.ingress.firebaseMessagingLabels object {} Firebase Messaging ingress resource labels. key app is taken nginx-ingress.ingress.openidAdditionalAnnotations object {} openid-configuration ingress resource additional annotations. nginx-ingress.ingress.openidConfigLabels object {} openid-configuration ingress resource labels. key app is taken nginx-ingress.ingress.scimAdditionalAnnotations object {} SCIM ingress resource additional annotations. nginx-ingress.ingress.scimConfigAdditionalAnnotations object {} SCIM config ingress resource additional annotations. nginx-ingress.ingress.scimConfigLabels object {} SCIM config ingress resource labels. key app is taken nginx-ingress.ingress.scimLabels object {} SCIM config ingress resource labels. key app is taken nginx-ingress.ingress.tls list [{\"hosts\":[\"demoexample.gluu.org\"],\"secretName\":\"tls-certificate\"}] Secrets holding HTTPS CA cert and key. nginx-ingress.ingress.u2fAdditionalAnnotations object {} u2f config ingress resource additional annotations. nginx-ingress.ingress.u2fConfigLabels object {} u2f config ingress resource labels. key app is taken nginx-ingress.ingress.uma2AdditionalAnnotations object {} uma2 config ingress resource additional annotations. nginx-ingress.ingress.uma2ConfigLabels object {} uma2 config ingress resource labels. key app is taken nginx-ingress.ingress.webdiscoveryAdditionalAnnotations object {} webdiscovery ingress resource additional annotations. nginx-ingress.ingress.webdiscoveryLabels object {} webdiscovery ingress resource labels. key app is taken nginx-ingress.ingress.webfingerAdditionalAnnotations object {} webfinger ingress resource additional annotations. nginx-ingress.ingress.webfingerLabels object {} webfinger ingress resource labels. key app is taken opendj object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"backup\":{\"cronJobSchedule\":\"*/59 * * * *\",\"enabled\":true},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"hpa\":{\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"gluufederation/opendj\",\"tag\":\"5.0.0_dev\"},\"livenessProbe\":{\"exec\":{\"command\":[\"python3\",\"/app/scripts/healthcheck.py\"]},\"failureThreshold\":20,\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5},\"pdb\":{\"enabled\":true,\"maxUnavailable\":1},\"persistence\":{\"size\":\"5Gi\"},\"ports\":{\"tcp-admin\":{\"nodePort\":\"\",\"port\":4444,\"protocol\":\"TCP\",\"targetPort\":4444},\"tcp-ldap\":{\"nodePort\":\"\",\"port\":1389,\"protocol\":\"TCP\",\"targetPort\":1389},\"tcp-ldaps\":{\"nodePort\":\"\",\"port\":1636,\"protocol\":\"TCP\",\"targetPort\":1636},\"tcp-repl\":{\"nodePort\":\"\",\"port\":8989,\"protocol\":\"TCP\",\"targetPort\":8989},\"tcp-serf\":{\"nodePort\":\"\",\"port\":7946,\"protocol\":\"TCP\",\"targetPort\":7946},\"udp-serf\":{\"nodePort\":\"\",\"port\":7946,\"protocol\":\"UDP\",\"targetPort\":7946}},\"readinessProbe\":{\"failureThreshold\":20,\"initialDelaySeconds\":60,\"periodSeconds\":25,\"tcpSocket\":{\"port\":1636},\"timeoutSeconds\":5},\"replicas\":1,\"resources\":{\"limits\":{\"cpu\":\"1500m\",\"memory\":\"2000Mi\"},\"requests\":{\"cpu\":\"1500m\",\"memory\":\"2000Mi\"}},\"topologySpreadConstraints\":{},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} OpenDJ is a directory server which implements a wide range of Lightweight Directory Access Protocol and related standards, including full compliance with LDAPv3 but also support for Directory Service Markup Language (DSMLv2).Written in Java, OpenDJ offers multi-master replication, access control, and many extensions. opendj.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of opendj.additionalLabels object {} Additional labels that will be added across the gateway in the format of opendj.backup object {\"cronJobSchedule\":\"*/59 * * * *\",\"enabled\":true} Configure ldap backup cronjob opendj.dnsConfig object {} Add custom dns config opendj.dnsPolicy string \"\" Add custom dns policy opendj.hpa object {\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50} Configure the HorizontalPodAutoscaler opendj.hpa.behavior object {} Scaling Policies opendj.hpa.metrics list [] metrics if targetCPUUtilizationPercentage is not set opendj.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. opendj.image.pullSecrets list [] Image Pull Secrets opendj.image.repository string \"gluufederation/opendj\" Image to use for deploying. opendj.image.tag string \"5.0.0_dev\" Image tag to use for deploying. opendj.livenessProbe object {\"exec\":{\"command\":[\"python3\",\"/app/scripts/healthcheck.py\"]},\"failureThreshold\":20,\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5} Configure the liveness healthcheck for OpenDJ if needed. https://github.com/GluuFederation/docker-opendj/blob/master/scripts/healthcheck.py opendj.livenessProbe.exec object {\"command\":[\"python3\",\"/app/scripts/healthcheck.py\"]} Executes the python3 healthcheck. opendj.pdb object {\"enabled\":true,\"maxUnavailable\":1} Configure the PodDisruptionBudget opendj.persistence.size string \"5Gi\" OpenDJ volume size opendj.readinessProbe object {\"failureThreshold\":20,\"initialDelaySeconds\":60,\"periodSeconds\":25,\"tcpSocket\":{\"port\":1636},\"timeoutSeconds\":5} Configure the readiness healthcheck for OpenDJ if needed. https://github.com/GluuFederation/docker-opendj/blob/master/scripts/healthcheck.py opendj.replicas int 1 Service replica number. opendj.resources object {\"limits\":{\"cpu\":\"1500m\",\"memory\":\"2000Mi\"},\"requests\":{\"cpu\":\"1500m\",\"memory\":\"2000Mi\"}} Resource specs. opendj.resources.limits.cpu string \"1500m\" CPU limit. opendj.resources.limits.memory string \"2000Mi\" Memory limit. opendj.resources.requests.cpu string \"1500m\" CPU request. opendj.resources.requests.memory string \"2000Mi\" Memory request. opendj.topologySpreadConstraints object {} Configure the topology spread constraints. Notice this is a map NOT a list as in the upstream API https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ opendj.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service opendj.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 opendj.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 opendj.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers opendj.volumes list [] Configure any additional volumes that need to be attached to the pod oxpassport object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"hpa\":{\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"gluufederation/oxpassport\",\"tag\":\"5.0.0_dev\"},\"livenessProbe\":{\"failureThreshold\":20,\"httpGet\":{\"path\":\"/passport/health-check\",\"port\":\"http-passport\"},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5},\"pdb\":{\"enabled\":true,\"maxUnavailable\":\"90%\"},\"readinessProbe\":{\"failureThreshold\":20,\"httpGet\":{\"path\":\"/passport/health-check\",\"port\":\"http-passport\"},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5},\"replicas\":1,\"resources\":{\"limits\":{\"cpu\":\"700m\",\"memory\":\"900Mi\"},\"requests\":{\"cpu\":\"700m\",\"memory\":\"900Mi\"}},\"topologySpreadConstraints\":{},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} Gluu interface to Passport.js to support social login and inbound identity. oxpassport.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of oxpassport.additionalLabels object {} Additional labels that will be added across the gateway in the format of oxpassport.dnsConfig object {} Add custom dns config oxpassport.dnsPolicy string \"\" Add custom dns policy oxpassport.hpa object {\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50} Configure the HorizontalPodAutoscaler oxpassport.hpa.behavior object {} Scaling Policies oxpassport.hpa.metrics list [] metrics if targetCPUUtilizationPercentage is not set oxpassport.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. oxpassport.image.pullSecrets list [] Image Pull Secrets oxpassport.image.repository string \"gluufederation/oxpassport\" Image to use for deploying. oxpassport.image.tag string \"5.0.0_dev\" Image tag to use for deploying. oxpassport.livenessProbe object {\"failureThreshold\":20,\"httpGet\":{\"path\":\"/passport/health-check\",\"port\":\"http-passport\"},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5} Configure the liveness healthcheck for oxPassport if needed. oxpassport.livenessProbe.httpGet.path string \"/passport/health-check\" http liveness probe endpoint oxpassport.pdb object {\"enabled\":true,\"maxUnavailable\":\"90%\"} Configure the PodDisruptionBudget oxpassport.readinessProbe object {\"failureThreshold\":20,\"httpGet\":{\"path\":\"/passport/health-check\",\"port\":\"http-passport\"},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5} Configure the readiness healthcheck for the oxPassport if needed. oxpassport.readinessProbe.httpGet.path string \"/passport/health-check\" http readiness probe endpoint oxpassport.replicas int 1 Service replica number oxpassport.resources object {\"limits\":{\"cpu\":\"700m\",\"memory\":\"900Mi\"},\"requests\":{\"cpu\":\"700m\",\"memory\":\"900Mi\"}} Resource specs. oxpassport.resources.limits.cpu string \"700m\" CPU limit. oxpassport.resources.limits.memory string \"900Mi\" Memory limit. oxpassport.resources.requests.cpu string \"700m\" CPU request. oxpassport.resources.requests.memory string \"900Mi\" Memory request. oxpassport.topologySpreadConstraints object {} Configure the topology spread constraints. Notice this is a map NOT a list as in the upstream API https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ oxpassport.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service oxpassport.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 oxpassport.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 oxpassport.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers oxpassport.volumes list [] Configure any additional volumes that need to be attached to the pod oxshibboleth object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"hpa\":{\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"gluufederation/oxshibboleth\",\"tag\":\"5.0.0_dev\"},\"livenessProbe\":{\"httpGet\":{\"path\":\"/idp\",\"port\":\"http-oxshib\"},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5},\"pdb\":{\"enabled\":true,\"maxUnavailable\":1},\"readinessProbe\":{\"httpGet\":{\"path\":\"/idp\",\"port\":\"http-oxshib\"},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5},\"replicas\":1,\"resources\":{\"limits\":{\"cpu\":\"1000m\",\"memory\":\"1000Mi\"},\"requests\":{\"cpu\":\"1000m\",\"memory\":\"1000Mi\"}},\"topologySpreadConstraints\":{},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} Shibboleth project for the Gluu Server's SAML IDP functionality. oxshibboleth.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of oxshibboleth.additionalLabels object {} Additional labels that will be added across the gateway in the format of oxshibboleth.dnsConfig object {} Add custom dns config oxshibboleth.dnsPolicy string \"\" Add custom dns policy oxshibboleth.hpa object {\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50} Configure the HorizontalPodAutoscaler oxshibboleth.hpa.behavior object {} Scaling Policies oxshibboleth.hpa.metrics list [] metrics if targetCPUUtilizationPercentage is not set oxshibboleth.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. oxshibboleth.image.pullSecrets list [] Image Pull Secrets oxshibboleth.image.repository string \"gluufederation/oxshibboleth\" Image to use for deploying. oxshibboleth.image.tag string \"5.0.0_dev\" Image tag to use for deploying. oxshibboleth.livenessProbe object {\"httpGet\":{\"path\":\"/idp\",\"port\":\"http-oxshib\"},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5} Configure the liveness healthcheck for the oxShibboleth if needed. oxshibboleth.livenessProbe.httpGet.path string \"/idp\" http liveness probe endpoint oxshibboleth.pdb object {\"enabled\":true,\"maxUnavailable\":1} Configure the PodDisruptionBudget oxshibboleth.readinessProbe object {\"httpGet\":{\"path\":\"/idp\",\"port\":\"http-oxshib\"},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5} Configure the readiness healthcheck for the casa if needed. oxshibboleth.readinessProbe.httpGet.path string \"/idp\" http liveness probe endpoint oxshibboleth.replicas int 1 Service replica number. oxshibboleth.resources object {\"limits\":{\"cpu\":\"1000m\",\"memory\":\"1000Mi\"},\"requests\":{\"cpu\":\"1000m\",\"memory\":\"1000Mi\"}} Resource specs. oxshibboleth.resources.limits.cpu string \"1000m\" CPU limit. oxshibboleth.resources.limits.memory string \"1000Mi\" Memory limit. oxshibboleth.resources.requests.cpu string \"1000m\" CPU request. oxshibboleth.resources.requests.memory string \"1000Mi\" Memory request. oxshibboleth.topologySpreadConstraints object {} Configure the topology spread constraints. Notice this is a map NOT a list as in the upstream API https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ oxshibboleth.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service oxshibboleth.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 oxshibboleth.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 oxshibboleth.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers oxshibboleth.volumes list [] Configure any additional volumes that need to be attached to the pod persistence object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"janssenproject/persistence-loader\",\"tag\":\"1.0.5-1\"},\"resources\":{\"limits\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"},\"requests\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"}},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} Job to generate data and initial config for Gluu Server persistence layer. persistence.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of persistence.additionalLabels object {} Additional labels that will be added across the gateway in the format of persistence.dnsConfig object {} Add custom dns config persistence.dnsPolicy string \"\" Add custom dns policy persistence.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. persistence.image.pullSecrets list [] Image Pull Secrets persistence.image.repository string \"janssenproject/persistence-loader\" Image to use for deploying. persistence.image.tag string \"1.0.5-1\" Image tag to use for deploying. persistence.resources object {\"limits\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"},\"requests\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"}} Resource specs. persistence.resources.limits.cpu string \"300m\" CPU limit persistence.resources.limits.memory string \"300Mi\" Memory limit. persistence.resources.requests.cpu string \"300m\" CPU request. persistence.resources.requests.memory string \"300Mi\" Memory request. persistence.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service persistence.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 persistence.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 persistence.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers persistence.volumes list [] Configure any additional volumes that need to be attached to the pod scim object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"hpa\":{\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"janssenproject/scim\",\"tag\":\"1.0.5-1\"},\"livenessProbe\":{\"httpGet\":{\"path\":\"/jans-scim/sys/health-check\",\"port\":8080},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5},\"pdb\":{\"enabled\":true,\"maxUnavailable\":\"90%\"},\"readinessProbe\":{\"httpGet\":{\"path\":\"/jans-scim/sys/health-check\",\"port\":8080},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5},\"replicas\":1,\"resources\":{\"limits\":{\"cpu\":\"1000m\",\"memory\":\"1000Mi\"},\"requests\":{\"cpu\":\"1000m\",\"memory\":\"1000Mi\"}},\"service\":{\"name\":\"http-scim\",\"port\":8080},\"topologySpreadConstraints\":{},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} System for Cross-domain Identity Management (SCIM) version 2.0 scim.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of scim.additionalLabels object {} Additional labels that will be added across the gateway in the format of scim.dnsConfig object {} Add custom dns config scim.dnsPolicy string \"\" Add custom dns policy scim.hpa object {\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50} Configure the HorizontalPodAutoscaler scim.hpa.behavior object {} Scaling Policies scim.hpa.metrics list [] metrics if targetCPUUtilizationPercentage is not set scim.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. scim.image.pullSecrets list [] Image Pull Secrets scim.image.repository string \"janssenproject/scim\" Image to use for deploying. scim.image.tag string \"1.0.5-1\" Image tag to use for deploying. scim.livenessProbe object {\"httpGet\":{\"path\":\"/jans-scim/sys/health-check\",\"port\":8080},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5} Configure the liveness healthcheck for SCIM if needed. scim.livenessProbe.httpGet.path string \"/jans-scim/sys/health-check\" http liveness probe endpoint scim.pdb object {\"enabled\":true,\"maxUnavailable\":\"90%\"} Configure the PodDisruptionBudget scim.readinessProbe object {\"httpGet\":{\"path\":\"/jans-scim/sys/health-check\",\"port\":8080},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5} Configure the readiness healthcheck for the SCIM if needed. scim.readinessProbe.httpGet.path string \"/jans-scim/sys/health-check\" http readiness probe endpoint scim.replicas int 1 Service replica number. scim.resources.limits.cpu string \"1000m\" CPU limit. scim.resources.limits.memory string \"1000Mi\" Memory limit. scim.resources.requests.cpu string \"1000m\" CPU request. scim.resources.requests.memory string \"1000Mi\" Memory request. scim.service.name string \"http-scim\" The name of the scim port within the scim service. Please keep it as default. scim.service.port int 8080 Port of the scim service. Please keep it as default. scim.topologySpreadConstraints object {} Configure the topology spread constraints. Notice this is a map NOT a list as in the upstream API https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ scim.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service scim.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 scim.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 scim.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers scim.volumes list [] Configure any additional volumes that need to be attached to the pod","title":"gluu"},{"location":"reference/kubernetes/helm-chart/#gluu","text":"Gluu Access and Identity Management Homepage: https://www.gluu.org","title":"gluu"},{"location":"reference/kubernetes/helm-chart/#maintainers","text":"Name Email Url moabu support@gluu.org","title":"Maintainers"},{"location":"reference/kubernetes/helm-chart/#source-code","text":"https://gluu.org/docs/gluu-server https://github.com/GluuFederation/flex/flex-cn-setup","title":"Source Code"},{"location":"reference/kubernetes/helm-chart/#requirements","text":"Kubernetes: >=v1.21.0-0 Repository Name Version admin-ui 5.0.10 auth-server 5.0.10 auth-server-key-rotation 5.0.10 casa 5.0.10 cn-istio-ingress 5.0.10 config 5.0.10 config-api 5.0.10 fido2 5.0.10 nginx-ingress 5.0.10 opendj 5.0.10 oxpassport 5.0.10 oxshibboleth 5.0.10 persistence 5.0.10 scim 5.0.10","title":"Requirements"},{"location":"reference/kubernetes/helm-chart/#values","text":"Key Type Default Description admin-ui object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"hpa\":{\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"gluufederation/admin-ui\",\"tag\":\"1.0.5-1\"},\"livenessProbe\":{\"failureThreshold\":20,\"initialDelaySeconds\":60,\"periodSeconds\":25,\"tcpSocket\":{\"port\":8080},\"timeoutSeconds\":5},\"pdb\":{\"enabled\":true,\"maxUnavailable\":\"90%\"},\"readinessProbe\":{\"failureThreshold\":20,\"initialDelaySeconds\":60,\"periodSeconds\":25,\"tcpSocket\":{\"port\":8080},\"timeoutSeconds\":5},\"replicas\":1,\"resources\":{\"limits\":{\"cpu\":\"2500m\",\"memory\":\"2500Mi\"},\"requests\":{\"cpu\":\"2500m\",\"memory\":\"2500Mi\"}},\"topologySpreadConstraints\":{},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} Admin GUI for configuration of the auth-server admin-ui.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of admin-ui.additionalLabels object {} Additional labels that will be added across the gateway in the format of admin-ui.dnsConfig object {} Add custom dns config admin-ui.dnsPolicy string \"\" Add custom dns policy admin-ui.hpa object {\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50} Configure the HorizontalPodAutoscaler admin-ui.hpa.behavior object {} Scaling Policies admin-ui.hpa.metrics list [] metrics if targetCPUUtilizationPercentage is not set admin-ui.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. admin-ui.image.pullSecrets list [] Image Pull Secrets admin-ui.image.repository string \"gluufederation/admin-ui\" Image to use for deploying. admin-ui.image.tag string \"1.0.5-1\" Image tag to use for deploying. admin-ui.livenessProbe object {\"failureThreshold\":20,\"initialDelaySeconds\":60,\"periodSeconds\":25,\"tcpSocket\":{\"port\":8080},\"timeoutSeconds\":5} Configure the liveness healthcheck for the admin ui if needed. admin-ui.pdb object {\"enabled\":true,\"maxUnavailable\":\"90%\"} Configure the PodDisruptionBudget admin-ui.readinessProbe object {\"failureThreshold\":20,\"initialDelaySeconds\":60,\"periodSeconds\":25,\"tcpSocket\":{\"port\":8080},\"timeoutSeconds\":5} Configure the readiness healthcheck for the admin ui if needed. admin-ui.replicas int 1 Service replica number. admin-ui.resources object {\"limits\":{\"cpu\":\"2500m\",\"memory\":\"2500Mi\"},\"requests\":{\"cpu\":\"2500m\",\"memory\":\"2500Mi\"}} Resource specs. admin-ui.resources.limits.cpu string \"2500m\" CPU limit. admin-ui.resources.limits.memory string \"2500Mi\" Memory limit. admin-ui.resources.requests.cpu string \"2500m\" CPU request. admin-ui.resources.requests.memory string \"2500Mi\" Memory request. admin-ui.topologySpreadConstraints object {} Configure the topology spread constraints. Notice this is a map NOT a list as in the upstream API https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ admin-ui.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service admin-ui.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 admin-ui.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 admin-ui.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers admin-ui.volumes list [] Configure any additional volumes that need to be attached to the pod auth-server object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"hpa\":{\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"janssenproject/auth-server\",\"tag\":\"1.0.5-1\"},\"livenessProbe\":{\"exec\":{\"command\":[\"python3\",\"/app/scripts/healthcheck.py\"]},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5},\"pdb\":{\"enabled\":true,\"maxUnavailable\":\"90%\"},\"readinessProbe\":{\"exec\":{\"command\":[\"python3\",\"/app/scripts/healthcheck.py\"]},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5},\"replicas\":1,\"resources\":{\"limits\":{\"cpu\":\"2500m\",\"memory\":\"2500Mi\"},\"requests\":{\"cpu\":\"2500m\",\"memory\":\"2500Mi\"}},\"topologySpreadConstraints\":{},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} OAuth Authorization Server, the OpenID Connect Provider, the UMA Authorization Server--this is the main Internet facing component of Gluu. It's the service that returns tokens, JWT's and identity assertions. This service must be Internet facing. auth-server-key-rotation object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"janssenproject/certmanager\",\"tag\":\"1.0.5-1\"},\"keysLife\":48,\"resources\":{\"limits\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"},\"requests\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"}},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} Responsible for regenerating auth-keys per x hours auth-server-key-rotation.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of auth-server-key-rotation.additionalLabels object {} Additional labels that will be added across the gateway in the format of auth-server-key-rotation.dnsConfig object {} Add custom dns config auth-server-key-rotation.dnsPolicy string \"\" Add custom dns policy auth-server-key-rotation.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. auth-server-key-rotation.image.pullSecrets list [] Image Pull Secrets auth-server-key-rotation.image.repository string \"janssenproject/certmanager\" Image to use for deploying. auth-server-key-rotation.image.tag string \"1.0.5-1\" Image tag to use for deploying. auth-server-key-rotation.keysLife int 48 Auth server key rotation keys life in hours auth-server-key-rotation.resources object {\"limits\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"},\"requests\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"}} Resource specs. auth-server-key-rotation.resources.limits.cpu string \"300m\" CPU limit. auth-server-key-rotation.resources.limits.memory string \"300Mi\" Memory limit. auth-server-key-rotation.resources.requests.cpu string \"300m\" CPU request. auth-server-key-rotation.resources.requests.memory string \"300Mi\" Memory request. auth-server-key-rotation.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service auth-server-key-rotation.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 auth-server-key-rotation.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 auth-server-key-rotation.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers auth-server-key-rotation.volumes list [] Configure any additional volumes that need to be attached to the pod auth-server.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of auth-server.additionalLabels object {} Additional labels that will be added across the gateway in the format of auth-server.dnsConfig object {} Add custom dns config auth-server.dnsPolicy string \"\" Add custom dns policy auth-server.hpa object {\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50} Configure the HorizontalPodAutoscaler auth-server.hpa.behavior object {} Scaling Policies auth-server.hpa.metrics list [] metrics if targetCPUUtilizationPercentage is not set auth-server.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. auth-server.image.pullSecrets list [] Image Pull Secrets auth-server.image.repository string \"janssenproject/auth-server\" Image to use for deploying. auth-server.image.tag string \"1.0.5-1\" Image tag to use for deploying. auth-server.livenessProbe object {\"exec\":{\"command\":[\"python3\",\"/app/scripts/healthcheck.py\"]},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5} Configure the liveness healthcheck for the auth server if needed. auth-server.livenessProbe.exec object {\"command\":[\"python3\",\"/app/scripts/healthcheck.py\"]} Executes the python3 healthcheck. https://github.com/JanssenProject/docker-jans-auth-server/blob/master/scripts/healthcheck.py auth-server.pdb object {\"enabled\":true,\"maxUnavailable\":\"90%\"} Configure the PodDisruptionBudget auth-server.readinessProbe object {\"exec\":{\"command\":[\"python3\",\"/app/scripts/healthcheck.py\"]},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5} Configure the readiness healthcheck for the auth server if needed. https://github.com/JanssenProject/docker-jans-auth-server/blob/master/scripts/healthcheck.py auth-server.replicas int 1 Service replica number. auth-server.resources object {\"limits\":{\"cpu\":\"2500m\",\"memory\":\"2500Mi\"},\"requests\":{\"cpu\":\"2500m\",\"memory\":\"2500Mi\"}} Resource specs. auth-server.resources.limits.cpu string \"2500m\" CPU limit. auth-server.resources.limits.memory string \"2500Mi\" Memory limit. auth-server.resources.requests.cpu string \"2500m\" CPU request. auth-server.resources.requests.memory string \"2500Mi\" Memory request. auth-server.topologySpreadConstraints object {} Configure the topology spread constraints. Notice this is a map NOT a list as in the upstream API https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ auth-server.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service auth-server.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 auth-server.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 auth-server.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers auth-server.volumes list [] Configure any additional volumes that need to be attached to the pod casa object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"hpa\":{\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"gluufederation/casa\",\"tag\":\"5.0.0-3\"},\"livenessProbe\":{\"httpGet\":{\"path\":\"/casa/health-check\",\"port\":\"http-casa\"},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5},\"pdb\":{\"enabled\":true,\"maxUnavailable\":\"90%\"},\"readinessProbe\":{\"httpGet\":{\"path\":\"/casa/health-check\",\"port\":\"http-casa\"},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5},\"replicas\":1,\"resources\":{\"limits\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"},\"requests\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"}},\"topologySpreadConstraints\":{},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} Gluu Casa (\"Casa\") is a self-service web portal for end-users to manage authentication and authorization preferences for their account in a Gluu Server. casa.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of casa.additionalLabels object {} Additional labels that will be added across the gateway in the format of casa.dnsConfig object {} Add custom dns config casa.dnsPolicy string \"\" Add custom dns policy casa.hpa object {\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50} Configure the HorizontalPodAutoscaler casa.hpa.behavior object {} Scaling Policies casa.hpa.metrics list [] metrics if targetCPUUtilizationPercentage is not set casa.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. casa.image.pullSecrets list [] Image Pull Secrets casa.image.repository string \"gluufederation/casa\" Image to use for deploying. casa.image.tag string \"5.0.0-3\" Image tag to use for deploying. casa.livenessProbe object {\"httpGet\":{\"path\":\"/casa/health-check\",\"port\":\"http-casa\"},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5} Configure the liveness healthcheck for casa if needed. casa.livenessProbe.httpGet.path string \"/casa/health-check\" http liveness probe endpoint casa.pdb object {\"enabled\":true,\"maxUnavailable\":\"90%\"} Configure the PodDisruptionBudget casa.readinessProbe object {\"httpGet\":{\"path\":\"/casa/health-check\",\"port\":\"http-casa\"},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5} Configure the readiness healthcheck for the casa if needed. casa.readinessProbe.httpGet.path string \"/casa/health-check\" http readiness probe endpoint casa.replicas int 1 Service replica number. casa.resources object {\"limits\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"},\"requests\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"}} Resource specs. casa.resources.limits.cpu string \"500m\" CPU limit. casa.resources.limits.memory string \"500Mi\" Memory limit. casa.resources.requests.cpu string \"500m\" CPU request. casa.resources.requests.memory string \"500Mi\" Memory request. casa.topologySpreadConstraints object {} Configure the topology spread constraints. Notice this is a map NOT a list as in the upstream API https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ casa.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service casa.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 casa.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 casa.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers casa.volumes list [] Configure any additional volumes that need to be attached to the pod config object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"adminPassword\":\"Test1234#\",\"city\":\"Austin\",\"configmap\":{\"cnAwsAccessKeyId\":\"\",\"cnAwsDefaultRegion\":\"us-west-1\",\"cnAwsProfile\":\"janssen\",\"cnAwsSecretAccessKey\":\"\",\"cnAwsSecretsEndpointUrl\":\"\",\"cnAwsSecretsNamePrefix\":\"janssen\",\"cnAwsSecretsReplicaRegions\":[],\"cnCacheType\":\"NATIVE_PERSISTENCE\",\"cnConfigGoogleSecretNamePrefix\":\"gluu\",\"cnConfigGoogleSecretVersionId\":\"latest\",\"cnConfigKubernetesConfigMap\":\"cn\",\"cnCouchbaseBucketPrefix\":\"jans\",\"cnCouchbaseCrt\":\"SWFtTm90YVNlcnZpY2VBY2NvdW50Q2hhbmdlTWV0b09uZQo=\",\"cnCouchbaseIndexNumReplica\":0,\"cnCouchbasePassword\":\"P@ssw0rd\",\"cnCouchbaseSuperUser\":\"admin\",\"cnCouchbaseSuperUserPassword\":\"Test1234#\",\"cnCouchbaseUrl\":\"cbgluu.default.svc.cluster.local\",\"cnCouchbaseUser\":\"gluu\",\"cnGoogleProjectId\":\"google-project-to-save-config-and-secrets-to\",\"cnGoogleSecretManagerPassPhrase\":\"Test1234#\",\"cnGoogleSecretManagerServiceAccount\":\"SWFtTm90YVNlcnZpY2VBY2NvdW50Q2hhbmdlTWV0b09uZQo=\",\"cnGoogleSpannerDatabaseId\":\"\",\"cnGoogleSpannerInstanceId\":\"\",\"cnJettyRequestHeaderSize\":8192,\"cnLdapUrl\":\"opendj:1636\",\"cnMaxRamPercent\":\"75.0\",\"cnPersistenceHybridMapping\":\"{}\",\"cnRedisSentinelGroup\":\"\",\"cnRedisSslTruststore\":\"\",\"cnRedisType\":\"STANDALONE\",\"cnRedisUrl\":\"redis.redis.svc.cluster.local:6379\",\"cnRedisUseSsl\":false,\"cnScimProtectionMode\":\"OAUTH\",\"cnSecretGoogleSecretNamePrefix\":\"gluu\",\"cnSecretGoogleSecretVersionId\":\"latest\",\"cnSecretKubernetesSecret\":\"cn\",\"cnSqlDbDialect\":\"mysql\",\"cnSqlDbHost\":\"my-release-mysql.default.svc.cluster.local\",\"cnSqlDbName\":\"jans\",\"cnSqlDbPort\":3306,\"cnSqlDbSchema\":\"\",\"cnSqlDbTimezone\":\"UTC\",\"cnSqlDbUser\":\"jans\",\"cnSqldbUserPassword\":\"Test1234#\",\"lbAddr\":\"\"},\"countryCode\":\"US\",\"dnsConfig\":{},\"dnsPolicy\":\"\",\"email\":\"support@gluu.org\",\"image\":{\"pullSecrets\":[],\"repository\":\"janssenproject/configurator\",\"tag\":\"1.0.5-1\"},\"ldapPassword\":\"P@ssw0rds\",\"migration\":{\"enabled\":false,\"migrationDataFormat\":\"ldif\",\"migrationDir\":\"/ce-migration\"},\"orgName\":\"Gluu\",\"redisPassword\":\"P@assw0rd\",\"resources\":{\"limits\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"},\"requests\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"}},\"state\":\"TX\",\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} Configuration parameters for setup and initial configuration secret and config layers used by Gluu services. config-api object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"hpa\":{\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"janssenproject/config-api\",\"tag\":\"1.0.5-1\"},\"livenessProbe\":{\"httpGet\":{\"path\":\"/jans-config-api/api/v1/health/live\",\"port\":8074},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5},\"pdb\":{\"enabled\":true,\"maxUnavailable\":\"90%\"},\"readinessProbe\":{\"httpGet\":{\"path\":\"jans-config-api/api/v1/health/ready\",\"port\":8074},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5},\"replicas\":1,\"resources\":{\"limits\":{\"cpu\":\"1000m\",\"memory\":\"400Mi\"},\"requests\":{\"cpu\":\"1000m\",\"memory\":\"400Mi\"}},\"topologySpreadConstraints\":{},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} Config Api endpoints can be used to configure the auth-server, which is an open-source OpenID Connect Provider (OP) and UMA Authorization Server (AS). config-api.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of config-api.additionalLabels object {} Additional labels that will be added across the gateway in the format of config-api.dnsConfig object {} Add custom dns config config-api.dnsPolicy string \"\" Add custom dns policy config-api.hpa object {\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50} Configure the HorizontalPodAutoscaler config-api.hpa.behavior object {} Scaling Policies config-api.hpa.metrics list [] metrics if targetCPUUtilizationPercentage is not set config-api.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. config-api.image.pullSecrets list [] Image Pull Secrets config-api.image.repository string \"janssenproject/config-api\" Image to use for deploying. config-api.image.tag string \"1.0.5-1\" Image tag to use for deploying. config-api.livenessProbe object {\"httpGet\":{\"path\":\"/jans-config-api/api/v1/health/live\",\"port\":8074},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5} Configure the liveness healthcheck for the auth server if needed. config-api.livenessProbe.httpGet object {\"path\":\"/jans-config-api/api/v1/health/live\",\"port\":8074} http liveness probe endpoint config-api.pdb object {\"enabled\":true,\"maxUnavailable\":\"90%\"} Configure the PodDisruptionBudget config-api.readinessProbe.httpGet object {\"path\":\"jans-config-api/api/v1/health/ready\",\"port\":8074} http readiness probe endpoint config-api.replicas int 1 Service replica number. config-api.resources object {\"limits\":{\"cpu\":\"1000m\",\"memory\":\"400Mi\"},\"requests\":{\"cpu\":\"1000m\",\"memory\":\"400Mi\"}} Resource specs. config-api.resources.limits.cpu string \"1000m\" CPU limit. config-api.resources.limits.memory string \"400Mi\" Memory limit. config-api.resources.requests.cpu string \"1000m\" CPU request. config-api.resources.requests.memory string \"400Mi\" Memory request. config-api.topologySpreadConstraints object {} Configure the topology spread constraints. Notice this is a map NOT a list as in the upstream API https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ config-api.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service config-api.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 config-api.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 config-api.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers config-api.volumes list [] Configure any additional volumes that need to be attached to the pod config.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of config.additionalLabels object {} Additional labels that will be added across the gateway in the format of config.adminPassword string \"Test1234#\" Admin password to log in to the UI. config.city string \"Austin\" City. Used for certificate creation. config.configmap.cnCacheType string \"NATIVE_PERSISTENCE\" Cache type. NATIVE_PERSISTENCE , REDIS . or IN_MEMORY . Defaults to NATIVE_PERSISTENCE . config.configmap.cnConfigGoogleSecretNamePrefix string \"gluu\" Prefix for Gluu configuration secret in Google Secret Manager. Defaults to gluu. If left intact gluu-configuration secret will be created. Used only when global.configAdapterName and global.configSecretAdapter is set to google. config.configmap.cnConfigGoogleSecretVersionId string \"latest\" Secret version to be used for configuration. Defaults to latest and should normally always stay that way. Used only when global.configAdapterName and global.configSecretAdapter is set to google. Used only when global.configAdapterName and global.configSecretAdapter is set to google. config.configmap.cnConfigKubernetesConfigMap string \"cn\" The name of the Kubernetes ConfigMap that will hold the configuration layer config.configmap.cnCouchbaseBucketPrefix string \"jans\" The prefix of couchbase buckets. This helps with separation in between different environments and allows for the same couchbase cluster to be used by different setups of Gluu. config.configmap.cnCouchbaseCrt string \"SWFtTm90YVNlcnZpY2VBY2NvdW50Q2hhbmdlTWV0b09uZQo=\" Couchbase certificate authority string. This must be encoded using base64. This can also be found in your couchbase UI Security > Root Certificate. In mTLS setups this is not required. config.configmap.cnCouchbaseIndexNumReplica int 0 The number of replicas per index created. Please note that the number of index nodes must be one greater than the number of index replicas. That means if your couchbase cluster only has 2 index nodes you cannot place the number of replicas to be higher than 1. config.configmap.cnCouchbasePassword string \"P@ssw0rd\" Couchbase password for the restricted user config.configmap.cnCouchbaseUser that is often used inside the services. The password must contain one digit, one uppercase letter, one lower case letter and one symbol . config.configmap.cnCouchbaseSuperUser string \"admin\" The Couchbase super user (admin) username. This user is used during initialization only. config.configmap.cnCouchbaseSuperUserPassword string \"Test1234#\" Couchbase password for the superuser config.configmap.cnCouchbaseSuperUser that is used during the initialization process. The password must contain one digit, one uppercase letter, one lower case letter and one symbol config.configmap.cnCouchbaseUrl string \"cbgluu.default.svc.cluster.local\" Couchbase URL. Used only when global.cnPersistenceType is hybrid or couchbase. This should be in FQDN format for either remote or local Couchbase clusters. The address can be an internal address inside the kubernetes cluster config.configmap.cnCouchbaseUser string \"gluu\" Couchbase restricted user. Used only when global.cnPersistenceType is hybrid or couchbase. config.configmap.cnGoogleProjectId string \"google-project-to-save-config-and-secrets-to\" Project id of the Google project the secret manager belongs to. Used only when global.configAdapterName and global.configSecretAdapter is set to google. config.configmap.cnGoogleSecretManagerPassPhrase string \"Test1234#\" Passphrase for Gluu secret in Google Secret Manager. This is used for encrypting and decrypting data from the Google Secret Manager. Used only when global.configAdapterName and global.configSecretAdapter is set to google. config.configmap.cnGoogleSecretManagerServiceAccount string \"SWFtTm90YVNlcnZpY2VBY2NvdW50Q2hhbmdlTWV0b09uZQo=\" Service account with roles roles/secretmanager.admin base64 encoded string. This is used often inside the services to reach the configuration layer. Used only when global.configAdapterName and global.configSecretAdapter is set to google. config.configmap.cnGoogleSpannerDatabaseId string \"\" Google Spanner Database ID. Used only when global.cnPersistenceType is spanner. config.configmap.cnGoogleSpannerInstanceId string \"\" Google Spanner ID. Used only when global.cnPersistenceType is spanner. config.configmap.cnJettyRequestHeaderSize int 8192 Jetty header size in bytes in the auth server config.configmap.cnLdapUrl string \"opendj:1636\" OpenDJ internal address. Leave as default. Used when global.cnPersistenceType is set to ldap . config.configmap.cnMaxRamPercent string \"75.0\" Value passed to Java option -XX:MaxRAMPercentage config.configmap.cnPersistenceHybridMapping string \"{}\" Specify data that should be saved in LDAP (one of default, user, cache, site, token, or session; default to default). Note this environment only takes effect when global.cnPersistenceType is set to hybrid . { \"default\": \"<couchbase config.configmap.cnRedisSentinelGroup string \"\" Redis Sentinel Group. Often set when config.configmap.cnRedisType is set to SENTINEL . Can be used when config.configmap.cnCacheType is set to REDIS . config.configmap.cnRedisSslTruststore string \"\" Redis SSL truststore. Optional. Can be used when config.configmap.cnCacheType is set to REDIS . config.configmap.cnRedisType string \"STANDALONE\" Redis service type. STANDALONE or CLUSTER . Can be used when config.configmap.cnCacheType is set to REDIS . config.configmap.cnRedisUrl string \"redis.redis.svc.cluster.local:6379\" Redis URL and port number : . Can be used when config.configmap.cnCacheType is set to REDIS . config.configmap.cnRedisUseSsl bool false Boolean to use SSL in Redis. Can be used when config.configmap.cnCacheType is set to REDIS . config.configmap.cnScimProtectionMode string \"OAUTH\" SCIM protection mode OAUTH config.configmap.cnSecretGoogleSecretNamePrefix string \"gluu\" Prefix for Gluu secret in Google Secret Manager. Defaults to gluu. If left gluu-secret secret will be created. Used only when global.configAdapterName and global.configSecretAdapter is set to google. config.configmap.cnSecretGoogleSecretVersionId string \"latest\" Secret version to be used for secret configuration. Defaults to latest and should normally always stay that way. Used only when global.configAdapterName and global.configSecretAdapter is set to google. config.configmap.cnSecretKubernetesSecret string \"cn\" Kubernetes secret name holding configuration keys. Used when global.configSecretAdapter is set to kubernetes which is the default. config.configmap.cnSqlDbDialect string \"mysql\" SQL database dialect. mysql or pgsql config.configmap.cnSqlDbHost string \"my-release-mysql.default.svc.cluster.local\" SQL database host uri. config.configmap.cnSqlDbName string \"jans\" SQL database name. config.configmap.cnSqlDbPort int 3306 SQL database port. config.configmap.cnSqlDbSchema string \"\" Schema name used by SQL database (default to empty-string; if using MySQL, the schema name will be resolved as the database name, whereas in PostgreSQL the schema name will be resolved as \"public\" ). config.configmap.cnSqlDbTimezone string \"UTC\" SQL database timezone. config.configmap.cnSqlDbUser string \"jans\" SQL database username. config.configmap.cnSqldbUserPassword string \"Test1234#\" SQL password injected the secrets . config.configmap.lbAddr string \"\" Load balancer address for AWS if the FQDN is not registered. config.countryCode string \"US\" Country code. Used for certificate creation. config.dnsConfig object {} Add custom dns config config.dnsPolicy string \"\" Add custom dns policy config.email string \"support@gluu.org\" Email address of the administrator usually. Used for certificate creation. config.image.pullSecrets list [] Image Pull Secrets config.image.repository string \"janssenproject/configurator\" Image to use for deploying. config.image.tag string \"1.0.5-1\" Image tag to use for deploying. config.ldapPassword string \"P@ssw0rds\" LDAP admin password if OpenDJ is used for persistence. config.migration object {\"enabled\":false,\"migrationDataFormat\":\"ldif\",\"migrationDir\":\"/ce-migration\"} CE to CN Migration section config.migration.enabled bool false Boolean flag to enable migration from CE config.migration.migrationDataFormat string \"ldif\" migration data-format depending on persistence backend. Supported data formats are ldif, couchbase+json, spanner+avro, postgresql+json, and mysql+json. config.migration.migrationDir string \"/ce-migration\" Directory holding all migration files config.orgName string \"Gluu\" Organization name. Used for certificate creation. config.redisPassword string \"P@assw0rd\" Redis admin password if config.configmap.cnCacheType is set to REDIS . config.resources object {\"limits\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"},\"requests\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"}} Resource specs. config.resources.limits.cpu string \"300m\" CPU limit. config.resources.limits.memory string \"300Mi\" Memory limit. config.resources.requests.cpu string \"300m\" CPU request. config.resources.requests.memory string \"300Mi\" Memory request. config.state string \"TX\" State code. Used for certificate creation. config.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service. config.usrEnvs.normal object {} Add custom normal envs to the service. variable1: value1 config.usrEnvs.secret object {} Add custom secret envs to the service. variable1: value1 config.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers config.volumes list [] Configure any additional volumes that need to be attached to the pod fido2 object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"hpa\":{\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"janssenproject/fido2\",\"tag\":\"1.0.5-1\"},\"livenessProbe\":{\"httpGet\":{\"path\":\"/jans-fido2/sys/health-check\",\"port\":\"http-fido2\"},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5},\"pdb\":{\"enabled\":true,\"maxUnavailable\":\"90%\"},\"readinessProbe\":{\"httpGet\":{\"path\":\"/jans-fido2/sys/health-check\",\"port\":\"http-fido2\"},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5},\"replicas\":1,\"resources\":{\"limits\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"},\"requests\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"}},\"service\":{\"name\":\"http-fido2\",\"port\":8080},\"topologySpreadConstraints\":{},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} FIDO 2.0 (FIDO2) is an open authentication standard that enables leveraging common devices to authenticate to online services in both mobile and desktop environments. fido2.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of fido2.additionalLabels object {} Additional labels that will be added across the gateway in the format of fido2.dnsConfig object {} Add custom dns config fido2.dnsPolicy string \"\" Add custom dns policy fido2.hpa object {\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50} Configure the HorizontalPodAutoscaler fido2.hpa.behavior object {} Scaling Policies fido2.hpa.metrics list [] metrics if targetCPUUtilizationPercentage is not set fido2.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. fido2.image.pullSecrets list [] Image Pull Secrets fido2.image.repository string \"janssenproject/fido2\" Image to use for deploying. fido2.image.tag string \"1.0.5-1\" Image tag to use for deploying. fido2.livenessProbe object {\"httpGet\":{\"path\":\"/jans-fido2/sys/health-check\",\"port\":\"http-fido2\"},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5} Configure the liveness healthcheck for the fido2 if needed. fido2.livenessProbe.httpGet object {\"path\":\"/jans-fido2/sys/health-check\",\"port\":\"http-fido2\"} http liveness probe endpoint fido2.pdb object {\"enabled\":true,\"maxUnavailable\":\"90%\"} Configure the PodDisruptionBudget fido2.readinessProbe object {\"httpGet\":{\"path\":\"/jans-fido2/sys/health-check\",\"port\":\"http-fido2\"},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5} Configure the readiness healthcheck for the fido2 if needed. fido2.replicas int 1 Service replica number. fido2.resources object {\"limits\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"},\"requests\":{\"cpu\":\"500m\",\"memory\":\"500Mi\"}} Resource specs. fido2.resources.limits.cpu string \"500m\" CPU limit. fido2.resources.limits.memory string \"500Mi\" Memory limit. fido2.resources.requests.cpu string \"500m\" CPU request. fido2.resources.requests.memory string \"500Mi\" Memory request. fido2.service.name string \"http-fido2\" The name of the fido2 port within the fido2 service. Please keep it as default. fido2.service.port int 8080 Port of the fido2 service. Please keep it as default. fido2.topologySpreadConstraints object {} Configure the topology spread constraints. Notice this is a map NOT a list as in the upstream API https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ fido2.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service fido2.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 fido2.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 fido2.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers fido2.volumes list [] Configure any additional volumes that need to be attached to the pod global object {\"admin-ui\":{\"adminUiServiceName\":\"admin-ui\",\"enabled\":true,\"ingress\":{\"adminUiEnabled\":false}},\"alb\":{\"ingress\":false},\"auth-server\":{\"appLoggers\":{\"auditStatsLogLevel\":\"INFO\",\"auditStatsLogTarget\":\"FILE\",\"authLogLevel\":\"INFO\",\"authLogTarget\":\"STDOUT\",\"httpLogLevel\":\"INFO\",\"httpLogTarget\":\"FILE\",\"ldapStatsLogLevel\":\"INFO\",\"ldapStatsLogTarget\":\"FILE\",\"persistenceDurationLogLevel\":\"INFO\",\"persistenceDurationLogTarget\":\"FILE\",\"persistenceLogLevel\":\"INFO\",\"persistenceLogTarget\":\"FILE\",\"scriptLogLevel\":\"INFO\",\"scriptLogTarget\":\"FILE\"},\"authEncKeys\":\"RSA1_5 RSA-OAEP\",\"authServerServiceName\":\"auth-server\",\"authSigKeys\":\"RS256 RS384 RS512 ES256 ES384 ES512 PS256 PS384 PS512\",\"enabled\":true,\"ingress\":{\"authServerEnabled\":true,\"authServerProtectedRegister\":false,\"authServerProtectedToken\":false,\"deviceCodeEnabled\":true,\"firebaseMessagingEnabled\":true,\"openidConfigEnabled\":true,\"u2fConfigEnabled\":true,\"uma2ConfigEnabled\":true,\"webdiscoveryEnabled\":true,\"webfingerEnabled\":true}},\"auth-server-key-rotation\":{\"enabled\":false},\"awsStorageType\":\"io1\",\"azureStorageAccountType\":\"Standard_LRS\",\"azureStorageKind\":\"Managed\",\"casa\":{\"casaServiceName\":\"casa\",\"enabled\":true,\"ingress\":{\"casaEnabled\":false}},\"cloud\":{\"testEnviroment\":false},\"cnAwsConfigFile\":\"/etc/jans/conf/aws_config_file\",\"cnAwsSecretsReplicaRegionsFile\":\"/etc/jans/conf/aws_secrets_replica_regions\",\"cnAwsSharedCredentialsFile\":\"/etc/jans/conf/aws_shared_credential_file\",\"cnDocumentStoreType\":\"LOCAL\",\"cnGoogleApplicationCredentials\":\"/etc/jans/conf/google-credentials.json\",\"cnObExtSigningAlias\":\"\",\"cnObExtSigningJwksCrt\":\"\",\"cnObExtSigningJwksKey\":\"\",\"cnObExtSigningJwksKeyPassPhrase\":\"\",\"cnObExtSigningJwksUri\":\"\",\"cnObStaticSigningKeyKid\":\"\",\"cnObTransportAlias\":\"\",\"cnObTransportCrt\":\"\",\"cnObTransportKey\":\"\",\"cnObTransportKeyPassPhrase\":\"\",\"cnObTransportTrustStore\":\"\",\"cnPersistenceType\":\"sql\",\"cnPrometheusPort\":\"\",\"config\":{\"enabled\":true},\"config-api\":{\"adminUiAppLoggers\":{\"adminUiAuditLogLevel\":\"INFO\",\"adminUiAuditLogTarget\":\"FILE\",\"adminUiLogLevel\":\"INFO\",\"adminUiLogTarget\":\"FILE\"},\"appLoggers\":{\"configApiLogLevel\":\"INFO\",\"configApiLogTarget\":\"STDOUT\",\"ldapStatsLogLevel\":\"INFO\",\"ldapStatsLogTarget\":\"FILE\",\"persistenceDurationLogLevel\":\"INFO\",\"persistenceDurationLogTarget\":\"FILE\",\"persistenceLogLevel\":\"INFO\",\"persistenceLogTarget\":\"FILE\",\"scriptLogLevel\":\"INFO\",\"scriptLogTarget\":\"FILE\"},\"configApiServerServiceName\":\"config-api\",\"enabled\":true,\"ingress\":{\"configApiEnabled\":true}},\"configAdapterName\":\"kubernetes\",\"configSecretAdapter\":\"kubernetes\",\"distribution\":\"default\",\"fido2\":{\"appLoggers\":{\"fido2LogLevel\":\"INFO\",\"fido2LogTarget\":\"STDOUT\",\"persistenceLogLevel\":\"INFO\",\"persistenceLogTarget\":\"FILE\"},\"enabled\":true,\"fido2ServiceName\":\"fido2\",\"ingress\":{\"fido2ConfigEnabled\":false}},\"fqdn\":\"demoexample.gluu.org\",\"gcePdStorageType\":\"pd-standard\",\"isFqdnRegistered\":false,\"istio\":{\"additionalAnnotations\":{},\"additionalLabels\":{},\"enabled\":false,\"gateways\":[],\"ingress\":false,\"namespace\":\"istio-system\"},\"jobTtlSecondsAfterFinished\":300,\"lbIp\":\"22.22.22.22\",\"nginx-ingress\":{\"enabled\":true},\"opendj\":{\"enabled\":false,\"ldapServiceName\":\"opendj\"},\"oxpassport\":{\"enabled\":false,\"oxPassportServiceName\":\"oxpassport\"},\"oxshibboleth\":{\"appLoggers\":{\"auditStatsLogLevel\":\"INFO\",\"auditStatsLogTarget\":\"FILE\",\"consentAuditLogLevel\":\"INFO\",\"consentAuditLogTarget\":\"FILE\",\"containerLogLevel\":\"\",\"encryptionLogLevel\":\"\",\"httpclientLogLevel\":\"\",\"idpLogLevel\":\"INFO\",\"idpLogTarget\":\"STDOUT\",\"ldapLogLevel\":\"\",\"messagesLogLevel\":\"\",\"opensamlLogLevel\":\"\",\"propsLogLevel\":\"\",\"scriptLogLevel\":\"INFO\",\"scriptLogTarget\":\"FILE\",\"springLogLevel\":\"\",\"xmlsecLogLevel\":\"\"},\"enabled\":false,\"oxShibbolethServiceName\":\"oxshibboleth\"},\"persistence\":{\"enabled\":true},\"scim\":{\"appLoggers\":{\"ldapStatsLogLevel\":\"INFO\",\"ldapStatsLogTarget\":\"FILE\",\"persistenceDurationLogLevel\":\"INFO\",\"persistenceDurationLogTarget\":\"FILE\",\"persistenceLogLevel\":\"INFO\",\"persistenceLogTarget\":\"FILE\",\"scimLogLevel\":\"INFO\",\"scimLogTarget\":\"STDOUT\",\"scriptLogLevel\":\"INFO\",\"scriptLogTarget\":\"FILE\"},\"enabled\":true,\"ingress\":{\"scimConfigEnabled\":false,\"scimEnabled\":false},\"scimServiceName\":\"scim\"},\"storageClass\":{\"allowVolumeExpansion\":true,\"allowedTopologies\":[],\"mountOptions\":[\"debug\"],\"parameters\":{},\"provisioner\":\"microk8s.io/hostpath\",\"reclaimPolicy\":\"Retain\",\"volumeBindingMode\":\"WaitForFirstConsumer\"},\"upgrade\":{\"enabled\":false},\"usrEnvs\":{\"normal\":{},\"secret\":{}}} Parameters used globally across all services helm charts. global.admin-ui.adminUiServiceName string \"admin-ui\" Name of the admin-ui service. Please keep it as default. global.admin-ui.enabled bool true Boolean flag to enable/disable the admin-ui chart and admin ui config api plugin. global.admin-ui.ingress.adminUiEnabled bool false Enable Admin UI endpoints in either istio or nginx ingress depending on users choice global.alb.ingress bool false Activates ALB ingress global.auth-server-key-rotation.enabled bool false Boolean flag to enable/disable the auth-server-key rotation cronjob chart. global.auth-server.appLoggers object {\"auditStatsLogLevel\":\"INFO\",\"auditStatsLogTarget\":\"FILE\",\"authLogLevel\":\"INFO\",\"authLogTarget\":\"STDOUT\",\"httpLogLevel\":\"INFO\",\"httpLogTarget\":\"FILE\",\"ldapStatsLogLevel\":\"INFO\",\"ldapStatsLogTarget\":\"FILE\",\"persistenceDurationLogLevel\":\"INFO\",\"persistenceDurationLogTarget\":\"FILE\",\"persistenceLogLevel\":\"INFO\",\"persistenceLogTarget\":\"FILE\",\"scriptLogLevel\":\"INFO\",\"scriptLogTarget\":\"FILE\"} App loggers can be configured to define where the logs will be redirected to and the level of each in which it should be displayed. global.auth-server.appLoggers.auditStatsLogLevel string \"INFO\" jans-auth_audit.log level global.auth-server.appLoggers.auditStatsLogTarget string \"FILE\" jans-auth_script.log target global.auth-server.appLoggers.authLogLevel string \"INFO\" jans-auth.log level global.auth-server.appLoggers.authLogTarget string \"STDOUT\" jans-auth.log target global.auth-server.appLoggers.httpLogLevel string \"INFO\" http_request_response.log level global.auth-server.appLoggers.httpLogTarget string \"FILE\" http_request_response.log target global.auth-server.appLoggers.ldapStatsLogLevel string \"INFO\" jans-auth_persistence_ldap_statistics.log level global.auth-server.appLoggers.ldapStatsLogTarget string \"FILE\" jans-auth_persistence_ldap_statistics.log target global.auth-server.appLoggers.persistenceDurationLogLevel string \"INFO\" jans-auth_persistence_duration.log level global.auth-server.appLoggers.persistenceDurationLogTarget string \"FILE\" jans-auth_persistence_duration.log target global.auth-server.appLoggers.persistenceLogLevel string \"INFO\" jans-auth_persistence.log level global.auth-server.appLoggers.persistenceLogTarget string \"FILE\" jans-auth_persistence.log target global.auth-server.appLoggers.scriptLogLevel string \"INFO\" jans-auth_script.log level global.auth-server.appLoggers.scriptLogTarget string \"FILE\" jans-auth_script.log target global.auth-server.authEncKeys string \"RSA1_5 RSA-OAEP\" space-separated key algorithm for encryption (default to RSA1_5 RSA-OAEP ) global.auth-server.authServerServiceName string \"auth-server\" Name of the auth-server service. Please keep it as default. global.auth-server.authSigKeys string \"RS256 RS384 RS512 ES256 ES384 ES512 PS256 PS384 PS512\" space-separated key algorithm for signing (default to RS256 RS384 RS512 ES256 ES384 ES512 PS256 PS384 PS512 ) global.auth-server.enabled bool true Boolean flag to enable/disable auth-server chart. You should never set this to false. global.auth-server.ingress object {\"authServerEnabled\":true,\"authServerProtectedRegister\":false,\"authServerProtectedToken\":false,\"deviceCodeEnabled\":true,\"firebaseMessagingEnabled\":true,\"openidConfigEnabled\":true,\"u2fConfigEnabled\":true,\"uma2ConfigEnabled\":true,\"webdiscoveryEnabled\":true,\"webfingerEnabled\":true} Enable endpoints in either istio or nginx ingress depending on users choice global.auth-server.ingress.authServerEnabled bool true Enable Auth server endpoints /jans-auth global.auth-server.ingress.authServerProtectedRegister bool false Enable mTLS onn Auth server endpoint /jans-auth/restv1/register. Currently not working in Istio. global.auth-server.ingress.authServerProtectedToken bool false Enable mTLS on Auth server endpoint /jans-auth/restv1/token. Currently not working in Istio. global.auth-server.ingress.deviceCodeEnabled bool true Enable endpoint /device-code global.auth-server.ingress.firebaseMessagingEnabled bool true Enable endpoint /firebase-messaging-sw.js global.auth-server.ingress.openidConfigEnabled bool true Enable endpoint /.well-known/openid-configuration global.auth-server.ingress.u2fConfigEnabled bool true Enable endpoint /.well-known/fido-configuration global.auth-server.ingress.uma2ConfigEnabled bool true Enable endpoint /.well-known/uma2-configuration global.auth-server.ingress.webdiscoveryEnabled bool true Enable endpoint /.well-known/simple-web-discovery global.auth-server.ingress.webfingerEnabled bool true Enable endpoint /.well-known/webfinger global.awsStorageType string \"io1\" Volume storage type if using AWS volumes. global.azureStorageAccountType string \"Standard_LRS\" Volume storage type if using Azure disks. global.azureStorageKind string \"Managed\" Azure storage kind if using Azure disks global.casa.casaServiceName string \"casa\" Name of the casa service. Please keep it as default. global.casa.enabled bool true Boolean flag to enable/disable the casa chart. global.casa.ingress object {\"casaEnabled\":false} Enable endpoints in either istio or nginx ingress depending on users choice global.casa.ingress.casaEnabled bool false Enable casa endpoints /casa global.cloud.testEnviroment bool false Boolean flag if enabled will strip resources requests and limits from all services. global.cnDocumentStoreType string \"LOCAL\" Document store type to use for shibboleth files LOCAL. global.cnGoogleApplicationCredentials string \"/etc/jans/conf/google-credentials.json\" Base64 encoded service account. The sa must have roles/secretmanager.admin to use Google secrets and roles/spanner.databaseUser to use Spanner. Leave as this is a sensible default. global.cnObExtSigningAlias string \"\" Open banking external signing AS Alias. This is a kid value.Used in SSA Validation, kid used while encoding a JWT sent to token URL i.e. XkwIzWy44xWSlcWnMiEc8iq9s2G global.cnObExtSigningJwksCrt string \"\" Open banking external signing jwks AS certificate authority string. Used in SSA Validation. This must be encoded using base64.. Used when .global.cnObExtSigningJwksUri is set. global.cnObExtSigningJwksKey string \"\" Open banking external signing jwks AS key string. Used in SSA Validation. This must be encoded using base64. Used when .global.cnObExtSigningJwksUri is set. global.cnObExtSigningJwksKeyPassPhrase string \"\" Open banking external signing jwks AS key passphrase to unlock provided key. This must be encoded using base64. Used when .global.cnObExtSigningJwksUri is set. global.cnObExtSigningJwksUri string \"\" Open banking external signing jwks uri. Used in SSA Validation. global.cnObStaticSigningKeyKid string \"\" Open banking signing AS kid to force the AS to use a specific signing key. i.e. Wy44xWSlcWnMiEc8iq9s2G global.cnObTransportAlias string \"\" Open banking transport Alias used inside the JVM. global.cnObTransportCrt string \"\" Open banking AS transport crt. Used in SSA Validation. This must be encoded using base64. global.cnObTransportKey string \"\" Open banking AS transport key. Used in SSA Validation. This must be encoded using base64. global.cnObTransportKeyPassPhrase string \"\" Open banking AS transport key passphrase to unlock AS transport key. This must be encoded using base64. global.cnObTransportTrustStore string \"\" Open banking AS transport truststore crt. This is normally generated from the OB issuing CA, OB Root CA and Signing CA. Used when .global.cnObExtSigningJwksUri is set. Used in SSA Validation. This must be encoded using base64. global.cnPersistenceType string \"sql\" Persistence backend to run Gluu with ldap global.cnPrometheusPort string \"\" Port used by Prometheus JMX agent (default to empty string). To enable Prometheus JMX agent, set the value to a number. global.config-api.adminUiAppLoggers.adminUiAuditLogLevel string \"INFO\" config-api admin-ui plugin audit log level global.config-api.adminUiAppLoggers.adminUiAuditLogTarget string \"FILE\" config-api admin-ui plugin audit log target global.config-api.adminUiAppLoggers.adminUiLogLevel string \"INFO\" config-api admin-ui plugin log target global.config-api.adminUiAppLoggers.adminUiLogTarget string \"FILE\" config-api admin-ui plugin log level global.config-api.appLoggers object {\"configApiLogLevel\":\"INFO\",\"configApiLogTarget\":\"STDOUT\",\"ldapStatsLogLevel\":\"INFO\",\"ldapStatsLogTarget\":\"FILE\",\"persistenceDurationLogLevel\":\"INFO\",\"persistenceDurationLogTarget\":\"FILE\",\"persistenceLogLevel\":\"INFO\",\"persistenceLogTarget\":\"FILE\",\"scriptLogLevel\":\"INFO\",\"scriptLogTarget\":\"FILE\"} App loggers can be configured to define where the logs will be redirected to and the level of each in which it should be displayed. global.config-api.appLoggers.configApiLogLevel string \"INFO\" configapi.log level global.config-api.appLoggers.configApiLogTarget string \"STDOUT\" configapi.log target global.config-api.appLoggers.ldapStatsLogLevel string \"INFO\" config-api_persistence_ldap_statistics.log level global.config-api.appLoggers.ldapStatsLogTarget string \"FILE\" config-api_persistence_ldap_statistics.log target global.config-api.appLoggers.persistenceDurationLogLevel string \"INFO\" config-api_persistence_duration.log level global.config-api.appLoggers.persistenceDurationLogTarget string \"FILE\" config-api_persistence_duration.log target global.config-api.appLoggers.persistenceLogLevel string \"INFO\" jans-auth_persistence.log level global.config-api.appLoggers.persistenceLogTarget string \"FILE\" config-api_persistence.log target global.config-api.appLoggers.scriptLogLevel string \"INFO\" config-api_script.log level global.config-api.appLoggers.scriptLogTarget string \"FILE\" config-api_script.log target global.config-api.configApiServerServiceName string \"config-api\" Name of the config-api service. Please keep it as default. global.config-api.enabled bool true Boolean flag to enable/disable the config-api chart. global.config-api.ingress object {\"configApiEnabled\":true} Enable endpoints in either istio or nginx ingress depending on users choice global.config.enabled bool true Boolean flag to enable/disable the configuration chart. This normally should never be false global.configAdapterName string \"kubernetes\" The config backend adapter that will hold Gluu configuration layer. aws global.configSecretAdapter string \"kubernetes\" The config backend adapter that will hold Gluu secret layer. aws global.distribution string \"default\" Gluu distributions supported are: default global.fido2.appLoggers object {\"fido2LogLevel\":\"INFO\",\"fido2LogTarget\":\"STDOUT\",\"persistenceLogLevel\":\"INFO\",\"persistenceLogTarget\":\"FILE\"} App loggers can be configured to define where the logs will be redirected to and the level of each in which it should be displayed. global.fido2.appLoggers.fido2LogLevel string \"INFO\" fido2.log level global.fido2.appLoggers.fido2LogTarget string \"STDOUT\" fido2.log target global.fido2.appLoggers.persistenceLogLevel string \"INFO\" fido2_persistence.log level global.fido2.appLoggers.persistenceLogTarget string \"FILE\" fido2_persistence.log target global.fido2.enabled bool true Boolean flag to enable/disable the fido2 chart. global.fido2.fido2ServiceName string \"fido2\" Name of the fido2 service. Please keep it as default. global.fido2.ingress object {\"fido2ConfigEnabled\":false} Enable endpoints in either istio or nginx ingress depending on users choice global.fido2.ingress.fido2ConfigEnabled bool false Enable endpoint /.well-known/fido2-configuration global.fqdn string \"demoexample.gluu.org\" Fully qualified domain name to be used for Gluu installation. This address will be used to reach Gluu services. global.gcePdStorageType string \"pd-standard\" GCE storage kind if using Google disks global.isFqdnRegistered bool false Boolean flag to enable mapping global.lbIp to global.fqdn inside pods on clouds that provide static ip for load balancers. On cloud that provide only addresses to the LB this flag will enable a script to actively scan config.configmap.lbAddr and update the hosts file inside the pods automatically. global.istio.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of global.istio.additionalLabels object {} Additional labels that will be added across the gateway in the format of global.istio.enabled bool false Boolean flag that enables using istio side-cars with Gluu services. global.istio.gateways list [] Override the gateway that can be created by default. This is used when istio ingress has already been setup and the gateway exists. global.istio.ingress bool false Boolean flag that enables using istio gateway for Gluu. This assumes istio ingress is installed and hence the LB is available. global.istio.namespace string \"istio-system\" The namespace istio is deployed in. The is normally istio-system. global.jobTtlSecondsAfterFinished int 300 https://kubernetes.io/docs/concepts/workloads/controllers/ttlafterfinished/ global.lbIp string \"22.22.22.22\" The Loadbalancer IP created by nginx or istio on clouds that provide static IPs. This is not needed if global.fqdn is globally resolvable. global.nginx-ingress.enabled bool true Boolean flag to enable/disable the nginx-ingress definitions chart. global.opendj.enabled bool false Boolean flag to enable/disable the OpenDJ chart. global.opendj.ldapServiceName string \"opendj\" Name of the OpenDJ service. Please keep it as default. global.oxpassport.enabled bool false Boolean flag to enable/disable passport chart global.oxpassport.oxPassportServiceName string \"oxpassport\" Name of the oxPassport service. Please keep it as default. global.oxshibboleth.appLoggers object {\"auditStatsLogLevel\":\"INFO\",\"auditStatsLogTarget\":\"FILE\",\"consentAuditLogLevel\":\"INFO\",\"consentAuditLogTarget\":\"FILE\",\"containerLogLevel\":\"\",\"encryptionLogLevel\":\"\",\"httpclientLogLevel\":\"\",\"idpLogLevel\":\"INFO\",\"idpLogTarget\":\"STDOUT\",\"ldapLogLevel\":\"\",\"messagesLogLevel\":\"\",\"opensamlLogLevel\":\"\",\"propsLogLevel\":\"\",\"scriptLogLevel\":\"INFO\",\"scriptLogTarget\":\"FILE\",\"springLogLevel\":\"\",\"xmlsecLogLevel\":\"\"} App loggers can be configured to define where the logs will be redirected to and the level of each in which it should be displayed. log levels are \"OFF\", \"FATAL\", \"ERROR\", \"WARN\", \"INFO\", \"DEBUG\", \"TRACE\" Targets are \"STDOUT\" and \"FILE\" global.oxshibboleth.appLoggers.auditStatsLogLevel string \"INFO\" idp-audit.log level global.oxshibboleth.appLoggers.auditStatsLogTarget string \"FILE\" idp-audit.log target global.oxshibboleth.appLoggers.consentAuditLogLevel string \"INFO\" idp-consent-audit.log level global.oxshibboleth.appLoggers.consentAuditLogTarget string \"FILE\" idp-consent-audit.log target global.oxshibboleth.appLoggers.idpLogLevel string \"INFO\" idp-process.log level global.oxshibboleth.appLoggers.idpLogTarget string \"STDOUT\" idp-process.log target global.oxshibboleth.appLoggers.ldapLogLevel string \"\" https://github.com/GluuFederation/docker-oxshibboleth#additional-logger-configuration The below are very noisy logs and are better left untouched global.oxshibboleth.appLoggers.scriptLogLevel string \"INFO\" idp-script.log level global.oxshibboleth.appLoggers.scriptLogTarget string \"FILE\" idp-script.log target global.oxshibboleth.enabled bool false Boolean flag to enable/disable the oxShibbboleth chart. global.oxshibboleth.oxShibbolethServiceName string \"oxshibboleth\" Name of the oxShibboleth service. Please keep it as default. global.persistence.enabled bool true Boolean flag to enable/disable the persistence chart. global.scim.appLoggers object {\"ldapStatsLogLevel\":\"INFO\",\"ldapStatsLogTarget\":\"FILE\",\"persistenceDurationLogLevel\":\"INFO\",\"persistenceDurationLogTarget\":\"FILE\",\"persistenceLogLevel\":\"INFO\",\"persistenceLogTarget\":\"FILE\",\"scimLogLevel\":\"INFO\",\"scimLogTarget\":\"STDOUT\",\"scriptLogLevel\":\"INFO\",\"scriptLogTarget\":\"FILE\"} App loggers can be configured to define where the logs will be redirected to and the level of each in which it should be displayed. global.scim.appLoggers.ldapStatsLogLevel string \"INFO\" jans-scim_persistence_ldap_statistics.log level global.scim.appLoggers.ldapStatsLogTarget string \"FILE\" jans-scim_persistence_ldap_statistics.log target global.scim.appLoggers.persistenceDurationLogLevel string \"INFO\" jans-scim_persistence_duration.log level global.scim.appLoggers.persistenceDurationLogTarget string \"FILE\" jans-scim_persistence_duration.log target global.scim.appLoggers.persistenceLogLevel string \"INFO\" jans-scim_persistence.log level global.scim.appLoggers.persistenceLogTarget string \"FILE\" jans-scim_persistence.log target global.scim.appLoggers.scimLogLevel string \"INFO\" jans-scim.log level global.scim.appLoggers.scimLogTarget string \"STDOUT\" jans-scim.log target global.scim.appLoggers.scriptLogLevel string \"INFO\" jans-scim_script.log level global.scim.appLoggers.scriptLogTarget string \"FILE\" jans-scim_script.log target global.scim.enabled bool true Boolean flag to enable/disable the SCIM chart. global.scim.ingress object {\"scimConfigEnabled\":false,\"scimEnabled\":false} Enable endpoints in either istio or nginx ingress depending on users choice global.scim.ingress.scimConfigEnabled bool false Enable endpoint /.well-known/scim-configuration global.scim.ingress.scimEnabled bool false Enable SCIM endpoints /jans-scim global.scim.scimServiceName string \"scim\" Name of the scim service. Please keep it as default. global.storageClass object {\"allowVolumeExpansion\":true,\"allowedTopologies\":[],\"mountOptions\":[\"debug\"],\"parameters\":{},\"provisioner\":\"microk8s.io/hostpath\",\"reclaimPolicy\":\"Retain\",\"volumeBindingMode\":\"WaitForFirstConsumer\"} StorageClass section for OpenDJ charts. This is not currently used by the openbanking distribution. You may specify custom parameters as needed. global.storageClass.parameters object {} parameters: fsType: \"\" kind: \"\" pool: \"\" storageAccountType: \"\" type: \"\" global.upgrade.enabled bool false Boolean flag used when running upgrading through versions command. Used when upgrading with LDAP as the persistence to load the 101x ldif. global.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service. Envs defined in global.userEnvs will be globally available to all services global.usrEnvs.normal object {} Add custom normal envs to the service. variable1: value1 global.usrEnvs.secret object {} Add custom secret envs to the service. variable1: value1 installer-settings object {\"acceptLicense\":\"\",\"aws\":{\"arn\":{\"arnAcmCert\":\"\",\"enabled\":\"\"},\"lbType\":\"\",\"vpcCidr\":\"0.0.0.0/0\"},\"confirmSettings\":false,\"couchbase\":{\"backup\":{\"fullSchedule\":\"\",\"incrementalSchedule\":\"\",\"retentionTime\":\"\",\"storageSize\":\"\"},\"clusterName\":\"\",\"commonName\":\"\",\"customFileOverride\":\"\",\"install\":\"\",\"lowResourceInstall\":\"\",\"namespace\":\"\",\"subjectAlternativeName\":\"\",\"totalNumberOfExpectedTransactionsPerSec\":\"\",\"totalNumberOfExpectedUsers\":\"\",\"volumeType\":\"\"},\"currentVersion\":\"\",\"google\":{\"useSecretManager\":\"\"},\"images\":{\"edit\":\"\"},\"ldap\":{\"backup\":{\"fullSchedule\":\"\"}},\"namespace\":\"\",\"nginxIngress\":{\"namespace\":\"\",\"releaseName\":\"\"},\"nodes\":{\"ips\":\"\",\"names\":\"\",\"zones\":\"\"},\"openbanking\":{\"cnObTransportTrustStoreP12password\":\"\",\"hasCnObTransportTrustStore\":false},\"postgres\":{\"install\":\"\",\"namespace\":\"\"},\"redis\":{\"install\":\"\",\"namespace\":\"\"},\"releaseName\":\"\",\"sql\":{\"install\":\"\",\"namespace\":\"\"},\"upgrade\":{\"image\":{\"repository\":\"\",\"tag\":\"\"},\"targetVersion\":\"\"},\"volumeProvisionStrategy\":\"\"} Only used by the installer. These settings do not affect nor are used by the chart nginx-ingress object {\"certManager\":{\"certificate\":{\"enabled\":false,\"issuerGroup\":\"cert-manager.io\",\"issuerKind\":\"ClusterIssuer\",\"issuerName\":\"\"}},\"ingress\":{\"additionalAnnotations\":{},\"additionalLabels\":{},\"adminUiAdditionalAnnotations\":{},\"adminUiLabels\":{},\"authServerAdditionalAnnotations\":{},\"authServerLabels\":{},\"authServerProtectedRegisterAdditionalAnnotations\":{},\"authServerProtectedRegisterLabels\":{},\"authServerProtectedTokenAdditionalAnnotations\":{},\"authServerProtectedTokenLabels\":{},\"casaAdditionalAnnotations\":{},\"casaLabels\":{},\"configApiAdditionalAnnotations\":{},\"configApiLabels\":{},\"deviceCodeAdditionalAnnotations\":{},\"deviceCodeLabels\":{},\"fido2ConfigAdditionalAnnotations\":{},\"fido2ConfigLabels\":{},\"firebaseMessagingAdditionalAnnotations\":{},\"firebaseMessagingLabels\":{},\"hosts\":[\"demoexample.gluu.org\"],\"openidAdditionalAnnotations\":{},\"openidConfigLabels\":{},\"path\":\"/\",\"scimAdditionalAnnotations\":{},\"scimConfigAdditionalAnnotations\":{},\"scimConfigLabels\":{},\"scimLabels\":{},\"tls\":[{\"hosts\":[\"demoexample.gluu.org\"],\"secretName\":\"tls-certificate\"}],\"u2fAdditionalAnnotations\":{},\"u2fConfigLabels\":{},\"uma2AdditionalAnnotations\":{},\"uma2ConfigLabels\":{},\"webdiscoveryAdditionalAnnotations\":{},\"webdiscoveryLabels\":{},\"webfingerAdditionalAnnotations\":{},\"webfingerLabels\":{}}} Nginx ingress definitions chart nginx-ingress.ingress.additionalAnnotations object {} Additional annotations that will be added across all ingress definitions in the format of {cert-manager.io/issuer: \"letsencrypt-prod\"} Enable client certificate authentication nginx.ingress.kubernetes.io/auth-tls-verify-client: \"optional\" Create the secret containing the trusted ca certificates nginx.ingress.kubernetes.io/auth-tls-secret: \"gluu/tls-certificate\" Specify the verification depth in the client certificates chain nginx.ingress.kubernetes.io/auth-tls-verify-depth: \"1\" Specify if certificates are passed to upstream server nginx.ingress.kubernetes.io/auth-tls-pass-certificate-to-upstream: \"true\" nginx-ingress.ingress.additionalLabels object {} Additional labels that will be added across all ingress definitions in the format of nginx-ingress.ingress.adminUiAdditionalAnnotations object {} openid-configuration ingress resource additional annotations. nginx-ingress.ingress.adminUiLabels object {} Admin UI ingress resource labels. key app is taken. nginx-ingress.ingress.authServerAdditionalAnnotations object {} Auth server ingress resource additional annotations. nginx-ingress.ingress.authServerLabels object {} Auth server ingress resource labels. key app is taken nginx-ingress.ingress.authServerProtectedRegisterAdditionalAnnotations object {} Auth server protected register ingress resource additional annotations. nginx-ingress.ingress.authServerProtectedRegisterLabels object {} Auth server protected token ingress resource labels. key app is taken nginx-ingress.ingress.authServerProtectedTokenAdditionalAnnotations object {} Auth server protected token ingress resource additional annotations. nginx-ingress.ingress.authServerProtectedTokenLabels object {} Auth server protected token ingress resource labels. key app is taken nginx-ingress.ingress.casaAdditionalAnnotations object {} Casa ingress resource additional annotations. nginx-ingress.ingress.casaLabels object {} Casa ingress resource labels. key app is taken nginx-ingress.ingress.configApiAdditionalAnnotations object {} ConfigAPI ingress resource additional annotations. nginx-ingress.ingress.configApiLabels object {} configAPI ingress resource labels. key app is taken nginx-ingress.ingress.deviceCodeAdditionalAnnotations object {} device-code ingress resource additional annotations. nginx-ingress.ingress.deviceCodeLabels object {} device-code ingress resource labels. key app is taken nginx-ingress.ingress.fido2ConfigAdditionalAnnotations object {} fido2 config ingress resource additional annotations. nginx-ingress.ingress.fido2ConfigLabels object {} fido2 config ingress resource labels. key app is taken nginx-ingress.ingress.firebaseMessagingAdditionalAnnotations object {} Firebase Messaging ingress resource additional annotations. nginx-ingress.ingress.firebaseMessagingLabels object {} Firebase Messaging ingress resource labels. key app is taken nginx-ingress.ingress.openidAdditionalAnnotations object {} openid-configuration ingress resource additional annotations. nginx-ingress.ingress.openidConfigLabels object {} openid-configuration ingress resource labels. key app is taken nginx-ingress.ingress.scimAdditionalAnnotations object {} SCIM ingress resource additional annotations. nginx-ingress.ingress.scimConfigAdditionalAnnotations object {} SCIM config ingress resource additional annotations. nginx-ingress.ingress.scimConfigLabels object {} SCIM config ingress resource labels. key app is taken nginx-ingress.ingress.scimLabels object {} SCIM config ingress resource labels. key app is taken nginx-ingress.ingress.tls list [{\"hosts\":[\"demoexample.gluu.org\"],\"secretName\":\"tls-certificate\"}] Secrets holding HTTPS CA cert and key. nginx-ingress.ingress.u2fAdditionalAnnotations object {} u2f config ingress resource additional annotations. nginx-ingress.ingress.u2fConfigLabels object {} u2f config ingress resource labels. key app is taken nginx-ingress.ingress.uma2AdditionalAnnotations object {} uma2 config ingress resource additional annotations. nginx-ingress.ingress.uma2ConfigLabels object {} uma2 config ingress resource labels. key app is taken nginx-ingress.ingress.webdiscoveryAdditionalAnnotations object {} webdiscovery ingress resource additional annotations. nginx-ingress.ingress.webdiscoveryLabels object {} webdiscovery ingress resource labels. key app is taken nginx-ingress.ingress.webfingerAdditionalAnnotations object {} webfinger ingress resource additional annotations. nginx-ingress.ingress.webfingerLabels object {} webfinger ingress resource labels. key app is taken opendj object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"backup\":{\"cronJobSchedule\":\"*/59 * * * *\",\"enabled\":true},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"hpa\":{\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"gluufederation/opendj\",\"tag\":\"5.0.0_dev\"},\"livenessProbe\":{\"exec\":{\"command\":[\"python3\",\"/app/scripts/healthcheck.py\"]},\"failureThreshold\":20,\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5},\"pdb\":{\"enabled\":true,\"maxUnavailable\":1},\"persistence\":{\"size\":\"5Gi\"},\"ports\":{\"tcp-admin\":{\"nodePort\":\"\",\"port\":4444,\"protocol\":\"TCP\",\"targetPort\":4444},\"tcp-ldap\":{\"nodePort\":\"\",\"port\":1389,\"protocol\":\"TCP\",\"targetPort\":1389},\"tcp-ldaps\":{\"nodePort\":\"\",\"port\":1636,\"protocol\":\"TCP\",\"targetPort\":1636},\"tcp-repl\":{\"nodePort\":\"\",\"port\":8989,\"protocol\":\"TCP\",\"targetPort\":8989},\"tcp-serf\":{\"nodePort\":\"\",\"port\":7946,\"protocol\":\"TCP\",\"targetPort\":7946},\"udp-serf\":{\"nodePort\":\"\",\"port\":7946,\"protocol\":\"UDP\",\"targetPort\":7946}},\"readinessProbe\":{\"failureThreshold\":20,\"initialDelaySeconds\":60,\"periodSeconds\":25,\"tcpSocket\":{\"port\":1636},\"timeoutSeconds\":5},\"replicas\":1,\"resources\":{\"limits\":{\"cpu\":\"1500m\",\"memory\":\"2000Mi\"},\"requests\":{\"cpu\":\"1500m\",\"memory\":\"2000Mi\"}},\"topologySpreadConstraints\":{},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} OpenDJ is a directory server which implements a wide range of Lightweight Directory Access Protocol and related standards, including full compliance with LDAPv3 but also support for Directory Service Markup Language (DSMLv2).Written in Java, OpenDJ offers multi-master replication, access control, and many extensions. opendj.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of opendj.additionalLabels object {} Additional labels that will be added across the gateway in the format of opendj.backup object {\"cronJobSchedule\":\"*/59 * * * *\",\"enabled\":true} Configure ldap backup cronjob opendj.dnsConfig object {} Add custom dns config opendj.dnsPolicy string \"\" Add custom dns policy opendj.hpa object {\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50} Configure the HorizontalPodAutoscaler opendj.hpa.behavior object {} Scaling Policies opendj.hpa.metrics list [] metrics if targetCPUUtilizationPercentage is not set opendj.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. opendj.image.pullSecrets list [] Image Pull Secrets opendj.image.repository string \"gluufederation/opendj\" Image to use for deploying. opendj.image.tag string \"5.0.0_dev\" Image tag to use for deploying. opendj.livenessProbe object {\"exec\":{\"command\":[\"python3\",\"/app/scripts/healthcheck.py\"]},\"failureThreshold\":20,\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5} Configure the liveness healthcheck for OpenDJ if needed. https://github.com/GluuFederation/docker-opendj/blob/master/scripts/healthcheck.py opendj.livenessProbe.exec object {\"command\":[\"python3\",\"/app/scripts/healthcheck.py\"]} Executes the python3 healthcheck. opendj.pdb object {\"enabled\":true,\"maxUnavailable\":1} Configure the PodDisruptionBudget opendj.persistence.size string \"5Gi\" OpenDJ volume size opendj.readinessProbe object {\"failureThreshold\":20,\"initialDelaySeconds\":60,\"periodSeconds\":25,\"tcpSocket\":{\"port\":1636},\"timeoutSeconds\":5} Configure the readiness healthcheck for OpenDJ if needed. https://github.com/GluuFederation/docker-opendj/blob/master/scripts/healthcheck.py opendj.replicas int 1 Service replica number. opendj.resources object {\"limits\":{\"cpu\":\"1500m\",\"memory\":\"2000Mi\"},\"requests\":{\"cpu\":\"1500m\",\"memory\":\"2000Mi\"}} Resource specs. opendj.resources.limits.cpu string \"1500m\" CPU limit. opendj.resources.limits.memory string \"2000Mi\" Memory limit. opendj.resources.requests.cpu string \"1500m\" CPU request. opendj.resources.requests.memory string \"2000Mi\" Memory request. opendj.topologySpreadConstraints object {} Configure the topology spread constraints. Notice this is a map NOT a list as in the upstream API https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ opendj.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service opendj.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 opendj.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 opendj.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers opendj.volumes list [] Configure any additional volumes that need to be attached to the pod oxpassport object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"hpa\":{\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"gluufederation/oxpassport\",\"tag\":\"5.0.0_dev\"},\"livenessProbe\":{\"failureThreshold\":20,\"httpGet\":{\"path\":\"/passport/health-check\",\"port\":\"http-passport\"},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5},\"pdb\":{\"enabled\":true,\"maxUnavailable\":\"90%\"},\"readinessProbe\":{\"failureThreshold\":20,\"httpGet\":{\"path\":\"/passport/health-check\",\"port\":\"http-passport\"},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5},\"replicas\":1,\"resources\":{\"limits\":{\"cpu\":\"700m\",\"memory\":\"900Mi\"},\"requests\":{\"cpu\":\"700m\",\"memory\":\"900Mi\"}},\"topologySpreadConstraints\":{},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} Gluu interface to Passport.js to support social login and inbound identity. oxpassport.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of oxpassport.additionalLabels object {} Additional labels that will be added across the gateway in the format of oxpassport.dnsConfig object {} Add custom dns config oxpassport.dnsPolicy string \"\" Add custom dns policy oxpassport.hpa object {\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50} Configure the HorizontalPodAutoscaler oxpassport.hpa.behavior object {} Scaling Policies oxpassport.hpa.metrics list [] metrics if targetCPUUtilizationPercentage is not set oxpassport.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. oxpassport.image.pullSecrets list [] Image Pull Secrets oxpassport.image.repository string \"gluufederation/oxpassport\" Image to use for deploying. oxpassport.image.tag string \"5.0.0_dev\" Image tag to use for deploying. oxpassport.livenessProbe object {\"failureThreshold\":20,\"httpGet\":{\"path\":\"/passport/health-check\",\"port\":\"http-passport\"},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5} Configure the liveness healthcheck for oxPassport if needed. oxpassport.livenessProbe.httpGet.path string \"/passport/health-check\" http liveness probe endpoint oxpassport.pdb object {\"enabled\":true,\"maxUnavailable\":\"90%\"} Configure the PodDisruptionBudget oxpassport.readinessProbe object {\"failureThreshold\":20,\"httpGet\":{\"path\":\"/passport/health-check\",\"port\":\"http-passport\"},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5} Configure the readiness healthcheck for the oxPassport if needed. oxpassport.readinessProbe.httpGet.path string \"/passport/health-check\" http readiness probe endpoint oxpassport.replicas int 1 Service replica number oxpassport.resources object {\"limits\":{\"cpu\":\"700m\",\"memory\":\"900Mi\"},\"requests\":{\"cpu\":\"700m\",\"memory\":\"900Mi\"}} Resource specs. oxpassport.resources.limits.cpu string \"700m\" CPU limit. oxpassport.resources.limits.memory string \"900Mi\" Memory limit. oxpassport.resources.requests.cpu string \"700m\" CPU request. oxpassport.resources.requests.memory string \"900Mi\" Memory request. oxpassport.topologySpreadConstraints object {} Configure the topology spread constraints. Notice this is a map NOT a list as in the upstream API https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ oxpassport.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service oxpassport.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 oxpassport.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 oxpassport.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers oxpassport.volumes list [] Configure any additional volumes that need to be attached to the pod oxshibboleth object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"hpa\":{\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"gluufederation/oxshibboleth\",\"tag\":\"5.0.0_dev\"},\"livenessProbe\":{\"httpGet\":{\"path\":\"/idp\",\"port\":\"http-oxshib\"},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5},\"pdb\":{\"enabled\":true,\"maxUnavailable\":1},\"readinessProbe\":{\"httpGet\":{\"path\":\"/idp\",\"port\":\"http-oxshib\"},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5},\"replicas\":1,\"resources\":{\"limits\":{\"cpu\":\"1000m\",\"memory\":\"1000Mi\"},\"requests\":{\"cpu\":\"1000m\",\"memory\":\"1000Mi\"}},\"topologySpreadConstraints\":{},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} Shibboleth project for the Gluu Server's SAML IDP functionality. oxshibboleth.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of oxshibboleth.additionalLabels object {} Additional labels that will be added across the gateway in the format of oxshibboleth.dnsConfig object {} Add custom dns config oxshibboleth.dnsPolicy string \"\" Add custom dns policy oxshibboleth.hpa object {\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50} Configure the HorizontalPodAutoscaler oxshibboleth.hpa.behavior object {} Scaling Policies oxshibboleth.hpa.metrics list [] metrics if targetCPUUtilizationPercentage is not set oxshibboleth.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. oxshibboleth.image.pullSecrets list [] Image Pull Secrets oxshibboleth.image.repository string \"gluufederation/oxshibboleth\" Image to use for deploying. oxshibboleth.image.tag string \"5.0.0_dev\" Image tag to use for deploying. oxshibboleth.livenessProbe object {\"httpGet\":{\"path\":\"/idp\",\"port\":\"http-oxshib\"},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5} Configure the liveness healthcheck for the oxShibboleth if needed. oxshibboleth.livenessProbe.httpGet.path string \"/idp\" http liveness probe endpoint oxshibboleth.pdb object {\"enabled\":true,\"maxUnavailable\":1} Configure the PodDisruptionBudget oxshibboleth.readinessProbe object {\"httpGet\":{\"path\":\"/idp\",\"port\":\"http-oxshib\"},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5} Configure the readiness healthcheck for the casa if needed. oxshibboleth.readinessProbe.httpGet.path string \"/idp\" http liveness probe endpoint oxshibboleth.replicas int 1 Service replica number. oxshibboleth.resources object {\"limits\":{\"cpu\":\"1000m\",\"memory\":\"1000Mi\"},\"requests\":{\"cpu\":\"1000m\",\"memory\":\"1000Mi\"}} Resource specs. oxshibboleth.resources.limits.cpu string \"1000m\" CPU limit. oxshibboleth.resources.limits.memory string \"1000Mi\" Memory limit. oxshibboleth.resources.requests.cpu string \"1000m\" CPU request. oxshibboleth.resources.requests.memory string \"1000Mi\" Memory request. oxshibboleth.topologySpreadConstraints object {} Configure the topology spread constraints. Notice this is a map NOT a list as in the upstream API https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ oxshibboleth.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service oxshibboleth.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 oxshibboleth.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 oxshibboleth.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers oxshibboleth.volumes list [] Configure any additional volumes that need to be attached to the pod persistence object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"janssenproject/persistence-loader\",\"tag\":\"1.0.5-1\"},\"resources\":{\"limits\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"},\"requests\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"}},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} Job to generate data and initial config for Gluu Server persistence layer. persistence.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of persistence.additionalLabels object {} Additional labels that will be added across the gateway in the format of persistence.dnsConfig object {} Add custom dns config persistence.dnsPolicy string \"\" Add custom dns policy persistence.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. persistence.image.pullSecrets list [] Image Pull Secrets persistence.image.repository string \"janssenproject/persistence-loader\" Image to use for deploying. persistence.image.tag string \"1.0.5-1\" Image tag to use for deploying. persistence.resources object {\"limits\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"},\"requests\":{\"cpu\":\"300m\",\"memory\":\"300Mi\"}} Resource specs. persistence.resources.limits.cpu string \"300m\" CPU limit persistence.resources.limits.memory string \"300Mi\" Memory limit. persistence.resources.requests.cpu string \"300m\" CPU request. persistence.resources.requests.memory string \"300Mi\" Memory request. persistence.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service persistence.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 persistence.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 persistence.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers persistence.volumes list [] Configure any additional volumes that need to be attached to the pod scim object {\"additionalAnnotations\":{},\"additionalLabels\":{},\"dnsConfig\":{},\"dnsPolicy\":\"\",\"hpa\":{\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50},\"image\":{\"pullPolicy\":\"IfNotPresent\",\"pullSecrets\":[],\"repository\":\"janssenproject/scim\",\"tag\":\"1.0.5-1\"},\"livenessProbe\":{\"httpGet\":{\"path\":\"/jans-scim/sys/health-check\",\"port\":8080},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5},\"pdb\":{\"enabled\":true,\"maxUnavailable\":\"90%\"},\"readinessProbe\":{\"httpGet\":{\"path\":\"/jans-scim/sys/health-check\",\"port\":8080},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5},\"replicas\":1,\"resources\":{\"limits\":{\"cpu\":\"1000m\",\"memory\":\"1000Mi\"},\"requests\":{\"cpu\":\"1000m\",\"memory\":\"1000Mi\"}},\"service\":{\"name\":\"http-scim\",\"port\":8080},\"topologySpreadConstraints\":{},\"usrEnvs\":{\"normal\":{},\"secret\":{}},\"volumeMounts\":[],\"volumes\":[]} System for Cross-domain Identity Management (SCIM) version 2.0 scim.additionalAnnotations object {} Additional annotations that will be added across the gateway in the format of scim.additionalLabels object {} Additional labels that will be added across the gateway in the format of scim.dnsConfig object {} Add custom dns config scim.dnsPolicy string \"\" Add custom dns policy scim.hpa object {\"behavior\":{},\"enabled\":true,\"maxReplicas\":10,\"metrics\":[],\"minReplicas\":1,\"targetCPUUtilizationPercentage\":50} Configure the HorizontalPodAutoscaler scim.hpa.behavior object {} Scaling Policies scim.hpa.metrics list [] metrics if targetCPUUtilizationPercentage is not set scim.image.pullPolicy string \"IfNotPresent\" Image pullPolicy to use for deploying. scim.image.pullSecrets list [] Image Pull Secrets scim.image.repository string \"janssenproject/scim\" Image to use for deploying. scim.image.tag string \"1.0.5-1\" Image tag to use for deploying. scim.livenessProbe object {\"httpGet\":{\"path\":\"/jans-scim/sys/health-check\",\"port\":8080},\"initialDelaySeconds\":30,\"periodSeconds\":30,\"timeoutSeconds\":5} Configure the liveness healthcheck for SCIM if needed. scim.livenessProbe.httpGet.path string \"/jans-scim/sys/health-check\" http liveness probe endpoint scim.pdb object {\"enabled\":true,\"maxUnavailable\":\"90%\"} Configure the PodDisruptionBudget scim.readinessProbe object {\"httpGet\":{\"path\":\"/jans-scim/sys/health-check\",\"port\":8080},\"initialDelaySeconds\":25,\"periodSeconds\":25,\"timeoutSeconds\":5} Configure the readiness healthcheck for the SCIM if needed. scim.readinessProbe.httpGet.path string \"/jans-scim/sys/health-check\" http readiness probe endpoint scim.replicas int 1 Service replica number. scim.resources.limits.cpu string \"1000m\" CPU limit. scim.resources.limits.memory string \"1000Mi\" Memory limit. scim.resources.requests.cpu string \"1000m\" CPU request. scim.resources.requests.memory string \"1000Mi\" Memory request. scim.service.name string \"http-scim\" The name of the scim port within the scim service. Please keep it as default. scim.service.port int 8080 Port of the scim service. Please keep it as default. scim.topologySpreadConstraints object {} Configure the topology spread constraints. Notice this is a map NOT a list as in the upstream API https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/ scim.usrEnvs object {\"normal\":{},\"secret\":{}} Add custom normal and secret envs to the service scim.usrEnvs.normal object {} Add custom normal envs to the service variable1: value1 scim.usrEnvs.secret object {} Add custom secret envs to the service variable1: value1 scim.volumeMounts list [] Configure any additional volumesMounts that need to be attached to the containers scim.volumes list [] Configure any additional volumes that need to be attached to the pod","title":"Values"},{"location":"saml/","text":"Where is this content? # The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates.","title":"Index"},{"location":"saml/#where-is-this-content","text":"The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates.","title":"Where is this content?"},{"location":"saml/idp/","text":"Where is this content? # The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates.","title":"Idp"},{"location":"saml/idp/#where-is-this-content","text":"The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates.","title":"Where is this content?"},{"location":"saml/proxy/","text":"Where is this content? # The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates.","title":"Proxy"},{"location":"saml/proxy/#where-is-this-content","text":"The Gluu Flex documentation is a work in progress, and this document is currently a draft. Keep an eye on this page for updates.","title":"Where is this content?"}]}